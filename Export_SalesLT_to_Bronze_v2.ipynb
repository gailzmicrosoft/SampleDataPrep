{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23632548",
   "metadata": {},
   "source": [
    "# Export SalesLT Tables to Retail Data Model Bronze Layer (v2)\n",
    "\n",
    "This notebook dynamically discovers and exports all tables from the SalesLT schema to the retail data model bronze layer.\n",
    "**âœ… Updated with lakehouse shortcuts support and authentication handling**\n",
    "\n",
    "**Prerequisites:**\n",
    "- Fabric workspace with lakehouse shortcuts to SalesLT tables (âœ… Done!)\n",
    "- Retail data model lakehouse attached to this notebook\n",
    "- Appropriate permissions for lakehouse read/write operations\n",
    "\n",
    "**What's New in v2:**\n",
    "- âœ… Optimized for lakehouse shortcuts\n",
    "- âœ… Enhanced authentication handling\n",
    "- âœ… Better error reporting and guidance\n",
    "- âœ… Community-recommended approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7547b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "# Import required libraries (Fabric-compatible only)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pyspark.sql.functions import lit\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ğŸ“… Export started at: {datetime.now()}\")\n",
    "print(\"ğŸ”§ Using Fabric-native connectivity with shortcuts support\")\n",
    "print(\"ğŸ¯ Optimized for lakehouse shortcuts authentication method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Environment Diagnostic & Connectivity Check\n",
    "\n",
    "print(\"ğŸ” FABRIC ENVIRONMENT DIAGNOSTIC\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check 1: Spark session\n",
    "try:\n",
    "    spark_version = spark.version\n",
    "    print(f\"âœ… Spark session active: {spark_version}\")\n",
    "except NameError:\n",
    "    print(\"âŒ Spark session not available - ensure you're running in Fabric\")\n",
    "    raise Exception(\"Spark session required for this notebook\")\n",
    "\n",
    "# Check 2: Available databases\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\").toPandas()\n",
    "    db_column = 'namespace' if 'namespace' in databases.columns else 'databaseName'\n",
    "    print(f\"âœ… Available databases ({len(databases)}):\")\n",
    "    for db in databases[db_column][:5]:  # Show first 5\n",
    "        print(f\"   ğŸ“ {db}\")\n",
    "    if len(databases) > 5:\n",
    "        print(f\"   ... and {len(databases)-5} more\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Cannot list databases: {str(e)}\")\n",
    "\n",
    "# Check 3: Lakehouse tables (including shortcuts)\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    print(f\"âœ… Lakehouse tables visible ({len(tables)}):\")\n",
    "    \n",
    "    # Look specifically for SalesLT shortcuts\n",
    "    saleslt_indicators = ['saleslt', 'customer', 'product', 'address', 'salesorder']\n",
    "    saleslt_tables = []\n",
    "    \n",
    "    for table in tables['tableName']:\n",
    "        if any(indicator in table.lower() for indicator in saleslt_indicators):\n",
    "            saleslt_tables.append(table)\n",
    "    \n",
    "    if saleslt_tables:\n",
    "        print(f\"ğŸ‰ FOUND SALESLT SHORTCUTS ({len(saleslt_tables)}):\")\n",
    "        for table in saleslt_tables:\n",
    "            print(f\"   ğŸ”— {table}\")\n",
    "    else:\n",
    "        print(\"ğŸ“‹ First 5 tables:\")\n",
    "        for table in tables['tableName'][:5]:\n",
    "            print(f\"   ğŸ“Š {table}\")\n",
    "        if len(tables) > 5:\n",
    "            print(f\"   ... and {len(tables)-5} more\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Cannot access lakehouse tables: {str(e)}\")\n",
    "\n",
    "# Check 4: File system access\n",
    "try:\n",
    "    files_path = \"Files\"\n",
    "    bronze_path = \"Files/bronze\"\n",
    "    \n",
    "    if not os.path.exists(files_path):\n",
    "        os.makedirs(files_path, exist_ok=True)\n",
    "        print(f\"âœ… Created {files_path} directory\")\n",
    "    else:\n",
    "        print(f\"âœ… File system access: {files_path} exists\")\n",
    "        \n",
    "    if not os.path.exists(bronze_path):\n",
    "        os.makedirs(bronze_path, exist_ok=True)\n",
    "        print(f\"âœ… Created {bronze_path} directory\")\n",
    "    else:\n",
    "        print(f\"âœ… Bronze layer: {bronze_path} exists\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ File system access failed: {str(e)}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Environment diagnostic complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7929c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Smart SalesLT Table Discovery (Shortcuts Optimized)\n",
    "\n",
    "print(\"ğŸ” DISCOVERING SALESLT TABLES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ Optimized for lakehouse shortcuts detection\")\n",
    "print()\n",
    "\n",
    "tables_info = []\n",
    "schema_name = \"SalesLT\"\n",
    "auth_method = \"unknown\"\n",
    "connection_method = \"unknown\"\n",
    "\n",
    "# Method 1: Check for lakehouse shortcuts (most likely scenario)\n",
    "print(\"ğŸ“ Method 1: Detecting lakehouse shortcuts\")\n",
    "try:\n",
    "    all_tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    print(f\"   ğŸ“Š Total tables in lakehouse: {len(all_tables)}\")\n",
    "    \n",
    "    # Look for SalesLT-related tables (shortcuts often have different naming)\n",
    "    saleslt_patterns = [\n",
    "        'saleslt',  # Direct schema prefix\n",
    "        'customer', 'product', 'address',  # Core business entities\n",
    "        'salesorder', 'order'  # Sales entities\n",
    "    ]\n",
    "    \n",
    "    potential_tables = []\n",
    "    for _, row in all_tables.iterrows():\n",
    "        table_name = row['tableName'].lower()\n",
    "        if any(pattern in table_name for pattern in saleslt_patterns):\n",
    "            potential_tables.append(row['tableName'])\n",
    "    \n",
    "    if potential_tables:\n",
    "        print(f\"   âœ… Found {len(potential_tables)} SalesLT-related tables:\")\n",
    "        \n",
    "        for table_name in potential_tables:\n",
    "            # Categorize tables\n",
    "            category = 'Reference Data'\n",
    "            table_lower = table_name.lower()\n",
    "            \n",
    "            if 'customer' in table_lower:\n",
    "                category = 'Customer Data'\n",
    "            elif 'product' in table_lower:\n",
    "                category = 'Product Catalog'\n",
    "            elif any(sales_word in table_lower for sales_word in ['salesorder', 'order']):\n",
    "                category = 'Sales Transactions'\n",
    "            elif 'address' in table_lower:\n",
    "                category = 'Address Information'\n",
    "            \n",
    "            tables_info.append({\n",
    "                'table_name': table_name,\n",
    "                'full_name': table_name,  # For shortcuts, use table name directly\n",
    "                'type': 'SHORTCUT',\n",
    "                'category': category,\n",
    "                'source': 'lakehouse_shortcut'\n",
    "            })\n",
    "            \n",
    "            print(f\"      ğŸ”— {table_name} ({category})\")\n",
    "        \n",
    "        auth_method = \"lakehouse_shortcuts\"\n",
    "        connection_method = \"shortcuts_native\"\n",
    "        \n",
    "    else:\n",
    "        print(\"   âŒ No SalesLT-related tables found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error checking shortcuts: {str(e)[:100]}...\")\n",
    "\n",
    "# Method 2: Fallback - Standard SalesLT table structure\n",
    "if not tables_info:\n",
    "    print(\"\\nğŸ“ Method 2: Using standard SalesLT table structure (fallback)\")\n",
    "    print(\"   âš ï¸  No shortcuts detected - using expected table names\")\n",
    "    \n",
    "    standard_saleslt_tables = [\n",
    "        ('Address', 'Address Information'),\n",
    "        ('Customer', 'Customer Data'),\n",
    "        ('CustomerAddress', 'Customer Data'),\n",
    "        ('Product', 'Product Catalog'),\n",
    "        ('ProductCategory', 'Product Catalog'),\n",
    "        ('ProductDescription', 'Product Catalog'),\n",
    "        ('ProductModel', 'Product Catalog'),\n",
    "        ('ProductModelProductDescription', 'Product Catalog'),\n",
    "        ('SalesOrderDetail', 'Sales Transactions'),\n",
    "        ('SalesOrderHeader', 'Sales Transactions')\n",
    "    ]\n",
    "    \n",
    "    for table_name, category in standard_saleslt_tables:\n",
    "        tables_info.append({\n",
    "            'table_name': table_name,\n",
    "            'full_name': f\"SalesLT_{table_name}\",  # Assume prefix for fallback\n",
    "            'type': 'EXPECTED',\n",
    "            'category': category,\n",
    "            'source': 'standard_list'\n",
    "        })\n",
    "    \n",
    "    auth_method = \"manual_setup_needed\"\n",
    "    connection_method = \"shortcuts_required\"\n",
    "    print(f\"   ğŸ“‹ Using standard list of {len(tables_info)} expected tables\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nğŸ“Š DISCOVERY SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if tables_info:\n",
    "    # Group by category\n",
    "    by_category = defaultdict(list)\n",
    "    for table in tables_info:\n",
    "        by_category[table['category']].append(table)\n",
    "    \n",
    "    total_tables = len(tables_info)\n",
    "    print(f\"ğŸ“‹ Total Tables: {total_tables}\")\n",
    "    print(f\"ğŸ“‹ Source Method: {tables_info[0]['source']}\")\n",
    "    print(f\"ğŸ” Authentication: {auth_method}\")\n",
    "    print()\n",
    "    \n",
    "    for category, tables in by_category.items():\n",
    "        print(f\"ğŸ“ {category} ({len(tables)} tables):\")\n",
    "        for table in tables:\n",
    "            print(f\"   ğŸ”— {table['table_name']}\")\n",
    "    \n",
    "    if auth_method == \"lakehouse_shortcuts\":\n",
    "        print(f\"\\nğŸ‰ SHORTCUTS DETECTED - READY TO EXPORT!\")\n",
    "        print(f\"âœ… No authentication issues\")\n",
    "        print(f\"âœ… Direct table access available\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  SHORTCUTS NOT FOUND\")\n",
    "        print(f\"ğŸ”§ Please create lakehouse shortcuts to SalesLT tables first\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ NO TABLES DISCOVERED\")\n",
    "    print(\"ğŸ”§ Create lakehouse shortcuts to SQL Server SalesLT tables\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Optimized Export Function for Shortcuts\n",
    "\n",
    "def export_table_to_bronze_v2(table_info):\n",
    "    \"\"\"\n",
    "    Export a single table to bronze layer - optimized for lakehouse shortcuts\n",
    "    \"\"\"\n",
    "    table_name = table_info['table_name']\n",
    "    full_name = table_info['full_name']\n",
    "    category = table_info['category']\n",
    "    source = table_info['source']\n",
    "    \n",
    "    print(f\"ğŸ”„ Exporting {table_name} ({category})\")\n",
    "    print(f\"   ğŸ“‹ Source: {source}\")\n",
    "    \n",
    "    try:\n",
    "        # Read data based on source type\n",
    "        if source == \"lakehouse_shortcut\":\n",
    "            print(f\"   ğŸ”— Reading from shortcut: {table_name}\")\n",
    "            df = spark.sql(f\"SELECT * FROM {table_name}\")\n",
    "            \n",
    "        elif source == \"standard_list\":\n",
    "            print(f\"   âš ï¸  Attempting to read expected table: {full_name}\")\n",
    "            # Try different possible table names\n",
    "            possible_names = [table_name, full_name, f\"SalesLT_{table_name}\"]\n",
    "            df = None\n",
    "            \n",
    "            for name in possible_names:\n",
    "                try:\n",
    "                    df = spark.sql(f\"SELECT * FROM {name}\")\n",
    "                    print(f\"   âœ… Found table as: {name}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                raise Exception(f\"Table not found with any expected name: {possible_names}\")\n",
    "        \n",
    "        else:\n",
    "            raise Exception(f\"Unknown source type: {source}\")\n",
    "        \n",
    "        # Get row count and basic info\n",
    "        row_count = df.count()\n",
    "        columns = df.columns\n",
    "        \n",
    "        print(f\"   ğŸ“Š Loaded: {row_count:,} rows, {len(columns)} columns\")\n",
    "        \n",
    "        if row_count == 0:\n",
    "            print(f\"   âš ï¸  Warning: Table is empty\")\n",
    "        \n",
    "        # Create bronze layer path\n",
    "        bronze_path = f\"Files/bronze/saleslt/{table_name.lower()}\"\n",
    "        \n",
    "        # Add comprehensive metadata columns\n",
    "        df_with_metadata = df \\\n",
    "            .withColumn(\"_bronze_load_date\", lit(datetime.now().strftime(\"%Y-%m-%d\"))) \\\n",
    "            .withColumn(\"_bronze_load_timestamp\", lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))) \\\n",
    "            .withColumn(\"_source_system\", lit(\"SalesLT\")) \\\n",
    "            .withColumn(\"_source_table\", lit(table_name)) \\\n",
    "            .withColumn(\"_source_schema\", lit(\"SalesLT\")) \\\n",
    "            .withColumn(\"_load_method\", lit(source)) \\\n",
    "            .withColumn(\"_notebook_version\", lit(\"v2_shortcuts_optimized\")) \\\n",
    "            .withColumn(\"_export_id\", lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "        \n",
    "        # Write to bronze layer as Parquet\n",
    "        print(f\"   ğŸ’¾ Writing to: {bronze_path}\")\n",
    "        \n",
    "        df_with_metadata.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .parquet(bronze_path)\n",
    "        \n",
    "        # Create detailed metadata file\n",
    "        metadata = {\n",
    "            \"table_name\": table_name,\n",
    "            \"original_name\": full_name,\n",
    "            \"category\": category,\n",
    "            \"source_system\": \"SalesLT\",\n",
    "            \"row_count\": row_count,\n",
    "            \"column_count\": len(columns),\n",
    "            \"columns\": columns,\n",
    "            \"load_timestamp\": datetime.now().isoformat(),\n",
    "            \"load_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            \"load_method\": source,\n",
    "            \"bronze_path\": bronze_path,\n",
    "            \"format\": \"parquet\",\n",
    "            \"notebook_version\": \"v2_shortcuts_optimized\",\n",
    "            \"export_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        }\n",
    "        \n",
    "        # Ensure metadata directory exists\n",
    "        metadata_dir = \"Files/bronze/saleslt/_metadata\"\n",
    "        os.makedirs(metadata_dir, exist_ok=True)\n",
    "        \n",
    "        # Write metadata as JSON\n",
    "        metadata_df = spark.createDataFrame([metadata])\n",
    "        metadata_df.coalesce(1).write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .json(f\"Files/bronze/saleslt/_metadata/{table_name.lower()}\")\n",
    "        \n",
    "        print(f\"   âœ… Export completed: {row_count:,} rows\")\n",
    "        print(f\"   ğŸ“ Data: {bronze_path}\")\n",
    "        print(f\"   ğŸ“‹ Metadata: Files/bronze/saleslt/_metadata/{table_name.lower()}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_count\": row_count,\n",
    "            \"column_count\": len(columns),\n",
    "            \"bronze_path\": bronze_path,\n",
    "            \"load_method\": source\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"   âŒ Export failed: {error_msg[:150]}...\")\n",
    "        \n",
    "        # Provide specific guidance based on error type\n",
    "        if \"table not found\" in error_msg.lower() or \"cannot resolve\" in error_msg.lower():\n",
    "            print(f\"   ğŸ’¡ Table not accessible - check shortcut setup\")\n",
    "        elif \"permission\" in error_msg.lower() or \"access\" in error_msg.lower():\n",
    "            print(f\"   ğŸ’¡ Permission issue - check lakehouse access\")\n",
    "            \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"table_name\": table_name,\n",
    "            \"error\": error_msg,\n",
    "            \"load_method\": source,\n",
    "            \"recommendation\": \"Check table accessibility and permissions\"\n",
    "        }\n",
    "\n",
    "print(\"ğŸ› ï¸  EXPORT FUNCTION v2 READY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Shortcuts-optimized export function loaded\")\n",
    "print(\"âœ… Enhanced metadata tracking\")\n",
    "print(\"âœ… Improved error handling and guidance\")\n",
    "print(\"âœ… Multiple table name resolution strategies\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Execute Bulk Export with Progress Tracking\n",
    "\n",
    "print(\"ğŸš€ STARTING SALESLT TO BRONZE EXPORT (v2)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“… Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ” Authentication Method: {auth_method}\")\n",
    "print(f\"ğŸ“‹ Connection Method: {connection_method}\")\n",
    "print(f\"ğŸ“Š Tables to Export: {len(tables_info)}\")\n",
    "print()\n",
    "\n",
    "# Check readiness\n",
    "if auth_method == \"manual_setup_needed\":\n",
    "    print(\"âš ï¸  SHORTCUT SETUP REQUIRED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ”§ Please create lakehouse shortcuts to SalesLT tables first:\")\n",
    "    print(\"   1. Go to your lakehouse â†’ Tables section\")\n",
    "    print(\"   2. Click 'New shortcut' â†’ 'Azure SQL Database'\")\n",
    "    print(\"   3. Server: gaiye-sql-db.sql.fabric.microsoft.com\")\n",
    "    print(\"   4. Database: Gaiye-SQL-DB\")\n",
    "    print(\"   5. Select SalesLT schema and all tables\")\n",
    "    print(\"   6. Re-run this notebook\")\n",
    "    print(\"=\" * 60)\n",
    "elif not tables_info:\n",
    "    print(\"âŒ NO TABLES TO EXPORT\")\n",
    "    print(\"ğŸ”§ Check shortcut setup and re-run discovery\")\n",
    "else:\n",
    "    # Proceed with export\n",
    "    export_results = []\n",
    "    successful_exports = 0\n",
    "    failed_exports = 0\n",
    "    total_rows_exported = 0\n",
    "    \n",
    "    print(\"ğŸ’¾ STARTING TABLE EXPORTS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create bronze directory structure\n",
    "    try:\n",
    "        os.makedirs(\"Files/bronze/saleslt\", exist_ok=True)\n",
    "        os.makedirs(\"Files/bronze/saleslt/_metadata\", exist_ok=True)\n",
    "        print(\"âœ… Bronze directory structure ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Directory creation warning: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Export each table with progress tracking\n",
    "    for i, table_info in enumerate(tables_info, 1):\n",
    "        print(f\"\\n[{i}/{len(tables_info)}] {table_info['table_name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        result = export_table_to_bronze_v2(table_info)\n",
    "        export_results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            successful_exports += 1\n",
    "            total_rows_exported += result['row_count']\n",
    "        else:\n",
    "            failed_exports += 1\n",
    "            \n",
    "        # Small delay for system stability\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Generate comprehensive export summary\n",
    "    print(f\"\\nğŸ‰ EXPORT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"â±ï¸  Total Duration: {duration:.1f} seconds\")\n",
    "    print(f\"âœ… Successful Exports: {successful_exports}\")\n",
    "    print(f\"âŒ Failed Exports: {failed_exports}\")\n",
    "    print(f\"ğŸ“Š Total Tables: {len(tables_info)}\")\n",
    "    print(f\"ğŸ“ˆ Total Rows Exported: {total_rows_exported:,}\")\n",
    "    \n",
    "    if successful_exports > 0:\n",
    "        print(f\"\\nğŸ“ BRONZE LAYER STRUCTURE:\")\n",
    "        print(f\"   Files/bronze/saleslt/\")\n",
    "        \n",
    "        # Show exported tables by category\n",
    "        by_category = defaultdict(list)\n",
    "        for result in export_results:\n",
    "            if result['success']:\n",
    "                table_info = next((t for t in tables_info if t['table_name'] == result['table_name']), None)\n",
    "                if table_info:\n",
    "                    by_category[table_info['category']].append(result)\n",
    "        \n",
    "        for category, results in by_category.items():\n",
    "            category_rows = sum(r['row_count'] for r in results)\n",
    "            print(f\"   ğŸ“‚ {category} ({len(results)} tables, {category_rows:,} total rows):\")\n",
    "            for result in results:\n",
    "                print(f\"      ğŸ“„ {result['table_name'].lower()}/ ({result['row_count']:,} rows)\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ METADATA & TRACKING:\")\n",
    "        print(f\"   Files/bronze/saleslt/_metadata/ (JSON files for each table)\")\n",
    "        \n",
    "        # Create overall summary file\n",
    "        summary_data = {\n",
    "            \"export_timestamp\": datetime.now().isoformat(),\n",
    "            \"export_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            \"notebook_version\": \"v2_shortcuts_optimized\",\n",
    "            \"auth_method\": auth_method,\n",
    "            \"connection_method\": connection_method,\n",
    "            \"total_tables\": len(tables_info),\n",
    "            \"successful_exports\": successful_exports,\n",
    "            \"failed_exports\": failed_exports,\n",
    "            \"total_rows_exported\": total_rows_exported,\n",
    "            \"duration_seconds\": duration,\n",
    "            \"export_results\": export_results\n",
    "        }\n",
    "        \n",
    "        # Save summary as JSON\n",
    "        summary_df = spark.createDataFrame([summary_data])\n",
    "        summary_df.coalesce(1).write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .json(\"Files/bronze/saleslt/_export_summary\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ NEXT STEPS:\")\n",
    "        print(f\"   1. ğŸ” Validate data: Check row counts and data quality\")\n",
    "        print(f\"   2. ğŸ¥ˆ Create Silver layer: Data cleansing and standardization\")\n",
    "        print(f\"   3. ğŸ¥‡ Create Gold layer: Business aggregations and metrics\")\n",
    "        print(f\"   4. ğŸ“Š Build reports: Connect Power BI or create Fabric reports\")\n",
    "    \n",
    "    if failed_exports > 0:\n",
    "        print(f\"\\nâš ï¸  EXPORT ISSUES ({failed_exports} tables):\")\n",
    "        for result in export_results:\n",
    "            if not result['success']:\n",
    "                print(f\"   âŒ {result['table_name']}: {result.get('error', 'Unknown error')[:100]}...\")\n",
    "        \n",
    "        print(f\"\\nğŸ”§ TROUBLESHOOTING:\")\n",
    "        print(f\"   - Verify lakehouse shortcuts are properly created\")\n",
    "        print(f\"   - Check table names match exactly\")\n",
    "        print(f\"   - Ensure read permissions on source tables\")\n",
    "        print(f\"   - Try re-creating failed shortcuts individually\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“‹ Export process completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Validation & Quality Checks\n",
    "\n",
    "print(\"ğŸ” VALIDATION & QUALITY CHECKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "bronze_base_path = \"Files/bronze/saleslt\"\n",
    "\n",
    "try:\n",
    "    if os.path.exists(bronze_base_path):\n",
    "        # List all bronze layer contents\n",
    "        bronze_contents = [item for item in os.listdir(bronze_base_path) if not item.startswith('.')]\n",
    "        \n",
    "        print(f\"ğŸ“ Bronze layer contains {len(bronze_contents)} items:\")\n",
    "        \n",
    "        table_directories = []\n",
    "        other_items = []\n",
    "        \n",
    "        for item in sorted(bronze_contents):\n",
    "            item_path = os.path.join(bronze_base_path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                if item != '_metadata':\n",
    "                    # Check for parquet files\n",
    "                    try:\n",
    "                        files = os.listdir(item_path)\n",
    "                        parquet_files = [f for f in files if f.endswith('.parquet')]\n",
    "                        print(f\"   ğŸ“‚ {item}/ ({len(parquet_files)} parquet files)\")\n",
    "                        table_directories.append(item)\n",
    "                        \n",
    "                        # Quick row count check using Spark\n",
    "                        try:\n",
    "                            df = spark.read.parquet(item_path)\n",
    "                            row_count = df.count()\n",
    "                            col_count = len(df.columns)\n",
    "                            print(f\"      ğŸ“Š {row_count:,} rows, {col_count} columns\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"      âš ï¸  Row count check failed: {str(e)[:50]}...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"   âŒ Error reading {item}: {str(e)[:50]}...\")\n",
    "                else:\n",
    "                    # Metadata directory\n",
    "                    try:\n",
    "                        metadata_files = os.listdir(item_path)\n",
    "                        print(f\"   ğŸ“‹ _metadata/ ({len(metadata_files)} metadata files)\")\n",
    "                    except:\n",
    "                        print(f\"   ğŸ“‹ _metadata/ (cannot read contents)\")\n",
    "            else:\n",
    "                other_items.append(item)\n",
    "                print(f\"   ğŸ“„ {item}\")\n",
    "        \n",
    "        # Summary validation\n",
    "        print(f\"\\nğŸ“Š VALIDATION SUMMARY:\")\n",
    "        print(f\"   âœ… Table directories: {len(table_directories)}\")\n",
    "        print(f\"   ğŸ“‹ Metadata available: {'Yes' if '_metadata' in bronze_contents else 'No'}\")\n",
    "        print(f\"   ğŸ“„ Other files: {len(other_items)}\")\n",
    "        \n",
    "        # Expected vs Actual comparison\n",
    "        if 'successful_exports' in locals() and successful_exports > 0:\n",
    "            print(f\"   ğŸ¯ Expected vs Actual: {successful_exports} expected, {len(table_directories)} found\")\n",
    "            if successful_exports == len(table_directories):\n",
    "                print(f\"   âœ… All expected tables present\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸  Mismatch in expected vs actual table count\")\n",
    "        \n",
    "        # Data quality spot checks\n",
    "        print(f\"\\nğŸ” DATA QUALITY SPOT CHECKS:\")\n",
    "        sample_tables = table_directories[:3]  # Check first 3 tables\n",
    "        \n",
    "        for table_dir in sample_tables:\n",
    "            try:\n",
    "                table_path = os.path.join(bronze_base_path, table_dir)\n",
    "                df = spark.read.parquet(table_path)\n",
    "                \n",
    "                # Basic checks\n",
    "                row_count = df.count()\n",
    "                null_counts = df.select([col for col in df.columns if not col.startswith('_')]).summary('count').collect()[0]\n",
    "                \n",
    "                print(f\"   ğŸ“Š {table_dir}:\")\n",
    "                print(f\"      ğŸ“ˆ Rows: {row_count:,}\")\n",
    "                print(f\"      ğŸ“‹ Columns: {len(df.columns)}\")\n",
    "                print(f\"      ğŸ” Has metadata columns: {any(col.startswith('_') for col in df.columns)}\")\n",
    "                \n",
    "                if row_count == 0:\n",
    "                    print(f\"      âš ï¸  Warning: Empty table\")\n",
    "                elif row_count < 10:\n",
    "                    print(f\"      âš ï¸  Warning: Very few rows ({row_count})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error checking {table_dir}: {str(e)[:80]}...\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âŒ Bronze layer path not found: {bronze_base_path}\")\n",
    "        print(f\"ğŸ”§ This suggests the export may not have completed successfully\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during validation: {str(e)}\")\n",
    "\n",
    "print(f\"\\nâœ… Validation completed at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbd3ab",
   "metadata": {},
   "source": [
    "## Success! Next Steps for Your Data Pipeline\n",
    "\n",
    "ğŸ‰ **Congratulations!** Your SalesLT data has been successfully exported to the bronze layer with lakehouse shortcuts.\n",
    "\n",
    "### What You've Accomplished:\n",
    "- âœ… **Bypassed authentication issues** using lakehouse shortcuts\n",
    "- âœ… **Automated table discovery** from your shortcuts\n",
    "- âœ… **Organized bronze layer** with proper medallion architecture\n",
    "- âœ… **Added comprehensive metadata** for data lineage tracking\n",
    "- âœ… **Quality validation** with row counts and structure checks\n",
    "\n",
    "### Recommended Next Steps:\n",
    "\n",
    "#### 1. ğŸ¥ˆ **Silver Layer Development**\n",
    "Create a new notebook for data cleansing and standardization:\n",
    "- Remove duplicates and handle missing values\n",
    "- Standardize data formats and naming conventions\n",
    "- Apply business rules and data validation\n",
    "- Create clean, analysis-ready datasets\n",
    "\n",
    "#### 2. ğŸ¥‡ **Gold Layer Implementation**\n",
    "Build business-ready aggregated tables:\n",
    "- Create dimensional models (facts and dimensions)\n",
    "- Build summary tables for key business metrics\n",
    "- Implement slowly changing dimensions (SCD)\n",
    "- Optimize for analytical workloads\n",
    "\n",
    "#### 3. ğŸ“Š **Analytics and Reporting**\n",
    "Connect your data to visualization tools:\n",
    "- Create Power BI semantic models\n",
    "- Build interactive dashboards\n",
    "- Set up automated report distribution\n",
    "- Enable self-service analytics\n",
    "\n",
    "#### 4. ğŸ”„ **Pipeline Automation**\n",
    "Set up automated data refresh:\n",
    "- Schedule regular data updates\n",
    "- Implement incremental loading\n",
    "- Add data quality monitoring\n",
    "- Create alerting for pipeline failures\n",
    "\n",
    "### Quick Data Exploration:\n",
    "\n",
    "```python\n",
    "# List all your bronze tables\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Quick look at customer data\n",
    "spark.sql(\"SELECT * FROM bronze.customer LIMIT 10\").show()\n",
    "\n",
    "# Check data freshness\n",
    "spark.sql(\"SELECT _bronze_load_timestamp, COUNT(*) FROM bronze.customer GROUP BY _bronze_load_timestamp\").show()\n",
    "```\n",
    "\n",
    "### Pro Tips:\n",
    "- **Monitor shortcut refresh**: Shortcuts automatically sync, but monitor for any issues\n",
    "- **Document your pipeline**: Keep track of transformations and business logic\n",
    "- **Test with subsets**: Use sampling for development and testing\n",
    "- **Version your notebooks**: Keep different versions for different environments\n",
    "\n",
    "---\n",
    "\n",
    "*This v2 notebook successfully overcame the authentication challenges by leveraging lakehouse shortcuts - the community-recommended approach for reliable SQL Server connectivity in Microsoft Fabric.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
