{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3592a3e",
   "metadata": {},
   "source": [
    "# Export SalesLT Tables to Retail Data Model Bronze Layer\n",
    "\n",
    "This notebook dynamically discovers and exports all tables from the SalesLT schema in Gaiye-SQL-DB to the retail data model bronze layer.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Fabric workspace with access to Gaiye-SQL-DB\n",
    "- Retail data model lakehouse attached to this notebook\n",
    "- Appropriate permissions for SQL database and lakehouse access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749177a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries (Fabric-compatible only)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Export started at: {datetime.now()}\")\n",
    "print(\"üîß Using Fabric-native connectivity (no external dependencies)\")\n",
    "\n",
    "# Check if we're running in Fabric environment\n",
    "try:\n",
    "    spark_version = spark.version\n",
    "    print(f\"üè≠ Fabric Spark session detected: {spark_version}\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è  Spark session not available - ensure you're running in Fabric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric Environment Diagnostic Check\n",
    "print(\"üîç FABRIC ENVIRONMENT DIAGNOSTIC\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check 1: Spark session\n",
    "try:\n",
    "    print(f\"‚úÖ Spark session active: {spark.version}\")\n",
    "except:\n",
    "    print(\"‚ùå Spark session not available\")\n",
    "\n",
    "# Check 2: Available databases\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\").toPandas()\n",
    "    print(f\"‚úÖ Available databases ({len(databases)}):\")\n",
    "    for db in databases['databaseName']:\n",
    "        print(f\"   üìÅ {db}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cannot list databases: {str(e)}\")\n",
    "\n",
    "# Check 3: Lakehouse attachment\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    print(f\"‚úÖ Lakehouse tables visible ({len(tables)}):\")\n",
    "    for table in tables['tableName'][:5]:  # Show first 5\n",
    "        print(f\"   üìä {table}\")\n",
    "    if len(tables) > 5:\n",
    "        print(f\"   ... and {len(tables)-5} more\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cannot access lakehouse tables: {str(e)}\")\n",
    "\n",
    "# Check 4: File system access\n",
    "try:\n",
    "    files_path = \"Files\"\n",
    "    if os.path.exists(files_path):\n",
    "        print(f\"‚úÖ File system access: {files_path} exists\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  File system: {files_path} not found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå File system access failed: {str(e)}\")\n",
    "\n",
    "# Check 5: SQL database access (will be checked after database name is defined)\n",
    "print(\"‚è≥ SQL Database access check will run after database configuration...\")\n",
    "\n",
    "print(\"\\nüìã Diagnostic complete. Address any ‚ùå issues before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration for Fabric environment\n",
    "# Using Fabric's built-in Spark SQL connectivity\n",
    "database_name = \"Gaiye-SQL-DB\"\n",
    "schema_name = \"SalesLT\"\n",
    "\n",
    "print(f\"üîó Connecting to Fabric SQL database: {database_name}\")\n",
    "print(f\"üìä Target schema: {schema_name}\")\n",
    "print(f\"üîê Authentication: Fabric built-in connectivity\")\n",
    "\n",
    "# Test basic Spark SQL functionality\n",
    "try:\n",
    "    # Simple test query\n",
    "    test_df = spark.sql(\"SELECT 1 as test_connection\").toPandas()\n",
    "    print(\"‚úÖ Fabric Spark SQL connection successful\")\n",
    "    \n",
    "    # Now test SQL database access\n",
    "    try:\n",
    "        test_query = f\"SELECT COUNT(*) as table_count FROM {database_name}.information_schema.tables\"\n",
    "        result = spark.sql(test_query).toPandas()\n",
    "        table_count = result.iloc[0]['table_count']\n",
    "        print(f\"‚úÖ SQL Database access successful: {database_name} has {table_count} tables\")\n",
    "    except Exception as db_error:\n",
    "        print(f\"‚ö†Ô∏è  SQL Database access issue: {str(db_error)}\")\n",
    "        print(\"üí° Trying alternative database access patterns...\")\n",
    "        \n",
    "        # Try without database prefix\n",
    "        try:\n",
    "            alt_query = \"SELECT COUNT(*) as table_count FROM information_schema.tables\"\n",
    "            result = spark.sql(alt_query).toPandas()\n",
    "            table_count = result.iloc[0]['table_count']\n",
    "            print(f\"‚úÖ Alternative database access successful: Found {table_count} tables\")\n",
    "        except Exception as alt_error:\n",
    "            print(f\"‚ùå Alternative database access failed: {str(alt_error)}\")\n",
    "            print(\"üí° Please ensure:\")\n",
    "            print(f\"   1. {database_name} is linked to this Fabric workspace\")\n",
    "            print(\"   2. This notebook has a lakehouse attached\")\n",
    "            print(\"   3. You have proper permissions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Basic Spark SQL test failed: {str(e)}\")\n",
    "    print(\"üí° Ensure you're running this notebook in Fabric with Spark enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb666754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all tables in SalesLT schema using Fabric SQL\n",
    "print(\"üîç Discovering tables in SalesLT schema using Fabric connectivity...\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Try using Fabric SQL Warehouse query\n",
    "    table_discovery_query = f\"\"\"\n",
    "    SELECT table_name as TABLE_NAME\n",
    "    FROM {database_name}.information_schema.tables \n",
    "    WHERE table_schema = '{schema_name}' \n",
    "    AND table_type = 'BASE TABLE'\n",
    "    ORDER BY table_name\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\udcdd Executing query: {table_discovery_query}\")\n",
    "    \n",
    "    # Execute using Spark SQL\n",
    "    tables_spark_df = spark.sql(table_discovery_query)\n",
    "    tables_df = tables_spark_df.toPandas()\n",
    "    \n",
    "    # Extract table names\n",
    "    table_names = tables_df['TABLE_NAME'].tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(table_names)} tables in {schema_name} schema:\")\n",
    "    for i, table in enumerate(table_names, 1):\n",
    "        print(f\"   {i}. {table}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fabric SQL query failed: {str(e)}\")\n",
    "    print(\"üîÑ Trying alternative method...\")\n",
    "    \n",
    "    try:\n",
    "        # Method 2: Try direct database access pattern\n",
    "        alternative_query = f\"\"\"\n",
    "        SELECT table_name as TABLE_NAME\n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = '{schema_name}' \n",
    "        AND table_type = 'BASE TABLE'\n",
    "        ORDER BY table_name\n",
    "        \"\"\"\n",
    "        \n",
    "        # Try without database prefix\n",
    "        tables_spark_df = spark.sql(alternative_query)\n",
    "        tables_df = tables_spark_df.toPandas()\n",
    "        table_names = tables_df['TABLE_NAME'].tolist()\n",
    "        \n",
    "        print(f\"‚úÖ Alternative method successful - Found {len(table_names)} tables:\")\n",
    "        for i, table in enumerate(table_names, 1):\n",
    "            print(f\"   {i}. {table}\")\n",
    "            \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Alternative method also failed: {str(e2)}\")\n",
    "        print(\"üí° Manual table list fallback...\")\n",
    "        \n",
    "        # Fallback: Common SalesLT tables (manual list as last resort)\n",
    "        table_names = [\n",
    "            'Address',\n",
    "            'Customer', \n",
    "            'CustomerAddress',\n",
    "            'Product',\n",
    "            'ProductCategory',\n",
    "            'ProductDescription',\n",
    "            'ProductModel',\n",
    "            'ProductModelProductDescription',\n",
    "            'SalesOrderDetail',\n",
    "            'SalesOrderHeader'\n",
    "        ]\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è  Using fallback table list ({len(table_names)} common SalesLT tables):\")\n",
    "        for i, table in enumerate(table_names, 1):\n",
    "            print(f\"   {i}. {table}\")\n",
    "        print(\"üí° Note: This is a fallback list - actual schema may have different tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48900e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enhanced export function using Fabric Spark SQL\n",
    "def export_table_to_bronze_fabric(table_name, database_name, schema_name=\"SalesLT\"):\n",
    "    \"\"\"\n",
    "    Export a single table to bronze layer using Fabric's Spark SQL connectivity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting Fabric export for table: {table_name}\")\n",
    "        \n",
    "        # Method 1: Try with database prefix\n",
    "        try:\n",
    "            query = f\"SELECT * FROM {database_name}.{schema_name}.{table_name}\"\n",
    "            print(f\"üîç Trying query: {query}\")\n",
    "            \n",
    "            # Execute using Spark SQL\n",
    "            spark_df = spark.sql(query)\n",
    "            df = spark_df.toPandas()\n",
    "            \n",
    "        except Exception as e1:\n",
    "            print(f\"‚ö†Ô∏è  Database prefix failed, trying alternative: {str(e1)}\")\n",
    "            \n",
    "            # Method 2: Try without database prefix\n",
    "            try:\n",
    "                query = f\"SELECT * FROM {schema_name}.{table_name}\"\n",
    "                print(f\"üîç Trying alternative query: {query}\")\n",
    "                \n",
    "                spark_df = spark.sql(query)\n",
    "                df = spark_df.toPandas()\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"‚ö†Ô∏è  Schema prefix failed, trying table only: {str(e2)}\")\n",
    "                \n",
    "                # Method 3: Try table name only (if schema is default)\n",
    "                query = f\"SELECT * FROM {table_name}\"\n",
    "                print(f\"üîç Trying table-only query: {query}\")\n",
    "                \n",
    "                spark_df = spark.sql(query)\n",
    "                df = spark_df.toPandas()\n",
    "        \n",
    "        # Handle data type conversions for Parquet compatibility\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                # Convert object columns to string to handle mixed types\n",
    "                df[col] = df[col].astype(str)\n",
    "            elif 'datetime' in str(df[col].dtype):\n",
    "                # Ensure datetime columns are properly formatted\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        # Add comprehensive metadata\n",
    "        df['_source_table'] = f\"{schema_name}.{table_name}\"\n",
    "        df['_extraction_timestamp'] = datetime.now()\n",
    "        df['_source_database'] = database_name\n",
    "        df['_extraction_method'] = 'fabric_spark_sql'\n",
    "        df['_record_count'] = len(df)\n",
    "        \n",
    "        # Create bronze layer directory structure\n",
    "        bronze_path = f\"Files/bronze/saleslt/{table_name.lower()}\"\n",
    "        os.makedirs(bronze_path, exist_ok=True)\n",
    "        \n",
    "        # Save to bronze layer as Parquet\n",
    "        parquet_file = f\"{bronze_path}/{table_name.lower()}.parquet\"\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        \n",
    "        # Also save schema information\n",
    "        schema_info = {\n",
    "            'table_name': table_name,\n",
    "            'column_count': len(df.columns),\n",
    "            'columns': [{'name': col, 'dtype': str(df[col].dtype)} for col in df.columns],\n",
    "            'extraction_timestamp': datetime.now().isoformat(),\n",
    "            'query_used': query\n",
    "        }\n",
    "        \n",
    "        with open(f\"{bronze_path}/schema_info.json\", 'w') as f:\n",
    "            import json\n",
    "            json.dump(schema_info, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Successfully exported {table_name}: {len(df)} rows\")\n",
    "        \n",
    "        return {\n",
    "            'table_name': table_name,\n",
    "            'status': 'success',\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'columns': list(df.columns),\n",
    "            'bronze_path': bronze_path,\n",
    "            'file_size_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),\n",
    "            'parquet_file': parquet_file,\n",
    "            'query_used': query\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error exporting {table_name}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        \n",
    "        return {\n",
    "            'table_name': table_name,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'row_count': 0\n",
    "        }\n",
    "\n",
    "print(\"üìù Fabric-compatible export function defined\")\n",
    "print(\"üîß Features: Spark SQL connectivity, multiple query patterns, fallback methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute export for all discovered tables using Fabric connectivity\n",
    "print(f\"üöÄ Starting Fabric-based export of {len(table_names)} tables to bronze layer...\\n\")\n",
    "\n",
    "export_results = []\n",
    "successful_exports = 0\n",
    "failed_exports = 0\n",
    "\n",
    "for i, table_name in enumerate(table_names, 1):\n",
    "    print(f\"üìä Processing table {i}/{len(table_names)}: {table_name}\")\n",
    "    \n",
    "    # Export table using Fabric method\n",
    "    result = export_table_to_bronze_fabric(table_name, database_name, schema_name)\n",
    "    export_results.append(result)\n",
    "    \n",
    "    # Track success/failure\n",
    "    if result['status'] == 'success':\n",
    "        successful_exports += 1\n",
    "        print(f\"   ‚úÖ {result['row_count']} rows exported ({result.get('file_size_mb', 0)} MB)\")\n",
    "        if 'query_used' in result:\n",
    "            print(f\"   üìù Query: {result['query_used']}\")\n",
    "    else:\n",
    "        failed_exports += 1\n",
    "        print(f\"   ‚ùå Export failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print()  # Empty line for readability\n",
    "\n",
    "print(f\"üéØ Fabric export completed!\")\n",
    "print(f\"‚úÖ Successful: {successful_exports} tables\")\n",
    "print(f\"‚ùå Failed: {failed_exports} tables\")\n",
    "\n",
    "# If all exports failed, provide troubleshooting guidance\n",
    "if failed_exports == len(table_names) and len(table_names) > 0:\n",
    "    print(\"\\nüö® ALL EXPORTS FAILED - TROUBLESHOOTING GUIDE:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. ‚úÖ Verify Gaiye-SQL-DB is linked to this Fabric workspace\")\n",
    "    print(\"2. ‚úÖ Ensure this notebook has a lakehouse attached\")\n",
    "    print(\"3. ‚úÖ Check that you have read permissions on the SQL database\")\n",
    "    print(\"4. ‚úÖ Confirm the SalesLT schema exists in Gaiye-SQL-DB\")\n",
    "    print(\"5. ‚úÖ Try running: spark.sql('SHOW DATABASES').show() to see available databases\")\n",
    "    print(\"\\nüí° Alternative: Use Data Factory pipeline instead of notebook for better connectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed export summary and pipeline configuration\n",
    "print(\"üìã EXPORT SUMMARY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Source Database: {database_name}\")\n",
    "print(f\"Source Schema: {schema_name}\")\n",
    "print(f\"Destination: Bronze Layer (Files/bronze/saleslt/)\")\n",
    "print()\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(export_results)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    # Display successful exports\n",
    "    successful_df = summary_df[summary_df['status'] == 'success']\n",
    "    if not successful_df.empty:\n",
    "        print(\"‚úÖ SUCCESSFUL EXPORTS:\")\n",
    "        print(successful_df[['table_name', 'row_count', 'column_count', 'file_size_mb']].to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "        total_rows = successful_df['row_count'].sum()\n",
    "        total_size = successful_df['file_size_mb'].sum()\n",
    "        total_tables = len(successful_df)\n",
    "        print(f\"üìä Total tables exported: {total_tables}\")\n",
    "        print(f\"üìä Total rows exported: {total_rows:,}\")\n",
    "        print(f\"üíæ Total data size: {total_size:.2f} MB\")\n",
    "        print()\n",
    "    \n",
    "    # Display failed exports\n",
    "    failed_df = summary_df[summary_df['status'] == 'failed']\n",
    "    if not failed_df.empty:\n",
    "        print(\"‚ùå FAILED EXPORTS:\")\n",
    "        print(failed_df[['table_name', 'error']].to_string(index=False))\n",
    "        print()\n",
    "\n",
    "# Create comprehensive summary report\n",
    "summary_report = {\n",
    "    'pipeline_info': {\n",
    "        'name': 'SalesLT Dynamic Export Pipeline',\n",
    "        'type': 'schema_discovery_export',\n",
    "        'authentication': 'fabric_spark_sql',\n",
    "        'method': 'self_contained_notebook'\n",
    "    },\n",
    "    'export_metadata': {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'source_database': database_name,\n",
    "        'source_schema': schema_name,\n",
    "        'destination_path': 'Files/bronze/saleslt/',\n",
    "        'total_tables_discovered': len(table_names),\n",
    "        'successful_exports': successful_exports,\n",
    "        'failed_exports': failed_exports\n",
    "    },\n",
    "    'table_inventory': table_names,\n",
    "    'export_details': export_results,\n",
    "    'data_factory_integration': {\n",
    "        'server_format': f'{database_name}.sql.fabric.microsoft.com',\n",
    "        'authentication_type': 'organizational_account',\n",
    "        'bronze_path_pattern': 'Files/bronze/saleslt/{table_name_lower}/{table_name_lower}.parquet',\n",
    "        'metadata_pattern': 'Files/bronze/saleslt/{table_name_lower}/schema_info.json'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary as JSON in bronze layer\n",
    "summary_path = \"Files/bronze/saleslt/_pipeline_summary.json\"\n",
    "os.makedirs(\"Files/bronze/saleslt\", exist_ok=True)\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìÑ Pipeline summary saved to: {summary_path}\")\n",
    "print(\"\\nüéâ SalesLT dynamic schema export to bronze layer completed!\")\n",
    "print(\"üîó Ready for Data Factory integration or silver layer processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Verify files exist in bronze layer\n",
    "print(\"üîç VALIDATION: Checking bronze layer contents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "bronze_base_path = \"Files/bronze\"\n",
    "\n",
    "try:\n",
    "    # List bronze layer directories\n",
    "    if os.path.exists(bronze_base_path):\n",
    "        bronze_contents = os.listdir(bronze_base_path)\n",
    "        \n",
    "        print(f\"üìÅ Bronze layer contains {len(bronze_contents)} items:\")\n",
    "        for item in sorted(bronze_contents):\n",
    "            item_path = os.path.join(bronze_base_path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                # Check for parquet files in the directory\n",
    "                files = [f for f in os.listdir(item_path) if f.endswith('.parquet')]\n",
    "                print(f\"   üìÇ {item}/ ({len(files)} parquet files)\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {item}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Bronze layer path not found: {bronze_base_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error validating bronze layer: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Validation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ef76d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this notebook, you should have:\n",
    "\n",
    "1. **Exported Data**: All SalesLT tables saved as Parquet files in the bronze layer\n",
    "2. **Metadata**: Each file includes source tracking and extraction timestamps\n",
    "3. **Summary Report**: JSON file with complete export details\n",
    "4. **File Organization**: Each table in its own folder within `/Files/bronze/`\n",
    "\n",
    "### Recommended Next Actions:\n",
    "\n",
    "- **Review the data**: Check the bronze layer files to ensure data quality\n",
    "- **Create silver layer transformations**: Build data pipelines to clean and standardize the data\n",
    "- **Map to retail model**: Align the SalesLT schema with your retail data model requirements\n",
    "- **Set up monitoring**: Create alerts for data freshness and quality\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **Connection issues**: Verify your Fabric workspace has access to the SQL database\n",
    "- **Permission errors**: Ensure you have read access to SalesLT schema and write access to the lakehouse\n",
    "- **Large tables**: For very large tables, consider implementing chunked processing\n",
    "- **Data types**: Some SQL Server data types may need special handling during export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d9584",
   "metadata": {},
   "source": [
    "## Data Factory Pipeline Alternative\n",
    "\n",
    "If you prefer using **Data Factory** instead of this notebook, here's how to create a dynamic pipeline:\n",
    "\n",
    "### 1. Create Parameterized Pipeline\n",
    "\n",
    "Create a Data Factory pipeline with these components:\n",
    "\n",
    "**Pipeline Parameters:**\n",
    "- `sourceSchema` = \"SalesLT\"\n",
    "- `targetPath` = \"Files/bronze/saleslt\"\n",
    "\n",
    "**Activities:**\n",
    "1. **Lookup Activity**: Get table list from `INFORMATION_SCHEMA.TABLES`\n",
    "2. **ForEach Activity**: Loop through discovered tables  \n",
    "3. **Copy Data Activity**: Copy each table dynamically\n",
    "\n",
    "### 2. Connection Configuration\n",
    "\n",
    "**Source (Fabric SQL Database):**\n",
    "- **Server**: `Gaiye-SQL-DB.sql.fabric.microsoft.com`\n",
    "- **Database**: `Gaiye-SQL-DB`\n",
    "- **Authentication**: `Organizational Account`\n",
    "\n",
    "**Sink (Lakehouse):**\n",
    "- **Path**: `@concat(pipeline().parameters.targetPath, '/', toLower(item().TABLE_NAME), '/', toLower(item().TABLE_NAME), '.parquet')`\n",
    "- **Format**: `Parquet`\n",
    "\n",
    "### 3. Dynamic Query\n",
    "\n",
    "**Lookup Query:**\n",
    "```sql\n",
    "SELECT TABLE_NAME \n",
    "FROM INFORMATION_SCHEMA.TABLES \n",
    "WHERE TABLE_SCHEMA = '@{pipeline().parameters.sourceSchema}' \n",
    "AND TABLE_TYPE = 'BASE TABLE'\n",
    "```\n",
    "\n",
    "**Copy Data Source Query:**\n",
    "```sql\n",
    "SELECT * FROM [@{pipeline().parameters.sourceSchema}].[@{item().TABLE_NAME}]\n",
    "```\n",
    "\n",
    "This approach gives you the same dynamic table discovery but through Data Factory's visual interface."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
