{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2d1fff",
   "metadata": {},
   "source": [
    "# Microsoft Fabric SQL Database Connection Troubleshooting\n",
    "\n",
    "## Overview\n",
    "This notebook helps diagnose and resolve connection issues with **Gaiye-SQL-DB** in Microsoft Fabric, then demonstrates the complete retail data pipeline implementation.\n",
    "\n",
    "### What This Notebook Does:\n",
    "1. **üîç Diagnose Connection Issues**: Test various connection methods and identify problems\n",
    "2. **üîß Fix Common Problems**: Provide solutions for typical Fabric SQL connectivity issues  \n",
    "3. **üìä Validate Data Access**: Verify you can access SalesLT schema tables\n",
    "4. **üöÄ Run Complete Pipeline**: Execute Bronze-Silver-Gold medallion architecture\n",
    "5. **‚úÖ Export Results**: Save data in multiple formats for Fabric deployment\n",
    "\n",
    "### Prerequisites:\n",
    "- Microsoft Fabric workspace access\n",
    "- Gaiye-SQL-DB permissions (read access to SalesLT schema)\n",
    "- Retail data model lakehouse attached to this notebook\n",
    "- Python libraries: pandas, pyodbc, numpy\n",
    "\n",
    "---\n",
    "\n",
    "**üö® Start Here If:**\n",
    "- You can't see Gaiye-SQL-DB in your workspace\n",
    "- Getting connection timeout errors\n",
    "- Authentication failures when accessing SQL database\n",
    "- Need to verify SalesLT schema access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041dbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and setup\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the retail pipeline class\n",
    "try:\n",
    "    from fabric_retail_pipeline import FabricRetailDataPipeline\n",
    "    print(\"‚úÖ Successfully imported FabricRetailDataPipeline\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import FabricRetailDataPipeline: {e}\")\n",
    "    print(\"üìù Note: The pipeline will be created inline if import fails\")\n",
    "\n",
    "# Display system information\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")\n",
    "print(f\"üìÖ Current timestamp: {datetime.now()}\")\n",
    "\n",
    "# Check if we're in Fabric environment\n",
    "try:\n",
    "    import notebookutils\n",
    "    print(\"‚úÖ Running in Microsoft Fabric environment\")\n",
    "    FABRIC_ENV = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Not detected as Fabric environment - using local setup\")\n",
    "    FABRIC_ENV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Server Connection Configuration\n",
    "print(\"üîß STEP 1: Configure SQL Server Connection Parameters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Database configuration\n",
    "DATABASE_NAME = \"Gaiye-SQL-DB\"\n",
    "SCHEMA_NAME = \"SalesLT\"\n",
    "SERVER_NAME = f\"{DATABASE_NAME}.sql.fabric.microsoft.com\"\n",
    "\n",
    "# Multiple connection string options to try\n",
    "CONNECTION_STRINGS = {\n",
    "    \"fabric_integrated\": f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER=tcp:{SERVER_NAME},1433;DATABASE={DATABASE_NAME};Authentication=ActiveDirectoryIntegrated;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\",\n",
    "    \n",
    "    \"fabric_default\": f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER=tcp:{SERVER_NAME},1433;DATABASE={DATABASE_NAME};Authentication=ActiveDirectoryDefault;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\",\n",
    "    \n",
    "    \"fabric_basic\": f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={SERVER_NAME};DATABASE={DATABASE_NAME};Trusted_Connection=yes;Encrypt=yes;Connection Timeout=30;\",\n",
    "    \n",
    "    \"local_test\": f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER=tcp:{SERVER_NAME},1433;DATABASE={DATABASE_NAME};Encrypt=yes;Connection Timeout=60;\"\n",
    "}\n",
    "\n",
    "print(f\"üìä Target Database: {DATABASE_NAME}\")\n",
    "print(f\"üîç Target Schema: {SCHEMA_NAME}\")\n",
    "print(f\"üåê Server Endpoint: {SERVER_NAME}\")\n",
    "print(f\"üîó Connection strings prepared: {len(CONNECTION_STRINGS)}\")\n",
    "\n",
    "# Function to test connection\n",
    "def test_connection(conn_string, description):\n",
    "    \"\"\"Test a specific connection string\"\"\"\n",
    "    try:\n",
    "        print(f\"\\nüîç Testing: {description}\")\n",
    "        conn = pyodbc.connect(conn_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Test basic connectivity\n",
    "        cursor.execute(\"SELECT @@VERSION\")\n",
    "        version = cursor.fetchone()[0]\n",
    "        \n",
    "        # Test database access\n",
    "        cursor.execute(\"SELECT DB_NAME()\")\n",
    "        db_name = cursor.fetchone()[0]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"‚úÖ SUCCESS: Connected to {db_name}\")\n",
    "        print(f\"   SQL Server: {version[:50]}...\")\n",
    "        return True, conn_string\n",
    "        \n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"‚ùå FAILED: {str(e)}\")\n",
    "        return False, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: {str(e)}\")\n",
    "        return False, None\n",
    "\n",
    "print(f\"\\nüöÄ Testing {len(CONNECTION_STRINGS)} connection methods...\")\n",
    "successful_connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8af0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all connection methods\n",
    "print(\"üîç STEP 2: Test Database Connectivity\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "successful_connection = None\n",
    "working_conn_string = None\n",
    "\n",
    "for method, conn_string in CONNECTION_STRINGS.items():\n",
    "    success, working_string = test_connection(conn_string, method)\n",
    "    if success:\n",
    "        successful_connection = method\n",
    "        working_conn_string = working_string\n",
    "        break\n",
    "\n",
    "if successful_connection:\n",
    "    print(f\"\\nüéâ SUCCESSFUL CONNECTION FOUND!\")\n",
    "    print(f\"‚úÖ Working method: {successful_connection}\")\n",
    "    print(f\"üîó Connection string: {working_conn_string}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå ALL CONNECTION ATTEMPTS FAILED\")\n",
    "    print(f\"\\nüîß TROUBLESHOOTING STEPS:\")\n",
    "    print(f\"1. ‚úîÔ∏è Verify Gaiye-SQL-DB exists in your Fabric workspace\")\n",
    "    print(f\"2. ‚úîÔ∏è Check you have read permissions on the database\")\n",
    "    print(f\"3. ‚úîÔ∏è Confirm you're in the correct Fabric workspace\")\n",
    "    print(f\"4. ‚úîÔ∏è Try refreshing your browser and reconnecting\")\n",
    "    print(f\"5. ‚úîÔ∏è Contact workspace admin for database access\")\n",
    "    \n",
    "    # Continue with a fallback connection for demonstration\n",
    "    working_conn_string = CONNECTION_STRINGS[\"fabric_integrated\"]\n",
    "    print(f\"\\n‚ö†Ô∏è Using fallback connection for demonstration purposes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c443675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available schemas and tables\n",
    "print(\"üîç STEP 3: Discover Database Schema and Tables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def discover_database_structure(conn_string):\n",
    "    \"\"\"Discover schemas, tables, and columns in the database\"\"\"\n",
    "    try:\n",
    "        conn = pyodbc.connect(conn_string)\n",
    "        \n",
    "        # Get all schemas\n",
    "        schema_query = \"\"\"\n",
    "        SELECT DISTINCT SCHEMA_NAME \n",
    "        FROM INFORMATION_SCHEMA.SCHEMATA \n",
    "        WHERE SCHEMA_NAME NOT IN ('sys', 'INFORMATION_SCHEMA')\n",
    "        ORDER BY SCHEMA_NAME\n",
    "        \"\"\"\n",
    "        schemas_df = pd.read_sql(schema_query, conn)\n",
    "        \n",
    "        print(f\"üìö Available Schemas ({len(schemas_df)}):\")\n",
    "        for schema in schemas_df['SCHEMA_NAME']:\n",
    "            print(f\"   ‚Ä¢ {schema}\")\n",
    "        \n",
    "        # Get tables in SalesLT schema specifically\n",
    "        saleslt_query = \"\"\"\n",
    "        SELECT TABLE_NAME, TABLE_TYPE\n",
    "        FROM INFORMATION_SCHEMA.TABLES \n",
    "        WHERE TABLE_SCHEMA = 'SalesLT'\n",
    "        ORDER BY TABLE_NAME\n",
    "        \"\"\"\n",
    "        \n",
    "        saleslt_tables = pd.read_sql(saleslt_query, conn)\n",
    "        \n",
    "        if len(saleslt_tables) > 0:\n",
    "            print(f\"\\nüìä SalesLT Schema Tables ({len(saleslt_tables)}):\")\n",
    "            for idx, row in saleslt_tables.iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['TABLE_NAME']} ({row['TABLE_TYPE']})\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå No tables found in SalesLT schema\")\n",
    "            \n",
    "            # Check all tables to see what's available\n",
    "            all_tables_query = \"\"\"\n",
    "            SELECT TABLE_SCHEMA, TABLE_NAME, TABLE_TYPE\n",
    "            FROM INFORMATION_SCHEMA.TABLES \n",
    "            WHERE TABLE_TYPE = 'BASE TABLE'\n",
    "            ORDER BY TABLE_SCHEMA, TABLE_NAME\n",
    "            \"\"\"\n",
    "            all_tables = pd.read_sql(all_tables_query, conn)\n",
    "            print(f\"\\nüìã All Available Tables ({len(all_tables)}):\")\n",
    "            for idx, row in all_tables.iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['TABLE_SCHEMA']}.{row['TABLE_NAME']}\")\n",
    "        \n",
    "        # Get sample data from first SalesLT table if available\n",
    "        if len(saleslt_tables) > 0:\n",
    "            first_table = saleslt_tables.iloc[0]['TABLE_NAME']\n",
    "            sample_query = f\"SELECT TOP 3 * FROM SalesLT.{first_table}\"\n",
    "            \n",
    "            try:\n",
    "                sample_data = pd.read_sql(sample_query, conn)\n",
    "                print(f\"\\nüìÑ Sample Data from SalesLT.{first_table}:\")\n",
    "                print(sample_data.to_string(index=False))\n",
    "                print(f\"   Columns: {list(sample_data.columns)}\")\n",
    "                print(f\"   Data types: {sample_data.dtypes.to_dict()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not fetch sample data: {e}\")\n",
    "        \n",
    "        conn.close()\n",
    "        return saleslt_tables\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error discovering database structure: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Run discovery\n",
    "if working_conn_string:\n",
    "    discovered_tables = discover_database_structure(working_conn_string)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping discovery - no working connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff79341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sample data for retail pipeline\n",
    "print(\"üìä STEP 4: Extract Sample Data for Retail Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_saleslt_data(conn_string):\n",
    "    \"\"\"Extract key tables from SalesLT schema for retail pipeline\"\"\"\n",
    "    \n",
    "    # Define the core tables we need for retail analytics\n",
    "    core_tables = {\n",
    "        'customers': 'SalesLT.Customer',\n",
    "        'products': 'SalesLT.Product', \n",
    "        'orders': 'SalesLT.SalesOrderHeader',\n",
    "        'order_details': 'SalesLT.SalesOrderDetail',\n",
    "        'addresses': 'SalesLT.Address',\n",
    "        'product_categories': 'SalesLT.ProductCategory'\n",
    "    }\n",
    "    \n",
    "    extracted_data = {}\n",
    "    \n",
    "    try:\n",
    "        conn = pyodbc.connect(conn_string)\n",
    "        \n",
    "        for table_alias, table_name in core_tables.items():\n",
    "            try:\n",
    "                print(f\"üì• Extracting {table_alias} from {table_name}\")\n",
    "                \n",
    "                # Use SELECT TOP for initial testing\n",
    "                query = f\"SELECT TOP 100 * FROM {table_name}\"\n",
    "                df = pd.read_sql(query, conn)\n",
    "                \n",
    "                # Add extraction metadata\n",
    "                df['_extracted_at'] = datetime.now()\n",
    "                df['_source_table'] = table_name\n",
    "                df['_extraction_method'] = 'direct_sql'\n",
    "                \n",
    "                extracted_data[table_alias] = df\n",
    "                \n",
    "                print(f\"   ‚úÖ Success: {len(df)} rows, {len(df.columns)} columns\")\n",
    "                \n",
    "                # Show sample of first few rows\n",
    "                if len(df) > 0:\n",
    "                    print(f\"   üìã Sample columns: {list(df.columns[:5])}\")\n",
    "                    if len(df.columns) > 5:\n",
    "                        print(f\"      ... and {len(df.columns) - 5} more columns\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to extract {table_alias}: {e}\")\n",
    "                # Create empty DataFrame with metadata for failed extractions\n",
    "                extracted_data[table_alias] = pd.DataFrame({\n",
    "                    '_extracted_at': [datetime.now()],\n",
    "                    '_source_table': [table_name],\n",
    "                    '_extraction_method': ['failed'],\n",
    "                    '_error': [str(e)]\n",
    "                })\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Summary of extraction\n",
    "        print(f\"\\nüìà EXTRACTION SUMMARY:\")\n",
    "        total_rows = 0\n",
    "        successful_tables = 0\n",
    "        \n",
    "        for table_name, df in extracted_data.items():\n",
    "            row_count = len(df)\n",
    "            if '_error' not in df.columns:\n",
    "                total_rows += row_count\n",
    "                successful_tables += 1\n",
    "                status = \"‚úÖ\"\n",
    "            else:\n",
    "                status = \"‚ùå\"\n",
    "            \n",
    "            print(f\"   {status} {table_name}: {row_count} rows\")\n",
    "        \n",
    "        print(f\"\\nüéØ Successfully extracted {successful_tables}/{len(core_tables)} tables\")\n",
    "        print(f\"üìä Total rows extracted: {total_rows:,}\")\n",
    "        \n",
    "        return extracted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fatal error during extraction: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Run data extraction\n",
    "if working_conn_string:\n",
    "    print(\"üöÄ Starting data extraction...\")\n",
    "    retail_data = extract_saleslt_data(working_conn_string)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping data extraction - no working connection\")\n",
    "    retail_data = {}\n",
    "    \n",
    "    # Create mock data for pipeline demonstration\n",
    "    print(\"üé≠ Creating mock data for pipeline demonstration...\")\n",
    "    retail_data = {\n",
    "        'customers': pd.DataFrame({\n",
    "            'CustomerID': range(1, 6),\n",
    "            'FirstName': ['John', 'Jane', 'Bob', 'Alice', 'Charlie'],\n",
    "            'LastName': ['Doe', 'Smith', 'Johnson', 'Williams', 'Brown'],\n",
    "            'EmailAddress': ['john@email.com', 'jane@email.com', 'bob@email.com', 'alice@email.com', 'charlie@email.com'],\n",
    "            '_extracted_at': [datetime.now()] * 5,\n",
    "            '_source_table': ['mock_data'] * 5,\n",
    "            '_extraction_method': ['mock'] * 5\n",
    "        }),\n",
    "        'products': pd.DataFrame({\n",
    "            'ProductID': range(1, 6),\n",
    "            'Name': ['Product A', 'Product B', 'Product C', 'Product D', 'Product E'],\n",
    "            'ListPrice': [29.99, 49.99, 79.99, 99.99, 129.99],\n",
    "            '_extracted_at': [datetime.now()] * 5,\n",
    "            '_source_table': ['mock_data'] * 5,\n",
    "            '_extraction_method': ['mock'] * 5\n",
    "        })\n",
    "    }\n",
    "    print(\"‚úÖ Mock data created for pipeline testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0236d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Fabric Retail Pipeline\n",
    "print(\"üè≠ STEP 5: Initialize Microsoft Fabric Retail Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simplified version of the pipeline for SQL data\n",
    "class SQLFabricRetailPipeline:\n",
    "    \"\"\"\n",
    "    Simplified retail pipeline for SQL Server data source\n",
    "    Implements Bronze -> Silver -> Gold medallion architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bronze_data = {}\n",
    "        self.silver_data = {}\n",
    "        self.gold_data = {}\n",
    "        self.pipeline_metadata = {\n",
    "            'started_at': datetime.now(),\n",
    "            'source_type': 'sql_server',\n",
    "            'database_name': DATABASE_NAME,\n",
    "            'schema_name': SCHEMA_NAME\n",
    "        }\n",
    "        \n",
    "    def ingest_bronze_from_sql(self, sql_data_dict):\n",
    "        \"\"\"Load SQL extracted data into Bronze layer\"\"\"\n",
    "        print(\"üì• Loading data into Bronze layer...\")\n",
    "        \n",
    "        ingestion_time = datetime.now().isoformat()\n",
    "        \n",
    "        for table_name, df in sql_data_dict.items():\n",
    "            if len(df) > 0 and '_error' not in df.columns:\n",
    "                # Add bronze layer metadata\n",
    "                bronze_df = df.copy()\n",
    "                bronze_df['_bronze_ingestion_timestamp'] = ingestion_time\n",
    "                bronze_df['_bronze_record_id'] = range(1, len(bronze_df) + 1)\n",
    "                bronze_df['_bronze_processing_status'] = 'ingested'\n",
    "                \n",
    "                self.bronze_data[table_name] = bronze_df\n",
    "                print(f\"   ‚úÖ {table_name}: {len(bronze_df)} rows ingested\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è {table_name}: Skipped due to errors or empty data\")\n",
    "        \n",
    "        print(f\"\\nüéØ Bronze layer complete: {len(self.bronze_data)} tables loaded\")\n",
    "        return len(self.bronze_data) > 0\n",
    "    \n",
    "    def transform_to_silver_basic(self):\n",
    "        \"\"\"Basic Silver layer transformations\"\"\"\n",
    "        print(\"üîß Transforming to Silver layer...\")\n",
    "        \n",
    "        for table_name, df in self.bronze_data.items():\n",
    "            print(f\"   üîÑ Processing {table_name}\")\n",
    "            \n",
    "            silver_df = df.copy()\n",
    "            \n",
    "            # Basic data cleaning\n",
    "            # Remove columns that are just metadata from extraction\n",
    "            metadata_cols = [col for col in silver_df.columns if col.startswith('_') and not col.startswith('_bronze')]\n",
    "            silver_df = silver_df.drop(columns=metadata_cols, errors='ignore')\n",
    "            \n",
    "            # Add silver layer metadata\n",
    "            silver_df['_silver_processed_timestamp'] = datetime.now().isoformat()\n",
    "            silver_df['_silver_data_quality_score'] = 100.0  # Placeholder\n",
    "            \n",
    "            # Basic data type inference and cleaning\n",
    "            for col in silver_df.columns:\n",
    "                if not col.startswith('_'):\n",
    "                    # Try to infer better data types\n",
    "                    if silver_df[col].dtype == 'object':\n",
    "                        # Check if it's numeric\n",
    "                        try:\n",
    "                            numeric_col = pd.to_numeric(silver_df[col], errors='coerce')\n",
    "                            if not numeric_col.isna().all():\n",
    "                                silver_df[col] = numeric_col\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        # Check if it's datetime\n",
    "                        try:\n",
    "                            date_col = pd.to_datetime(silver_df[col], errors='coerce')\n",
    "                            if not date_col.isna().all():\n",
    "                                silver_df[col] = date_col\n",
    "                        except:\n",
    "                            pass\n",
    "            \n",
    "            self.silver_data[table_name] = silver_df\n",
    "            print(f\"      ‚úÖ Cleaned {len(silver_df)} rows, {len(silver_df.columns)} columns\")\n",
    "        \n",
    "        print(f\"\\nüéØ Silver layer complete: {len(self.silver_data)} tables processed\")\n",
    "    \n",
    "    def create_gold_analytics_basic(self):\n",
    "        \"\"\"Basic Gold layer analytics\"\"\"\n",
    "        print(\"üíé Creating Gold layer analytics...\")\n",
    "        \n",
    "        # Customer analytics (if customers table exists)\n",
    "        if 'customers' in self.silver_data:\n",
    "            customers_df = self.silver_data['customers']\n",
    "            customer_analytics = pd.DataFrame({\n",
    "                'total_customers': [len(customers_df)],\n",
    "                'analysis_timestamp': [datetime.now().isoformat()],\n",
    "                'data_source': [f\"{DATABASE_NAME}.{SCHEMA_NAME}\"]\n",
    "            })\n",
    "            self.gold_data['customer_summary'] = customer_analytics\n",
    "            print(f\"   ‚úÖ Customer analytics: {len(customers_df)} customers analyzed\")\n",
    "        \n",
    "        # Product analytics (if products table exists) \n",
    "        if 'products' in self.silver_data:\n",
    "            products_df = self.silver_data['products']\n",
    "            product_analytics = pd.DataFrame({\n",
    "                'total_products': [len(products_df)],\n",
    "                'analysis_timestamp': [datetime.now().isoformat()],\n",
    "                'data_source': [f\"{DATABASE_NAME}.{SCHEMA_NAME}\"]\n",
    "            })\n",
    "            \n",
    "            # Basic price analysis if ListPrice column exists\n",
    "            price_cols = [col for col in products_df.columns if 'price' in col.lower()]\n",
    "            if price_cols:\n",
    "                price_col = price_cols[0]\n",
    "                try:\n",
    "                    price_data = pd.to_numeric(products_df[price_col], errors='coerce')\n",
    "                    if not price_data.isna().all():\n",
    "                        product_analytics['avg_price'] = [price_data.mean()]\n",
    "                        product_analytics['min_price'] = [price_data.min()]\n",
    "                        product_analytics['max_price'] = [price_data.max()]\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.gold_data['product_summary'] = product_analytics\n",
    "            print(f\"   ‚úÖ Product analytics: {len(products_df)} products analyzed\")\n",
    "        \n",
    "        # Overall pipeline summary\n",
    "        pipeline_summary = pd.DataFrame({\n",
    "            'pipeline_run_id': [f\"fabric_sql_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"],\n",
    "            'source_database': [DATABASE_NAME],\n",
    "            'source_schema': [SCHEMA_NAME],\n",
    "            'bronze_tables': [len(self.bronze_data)],\n",
    "            'silver_tables': [len(self.silver_data)],\n",
    "            'gold_tables': [len(self.gold_data)],\n",
    "            'total_rows_processed': [sum(len(df) for df in self.silver_data.values())],\n",
    "            'pipeline_completion_time': [datetime.now().isoformat()],\n",
    "            'status': ['completed']\n",
    "        })\n",
    "        \n",
    "        self.gold_data['pipeline_summary'] = pipeline_summary\n",
    "        print(f\"   ‚úÖ Pipeline summary created\")\n",
    "        \n",
    "        print(f\"\\nüéØ Gold layer complete: {len(self.gold_data)} analytics tables created\")\n",
    "\n",
    "# Initialize and run the pipeline\n",
    "print(\"üöÄ Initializing SQL Fabric Retail Pipeline...\")\n",
    "sql_pipeline = SQLFabricRetailPipeline()\n",
    "\n",
    "if retail_data and len(retail_data) > 0:\n",
    "    print(f\"üìä Input data: {len(retail_data)} tables available\")\n",
    "    \n",
    "    # Run Bronze layer\n",
    "    bronze_success = sql_pipeline.ingest_bronze_from_sql(retail_data)\n",
    "    \n",
    "    if bronze_success:\n",
    "        # Run Silver layer\n",
    "        sql_pipeline.transform_to_silver_basic()\n",
    "        \n",
    "        # Run Gold layer\n",
    "        sql_pipeline.create_gold_analytics_basic()\n",
    "        \n",
    "        print(f\"\\nüéâ PIPELINE EXECUTION COMPLETED!\")\n",
    "        print(f\"   üìä Bronze: {len(sql_pipeline.bronze_data)} tables\")\n",
    "        print(f\"   üîß Silver: {len(sql_pipeline.silver_data)} tables\") \n",
    "        print(f\"   üíé Gold: {len(sql_pipeline.gold_data)} tables\")\n",
    "    else:\n",
    "        print(\"‚ùå Pipeline failed at Bronze layer\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for pipeline execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate pipeline results and display insights\n",
    "print(\"üîç STEP 6: Validate Pipeline Results and Display Insights\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def display_pipeline_insights(pipeline):\n",
    "    \"\"\"Display comprehensive insights from the pipeline execution\"\"\"\n",
    "    \n",
    "    if not hasattr(pipeline, 'gold_data') or len(pipeline.gold_data) == 0:\n",
    "        print(\"‚ö†Ô∏è No gold layer data available for insights\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä PIPELINE INSIGHTS DASHBOARD\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Pipeline Summary\n",
    "    if 'pipeline_summary' in pipeline.gold_data:\n",
    "        summary = pipeline.gold_data['pipeline_summary']\n",
    "        print(\"\\nüéØ EXECUTION SUMMARY:\")\n",
    "        for col in summary.columns:\n",
    "            if not col.startswith('_'):\n",
    "                value = summary[col].iloc[0]\n",
    "                print(f\"   {col}: {value}\")\n",
    "    \n",
    "    # Customer Insights\n",
    "    if 'customer_summary' in pipeline.gold_data:\n",
    "        customer_summary = pipeline.gold_data['customer_summary']\n",
    "        print(f\"\\nüë• CUSTOMER INSIGHTS:\")\n",
    "        print(f\"   Total Customers: {customer_summary['total_customers'].iloc[0]:,}\")\n",
    "        \n",
    "        # Display sample customer data if available\n",
    "        if 'customers' in pipeline.silver_data:\n",
    "            customers_df = pipeline.silver_data['customers']\n",
    "            print(f\"   Sample Customer Data:\")\n",
    "            # Show first few columns and rows\n",
    "            display_cols = [col for col in customers_df.columns if not col.startswith('_')][:5]\n",
    "            sample_data = customers_df[display_cols].head(3)\n",
    "            print(sample_data.to_string(index=False))\n",
    "    \n",
    "    # Product Insights  \n",
    "    if 'product_summary' in pipeline.gold_data:\n",
    "        product_summary = pipeline.gold_data['product_summary']\n",
    "        print(f\"\\nüõçÔ∏è PRODUCT INSIGHTS:\")\n",
    "        print(f\"   Total Products: {product_summary['total_products'].iloc[0]:,}\")\n",
    "        \n",
    "        # Price analysis if available\n",
    "        if 'avg_price' in product_summary.columns:\n",
    "            print(f\"   Average Price: ${product_summary['avg_price'].iloc[0]:.2f}\")\n",
    "            print(f\"   Price Range: ${product_summary['min_price'].iloc[0]:.2f} - ${product_summary['max_price'].iloc[0]:.2f}\")\n",
    "        \n",
    "        # Display sample product data if available\n",
    "        if 'products' in pipeline.silver_data:\n",
    "            products_df = pipeline.silver_data['products']\n",
    "            print(f\"   Sample Product Data:\")\n",
    "            display_cols = [col for col in products_df.columns if not col.startswith('_')][:5]\n",
    "            sample_data = products_df[display_cols].head(3)\n",
    "            print(sample_data.to_string(index=False))\n",
    "    \n",
    "    # Data Quality Assessment\n",
    "    print(f\"\\nüìà DATA QUALITY ASSESSMENT:\")\n",
    "    \n",
    "    for layer_name, layer_data in [('Bronze', pipeline.bronze_data), \n",
    "                                   ('Silver', pipeline.silver_data), \n",
    "                                   ('Gold', pipeline.gold_data)]:\n",
    "        print(f\"\\n   {layer_name} Layer:\")\n",
    "        if layer_data:\n",
    "            total_rows = sum(len(df) for df in layer_data.values())\n",
    "            total_cols = sum(len(df.columns) for df in layer_data.values())\n",
    "            print(f\"      Tables: {len(layer_data)}\")\n",
    "            print(f\"      Total Rows: {total_rows:,}\")\n",
    "            print(f\"      Total Columns: {total_cols}\")\n",
    "            \n",
    "            # Show table breakdown\n",
    "            for table_name, df in layer_data.items():\n",
    "                print(f\"         ‚Ä¢ {table_name}: {len(df)} rows √ó {len(df.columns)} cols\")\n",
    "        else:\n",
    "            print(f\"      No data available\")\n",
    "\n",
    "# Display insights if pipeline was successful\n",
    "if 'sql_pipeline' in locals() and hasattr(sql_pipeline, 'gold_data'):\n",
    "    display_pipeline_insights(sql_pipeline)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pipeline not executed successfully - no insights available\")\n",
    "\n",
    "# Check for common issues and provide recommendations\n",
    "print(f\"\\nüîß TROUBLESHOOTING RECOMMENDATIONS:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if successful_connection:\n",
    "    print(\"‚úÖ Database Connection: Working\")\n",
    "else:\n",
    "    print(\"‚ùå Database Connection: Failed\")\n",
    "    print(\"   üí° Try: Check Fabric workspace permissions\")\n",
    "    print(\"   üí° Try: Verify database name spelling\")\n",
    "    print(\"   üí° Try: Contact workspace administrator\")\n",
    "\n",
    "if 'discovered_tables' in locals() and len(discovered_tables) > 0:\n",
    "    print(\"‚úÖ Schema Access: SalesLT schema accessible\")\n",
    "else:\n",
    "    print(\"‚ùå Schema Access: Cannot access SalesLT schema\")\n",
    "    print(\"   üí° Try: Check read permissions on SalesLT schema\")\n",
    "    print(\"   üí° Try: Verify schema exists in database\")\n",
    "\n",
    "if 'sql_pipeline' in locals() and len(sql_pipeline.bronze_data) > 0:\n",
    "    print(\"‚úÖ Data Extraction: Successful\")\n",
    "else:\n",
    "    print(\"‚ùå Data Extraction: Failed or incomplete\")\n",
    "    print(\"   üí° Try: Check table permissions\")\n",
    "    print(\"   üí° Try: Verify data exists in tables\")\n",
    "\n",
    "print(f\"\\nüìã NEXT STEPS:\")\n",
    "print(\"1. üîß Fix any connection issues identified above\")\n",
    "print(\"2. üìä Run the Export_SalesLT_to_Bronze.ipynb notebook\")\n",
    "print(\"3. üèóÔ∏è Set up Bronze-Silver-Gold layer transformations\")\n",
    "print(\"4. üìà Build retail analytics dashboards\")\n",
    "print(\"5. üöÄ Deploy to production Fabric environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4023da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export pipeline data and create documentation\n",
    "print(\"üíæ STEP 7: Export Data and Create Documentation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def export_pipeline_data(pipeline, output_prefix=\"fabric_sql_pipeline\"):\n",
    "    \"\"\"Export all pipeline data to multiple formats\"\"\"\n",
    "    \n",
    "    if not hasattr(pipeline, 'bronze_data'):\n",
    "        print(\"‚ö†Ô∏è No pipeline data to export\")\n",
    "        return\n",
    "    \n",
    "    export_summary = {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'source_database': DATABASE_NAME,\n",
    "        'source_schema': SCHEMA_NAME,\n",
    "        'exported_layers': [],\n",
    "        'file_formats': ['parquet', 'csv', 'json']\n",
    "    }\n",
    "    \n",
    "    # Export each layer\n",
    "    for layer_name, layer_data in [('bronze', pipeline.bronze_data), \n",
    "                                   ('silver', pipeline.silver_data), \n",
    "                                   ('gold', pipeline.gold_data)]:\n",
    "        \n",
    "        if not layer_data:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìÅ Exporting {layer_name.title()} Layer:\")\n",
    "        layer_info = {\n",
    "            'layer_name': layer_name,\n",
    "            'tables': [],\n",
    "            'total_rows': 0,\n",
    "            'total_files': 0\n",
    "        }\n",
    "        \n",
    "        for table_name, df in layer_data.items():\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "                \n",
    "            file_prefix = f\"Files/{layer_name}/{output_prefix}_{table_name}\"\n",
    "            \n",
    "            try:\n",
    "                # Export to Parquet (recommended for Fabric)\n",
    "                parquet_path = f\"{file_prefix}.parquet\"\n",
    "                df.to_parquet(parquet_path, index=False)\n",
    "                print(f\"   ‚úÖ {table_name}.parquet: {len(df)} rows\")\n",
    "                \n",
    "                # Export to CSV (for compatibility)\n",
    "                csv_path = f\"{file_prefix}.csv\"\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                print(f\"   ‚úÖ {table_name}.csv: {len(df)} rows\")\n",
    "                \n",
    "                # Export metadata as JSON\n",
    "                metadata = {\n",
    "                    'table_name': table_name,\n",
    "                    'layer': layer_name,\n",
    "                    'row_count': len(df),\n",
    "                    'column_count': len(df.columns),\n",
    "                    'columns': list(df.columns),\n",
    "                    'data_types': df.dtypes.astype(str).to_dict(),\n",
    "                    'export_timestamp': datetime.now().isoformat(),\n",
    "                    'file_size_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)\n",
    "                }\n",
    "                \n",
    "                json_path = f\"{file_prefix}_metadata.json\"\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(metadata, f, indent=2)\n",
    "                \n",
    "                layer_info['tables'].append({\n",
    "                    'name': table_name,\n",
    "                    'rows': len(df),\n",
    "                    'columns': len(df.columns),\n",
    "                    'files': [parquet_path, csv_path, json_path]\n",
    "                })\n",
    "                layer_info['total_rows'] += len(df)\n",
    "                layer_info['total_files'] += 3\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to export {table_name}: {e}\")\n",
    "        \n",
    "        export_summary['exported_layers'].append(layer_info)\n",
    "        print(f\"   üìä {layer_name.title()} summary: {layer_info['total_files']} files, {layer_info['total_rows']} rows\")\n",
    "    \n",
    "    # Create overall export summary\n",
    "    summary_path = f\"Files/{output_prefix}_export_summary.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(export_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìÑ Export summary saved: {summary_path}\")\n",
    "    return export_summary\n",
    "\n",
    "# Export data if pipeline was successful\n",
    "if 'sql_pipeline' in locals() and hasattr(sql_pipeline, 'bronze_data'):\n",
    "    if len(sql_pipeline.bronze_data) > 0:\n",
    "        print(\"üöÄ Starting data export...\")\n",
    "        export_result = export_pipeline_data(sql_pipeline)\n",
    "        \n",
    "        if export_result:\n",
    "            print(f\"\\nüéâ EXPORT COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"üìÅ Files saved to: Files/ directory\")\n",
    "            print(f\"üìä Total layers exported: {len(export_result['exported_layers'])}\")\n",
    "            total_files = sum(layer['total_files'] for layer in export_result['exported_layers'])\n",
    "            print(f\"üìÑ Total files created: {total_files}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data available for export\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pipeline not executed - skipping export\")\n",
    "\n",
    "# Final status and recommendations\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"üéØ FABRIC SQL CONNECTION TROUBLESHOOTING COMPLETE\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìã EXECUTION STATUS:\")\n",
    "if successful_connection:\n",
    "    print(\"‚úÖ Database Connection: SUCCESS\")\n",
    "else:\n",
    "    print(\"‚ùå Database Connection: FAILED\")\n",
    "\n",
    "if 'discovered_tables' in locals() and len(discovered_tables) > 0:\n",
    "    print(\"‚úÖ Schema Discovery: SUCCESS\")\n",
    "else:\n",
    "    print(\"‚ùå Schema Discovery: FAILED\")\n",
    "\n",
    "if 'sql_pipeline' in locals() and len(sql_pipeline.bronze_data) > 0:\n",
    "    print(\"‚úÖ Pipeline Execution: SUCCESS\")\n",
    "else:\n",
    "    print(\"‚ùå Pipeline Execution: FAILED\")\n",
    "\n",
    "print(f\"\\nüîß IF CONNECTIONS FAILED:\")\n",
    "print(\"1. Check your Fabric workspace has access to Gaiye-SQL-DB\")\n",
    "print(\"2. Verify you have read permissions on SalesLT schema\")\n",
    "print(\"3. Confirm database name spelling: 'Gaiye-SQL-DB'\")\n",
    "print(\"4. Try refreshing browser and reconnecting to workspace\")\n",
    "print(\"5. Contact workspace administrator for access verification\")\n",
    "\n",
    "print(f\"\\nüöÄ IF CONNECTIONS WORKED:\")\n",
    "print(\"1. Use the Export_SalesLT_to_Bronze.ipynb notebook\")\n",
    "print(\"2. Modify connection string based on working method found above\")\n",
    "print(\"3. Proceed with Bronze-Silver-Gold layer development\")\n",
    "print(\"4. Deploy retail analytics solution to Fabric\")\n",
    "\n",
    "print(f\"\\nüìö ADDITIONAL RESOURCES:\")\n",
    "print(\"‚Ä¢ Microsoft Fabric Documentation: https://docs.microsoft.com/fabric/\")\n",
    "print(\"‚Ä¢ SQL Database Connectivity: https://docs.microsoft.com/fabric/data-warehouse/\")\n",
    "print(\"‚Ä¢ Retail Industry Solutions: https://docs.microsoft.com/industry/retail/\")\n",
    "\n",
    "print(f\"\\n‚ú® Troubleshooting completed at: {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
