{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3592a3e",
   "metadata": {},
   "source": [
    "# Export SalesLT Tables to Retail Data Model Bronze Layer\n",
    "\n",
    "This notebook dynamically discovers and exports all tables from the SalesLT schema in Gaiye-SQL-DB to the retail data model bronze layer.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Fabric workspace with access to Gaiye-SQL-DB\n",
    "- Retail data model lakehouse attached to this notebook\n",
    "- Appropriate permissions for SQL database and lakehouse access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749177a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Export started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "# Note: In Fabric, use the built-in SQL database connectivity\n",
    "database_name = \"Gaiye-SQL-DB\"\n",
    "schema_name = \"SalesLT\"\n",
    "\n",
    "# Fabric SQL connection string (adjust as needed for your environment)\n",
    "# This uses Fabric's integrated authentication\n",
    "connection_string = f\"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER=tcp:{database_name}.sql.fabric.microsoft.com,1433;DATABASE={database_name};Authentication=ActiveDirectoryIntegrated;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    "\n",
    "print(f\"üîó Connecting to database: {database_name}\")\n",
    "print(f\"üìä Target schema: {schema_name}\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = pyodbc.connect(connection_string)\n",
    "    print(\"‚úÖ Database connection successful\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {str(e)}\")\n",
    "    print(\"üí° Note: You may need to adjust the connection string for your Fabric environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb666754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all tables in SalesLT schema\n",
    "table_discovery_query = \"\"\"\n",
    "SELECT TABLE_NAME \n",
    "FROM INFORMATION_SCHEMA.TABLES \n",
    "WHERE TABLE_SCHEMA = 'SalesLT' \n",
    "AND TABLE_TYPE = 'BASE TABLE'\n",
    "ORDER BY TABLE_NAME\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Discovering tables in SalesLT schema...\")\n",
    "\n",
    "try:\n",
    "    # Execute table discovery query\n",
    "    conn = pyodbc.connect(connection_string)\n",
    "    tables_df = pd.read_sql(table_discovery_query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Extract table names\n",
    "    table_names = tables_df['TABLE_NAME'].tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(table_names)} tables in {schema_name} schema:\")\n",
    "    for i, table in enumerate(table_names, 1):\n",
    "        print(f\"   {i}. {table}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error discovering tables: {str(e)}\")\n",
    "    table_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48900e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define export function\n",
    "def export_table_to_bronze(table_name, connection_string, schema_name=\"SalesLT\"):\n",
    "    \"\"\"\n",
    "    Export a single table to bronze layer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting export for table: {table_name}\")\n",
    "        \n",
    "        # Build dynamic query\n",
    "        query = f\"SELECT * FROM {schema_name}.{table_name}\"\n",
    "        \n",
    "        # Connect and extract data\n",
    "        conn = pyodbc.connect(connection_string)\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        # Add metadata\n",
    "        df['_source_table'] = f\"{schema_name}.{table_name}\"\n",
    "        df['_extraction_timestamp'] = datetime.now()\n",
    "        df['_source_database'] = database_name\n",
    "        \n",
    "        # Define bronze layer path\n",
    "        bronze_path = f\"Files/bronze/{table_name.lower()}\"\n",
    "        \n",
    "        # Save to bronze layer as Parquet\n",
    "        df.to_parquet(f\"{bronze_path}/{table_name.lower()}.parquet\", index=False)\n",
    "        \n",
    "        logger.info(f\"‚úÖ Successfully exported {table_name}: {len(df)} rows\")\n",
    "        \n",
    "        return {\n",
    "            'table_name': table_name,\n",
    "            'status': 'success',\n",
    "            'row_count': len(df),\n",
    "            'columns': list(df.columns),\n",
    "            'bronze_path': bronze_path,\n",
    "            'file_size_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error exporting {table_name}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        \n",
    "        return {\n",
    "            'table_name': table_name,\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'row_count': 0\n",
    "        }\n",
    "\n",
    "print(\"üìù Export function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute export for all discovered tables\n",
    "print(f\"üöÄ Starting export of {len(table_names)} tables to bronze layer...\\n\")\n",
    "\n",
    "export_results = []\n",
    "successful_exports = 0\n",
    "failed_exports = 0\n",
    "\n",
    "for i, table_name in enumerate(table_names, 1):\n",
    "    print(f\"üìä Processing table {i}/{len(table_names)}: {table_name}\")\n",
    "    \n",
    "    # Export table\n",
    "    result = export_table_to_bronze(table_name, connection_string, schema_name)\n",
    "    export_results.append(result)\n",
    "    \n",
    "    # Track success/failure\n",
    "    if result['status'] == 'success':\n",
    "        successful_exports += 1\n",
    "        print(f\"   ‚úÖ {result['row_count']} rows exported ({result.get('file_size_mb', 0)} MB)\")\n",
    "    else:\n",
    "        failed_exports += 1\n",
    "        print(f\"   ‚ùå Export failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print()  # Empty line for readability\n",
    "\n",
    "print(f\"üéØ Export completed!\")\n",
    "print(f\"‚úÖ Successful: {successful_exports} tables\")\n",
    "print(f\"‚ùå Failed: {failed_exports} tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed export summary\n",
    "print(\"üìã EXPORT SUMMARY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Source Database: {database_name}\")\n",
    "print(f\"Source Schema: {schema_name}\")\n",
    "print(f\"Destination: Bronze Layer\")\n",
    "print()\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(export_results)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    # Display successful exports\n",
    "    successful_df = summary_df[summary_df['status'] == 'success']\n",
    "    if not successful_df.empty:\n",
    "        print(\"‚úÖ SUCCESSFUL EXPORTS:\")\n",
    "        print(successful_df[['table_name', 'row_count', 'file_size_mb']].to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "        total_rows = successful_df['row_count'].sum()\n",
    "        total_size = successful_df['file_size_mb'].sum()\n",
    "        print(f\"üìä Total rows exported: {total_rows:,}\")\n",
    "        print(f\"üíæ Total data size: {total_size:.2f} MB\")\n",
    "        print()\n",
    "    \n",
    "    # Display failed exports\n",
    "    failed_df = summary_df[summary_df['status'] == 'failed']\n",
    "    if not failed_df.empty:\n",
    "        print(\"‚ùå FAILED EXPORTS:\")\n",
    "        print(failed_df[['table_name', 'error']].to_string(index=False))\n",
    "        print()\n",
    "\n",
    "# Save summary report\n",
    "summary_report = {\n",
    "    'export_timestamp': datetime.now().isoformat(),\n",
    "    'source_database': database_name,\n",
    "    'source_schema': schema_name,\n",
    "    'total_tables_discovered': len(table_names),\n",
    "    'successful_exports': successful_exports,\n",
    "    'failed_exports': failed_exports,\n",
    "    'export_details': export_results\n",
    "}\n",
    "\n",
    "# Save summary as JSON in bronze layer\n",
    "import json\n",
    "summary_path = \"Files/bronze/_export_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìÑ Export summary saved to: {summary_path}\")\n",
    "print(\"\\nüéâ SalesLT schema export to bronze layer completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Verify files exist in bronze layer\n",
    "print(\"üîç VALIDATION: Checking bronze layer contents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "bronze_base_path = \"Files/bronze\"\n",
    "\n",
    "try:\n",
    "    # List bronze layer directories\n",
    "    if os.path.exists(bronze_base_path):\n",
    "        bronze_contents = os.listdir(bronze_base_path)\n",
    "        \n",
    "        print(f\"üìÅ Bronze layer contains {len(bronze_contents)} items:\")\n",
    "        for item in sorted(bronze_contents):\n",
    "            item_path = os.path.join(bronze_base_path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                # Check for parquet files in the directory\n",
    "                files = [f for f in os.listdir(item_path) if f.endswith('.parquet')]\n",
    "                print(f\"   üìÇ {item}/ ({len(files)} parquet files)\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {item}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Bronze layer path not found: {bronze_base_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error validating bronze layer: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Validation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ef76d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this notebook, you should have:\n",
    "\n",
    "1. **Exported Data**: All SalesLT tables saved as Parquet files in the bronze layer\n",
    "2. **Metadata**: Each file includes source tracking and extraction timestamps\n",
    "3. **Summary Report**: JSON file with complete export details\n",
    "4. **File Organization**: Each table in its own folder within `/Files/bronze/`\n",
    "\n",
    "### Recommended Next Actions:\n",
    "\n",
    "- **Review the data**: Check the bronze layer files to ensure data quality\n",
    "- **Create silver layer transformations**: Build data pipelines to clean and standardize the data\n",
    "- **Map to retail model**: Align the SalesLT schema with your retail data model requirements\n",
    "- **Set up monitoring**: Create alerts for data freshness and quality\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **Connection issues**: Verify your Fabric workspace has access to the SQL database\n",
    "- **Permission errors**: Ensure you have read access to SalesLT schema and write access to the lakehouse\n",
    "- **Large tables**: For very large tables, consider implementing chunked processing\n",
    "- **Data types**: Some SQL Server data types may need special handling during export"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
