{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9fec4e",
   "metadata": {},
   "source": [
    "# Cross-Lakehouse Copy: Shortcuts to Bronze\n",
    "\n",
    "**IMPORTANT SETUP FOR CROSS-LAKEHOUSE COPY:**\n",
    "\n",
    "This notebook copies SalesLT data from shortcuts to a specific bronze lakehouse:\n",
    "- **Source**: `Gaiye_Test_Lakehouse` (contains shortcuts)  \n",
    "- **Target**: `RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze`\n",
    "\n",
    "**Required Configuration in Fabric:**\n",
    "1. **Default Lakehouse**: Set `RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze` as default\n",
    "2. **Additional Lakehouse**: Add `Gaiye_Test_Lakehouse` as additional lakehouse\n",
    "3. **Data Flow**: Shortcuts ‚Üí Bronze Layer Files/bronze/saleslt/\n",
    "\n",
    "**Expected Result:** Bronze layer created in target lakehouse with organized SalesLT data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f6444",
   "metadata": {},
   "source": [
    "## Step 1 - Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac97ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRONZE LAKEHOUSE VALIDATION AND SETUP\n",
    "print(\"üéØ BRONZE LAKEHOUSE CONFIGURATION CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check current lakehouse context\n",
    "try:\n",
    "    # Method 1: Check default lakehouse name\n",
    "    default_tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    print(f\"‚úÖ Default lakehouse connected with {len(default_tables)} tables\")\n",
    "    \n",
    "    # Method 2: Check Files directory (should be bronze lakehouse)\n",
    "    try:\n",
    "        files_list = dbutils.fs.ls(\"Files/\")\n",
    "        print(f\"‚úÖ Files directory accessible with {len(files_list)} items\")\n",
    "        \n",
    "        # Check if this looks like the bronze lakehouse\n",
    "        current_items = [item.name.rstrip('/') for item in files_list]\n",
    "        print(f\"üìÅ Current lakehouse contains: {current_items}\")\n",
    "        \n",
    "        # This should be our target bronze lakehouse\n",
    "        print(f\"üéØ Target: Write bronze data to Files/bronze/saleslt/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Cannot access Files directory: {str(e)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Lakehouse check failed: {str(e)}\")\n",
    "\n",
    "# Check available databases/lakehouses\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\").toPandas()\n",
    "    print(f\"\\nüìä Available databases/lakehouses:\")\n",
    "    for db in databases.iloc[:, 0]:\n",
    "        print(f\"   üìÇ {db}\")\n",
    "        \n",
    "    # Look for our source lakehouse (Gaiye_Test_Lakehouse)\n",
    "    source_candidates = [db for db in databases.iloc[:, 0] if 'gaiye' in db.lower() or 'test' in db.lower()]\n",
    "    if source_candidates:\n",
    "        print(f\"\\nüîç Potential source lakehouses: {source_candidates}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Source lakehouse (Gaiye_Test_Lakehouse) not found in databases\")\n",
    "        print(f\"üí° Ensure Gaiye_Test_Lakehouse is attached as additional lakehouse\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database check failed: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° SETUP REMINDER:\")\n",
    "print(\"   1. This notebook should run in: RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze\")\n",
    "print(\"   2. Gaiye_Test_Lakehouse should be attached as additional lakehouse\")\n",
    "print(\"   3. Data will be written to Files/bronze/saleslt/ in the current (bronze) lakehouse\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4dcbc",
   "metadata": {},
   "source": [
    "## Step 2 - Bronze Lakehouse Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f651cc",
   "metadata": {},
   "source": [
    "## Step 3 - Cross-Lakehouse Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11396f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS-LAKEHOUSE COPY CONFIGURATION\n",
    "print(\"üîÑ CONFIGURING CROSS-LAKEHOUSE COPY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration for cross-lakehouse copy\n",
    "TARGET_BRONZE_LAKEHOUSE = \"RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze\"\n",
    "SOURCE_SHORTCUTS_LAKEHOUSE = \"Gaiye_Test_Lakehouse\"\n",
    "\n",
    "print(f\"üì§ Source: {SOURCE_SHORTCUTS_LAKEHOUSE} (shortcuts)\")\n",
    "print(f\"üì• Target: {TARGET_BRONZE_LAKEHOUSE} (bronze layer)\")\n",
    "\n",
    "# CRITICAL: Determine the correct path for target lakehouse\n",
    "# In Fabric, when multiple lakehouses are attached, we need to route correctly\n",
    "print(f\"\\nüéØ LAKEHOUSE ROUTING CONFIGURATION:\")\n",
    "\n",
    "# Check available lakehouse mounts\n",
    "try:\n",
    "    # List available mount points\n",
    "    mount_points = dbutils.fs.mounts()\n",
    "    print(f\"üìÅ Available mount points:\")\n",
    "    \n",
    "    target_mount = None\n",
    "    for mount in mount_points:\n",
    "        mount_point = mount.mountPoint\n",
    "        source_info = mount.source\n",
    "        print(f\"   üîó {mount_point} ‚Üí {source_info}\")\n",
    "        \n",
    "        # Look for the target bronze lakehouse\n",
    "        if TARGET_BRONZE_LAKEHOUSE.lower() in source_info.lower():\n",
    "            target_mount = mount_point\n",
    "            print(f\"   ‚úÖ Found target lakehouse mount: {target_mount}\")\n",
    "    \n",
    "    if target_mount:\n",
    "        # Use specific mount point for bronze lakehouse\n",
    "        BRONZE_BASE_PATH = f\"{target_mount}bronze/saleslt\"\n",
    "        print(f\"üéØ Bronze data will be written to: {BRONZE_BASE_PATH}\")\n",
    "    else:\n",
    "        # Fallback to default Files/ (current lakehouse)\n",
    "        BRONZE_BASE_PATH = \"Files/bronze/saleslt\"\n",
    "        print(f\"‚ö†Ô∏è Using default Files/ path: {BRONZE_BASE_PATH}\")\n",
    "        print(f\"üí° Ensure {TARGET_BRONZE_LAKEHOUSE} is set as default lakehouse\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Mount check failed: {str(e)}\")\n",
    "    # Use default path\n",
    "    BRONZE_BASE_PATH = \"Files/bronze/saleslt\"\n",
    "    print(f\"üîÑ Using default path: {BRONZE_BASE_PATH}\")\n",
    "\n",
    "# Alternative method: Use abfss:// path for specific lakehouse\n",
    "print(f\"\\nüîç ALTERNATIVE: Direct lakehouse path\")\n",
    "try:\n",
    "    # Check if we can determine workspace and lakehouse info\n",
    "    workspace_info = spark.sql(\"DESCRIBE DETAIL default.information_schema\").collect()\n",
    "    print(f\"üìä Current workspace context available\")\n",
    "    \n",
    "    # Construct specific lakehouse path\n",
    "    BRONZE_ABFSS_PATH = f\"abfss://bronze@onelake.dfs.fabric.microsoft.com/bronze/saleslt\"\n",
    "    print(f\"üîó Alternative ABFSS path: {BRONZE_ABFSS_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Workspace info not available: {str(e)}\")\n",
    "\n",
    "# Override connection method for shortcut-based copy\n",
    "FORCE_SHORTCUT_COPY = True\n",
    "CONNECTION_METHOD_OVERRIDE = \"existing_tables\"  # Use existing tables (shortcuts)\n",
    "\n",
    "print(f\"\\nüìã FINAL CONFIGURATION:\")\n",
    "print(f\"   üì§ Source Method: {CONNECTION_METHOD_OVERRIDE}\")\n",
    "print(f\"   üì• Target Path: {BRONZE_BASE_PATH}\")\n",
    "print(f\"   üéØ Target Lakehouse: {TARGET_BRONZE_LAKEHOUSE}\")\n",
    "\n",
    "# Verify we can write to the target location\n",
    "print(f\"\\nüß™ TESTING WRITE ACCESS:\")\n",
    "try:\n",
    "    # Test write access to target path\n",
    "    test_path = f\"{BRONZE_BASE_PATH}/_connection_test/\"\n",
    "    \n",
    "    # Create test dataframe\n",
    "    test_df = spark.createDataFrame([{\"test\": \"connection\", \"timestamp\": str(datetime.now())}])\n",
    "    \n",
    "    # Test write\n",
    "    test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "    \n",
    "    # Verify write\n",
    "    test_read = spark.read.parquet(test_path)\n",
    "    test_count = test_read.count()\n",
    "    \n",
    "    # Clean up test\n",
    "    dbutils.fs.rm(test_path, True)\n",
    "    \n",
    "    print(f\"‚úÖ Write access confirmed to {BRONZE_BASE_PATH}\")\n",
    "    print(f\"‚úÖ Test file created and read successfully ({test_count} records)\")\n",
    "    print(f\"‚úÖ Ready to copy to bronze layer in target lakehouse\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Write access test failed: {str(e)}\")\n",
    "    print(f\"üí° TROUBLESHOOTING:\")\n",
    "    print(f\"   1. Ensure this notebook is attached to: {TARGET_BRONZE_LAKEHOUSE}\")\n",
    "    print(f\"   2. Or ensure {TARGET_BRONZE_LAKEHOUSE} is set as default lakehouse\")\n",
    "    print(f\"   3. Check lakehouse permissions and access\")\n",
    "\n",
    "# Check for source data (shortcuts)\n",
    "print(f\"\\nüîç CHECKING SOURCE SHORTCUTS:\")\n",
    "try:\n",
    "    # Look for SalesLT-related tables\n",
    "    all_tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    \n",
    "    # Expected shortcut table names (without SalesLT prefix)\n",
    "    expected_tables = ['address', 'customer', 'customeraddress', 'product', \n",
    "                      'productcategory', 'productdescription', 'productmodel',\n",
    "                      'productmodelproductdescription', 'salesorderdetail', 'salesorderheader']\n",
    "    \n",
    "    found_shortcuts = []\n",
    "    for expected in expected_tables:\n",
    "        matches = all_tables[all_tables['tableName'].str.lower() == expected.lower()]\n",
    "        if not matches.empty:\n",
    "            found_shortcuts.append(expected)\n",
    "            print(f\"   ‚úÖ Found shortcut: {expected}\")\n",
    "    \n",
    "    if len(found_shortcuts) > 0:\n",
    "        print(f\"\\nüéâ Found {len(found_shortcuts)} shortcuts ready for copy\")\n",
    "        print(f\"üìã Will copy: {', '.join(found_shortcuts)}\")\n",
    "        \n",
    "        # Store configuration for export function\n",
    "        globals()['BRONZE_TARGET_PATH'] = BRONZE_BASE_PATH\n",
    "        globals()['SOURCE_TABLES'] = found_shortcuts\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No shortcuts found in accessible tables\")\n",
    "        print(f\"üí° Available tables: {', '.join(all_tables['tableName'].tolist()[:10])}...\")\n",
    "        print(f\"üí° Ensure {SOURCE_SHORTCUTS_LAKEHOUSE} is attached as additional lakehouse\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Shortcut check failed: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ Configuration complete - ready for cross-lakehouse copy!\")\n",
    "print(f\"üéØ Data will be written to: {BRONZE_BASE_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7cdc00",
   "metadata": {},
   "source": [
    "## üéØ CROSS-LAKEHOUSE COPY SETUP GUIDE\n",
    "\n",
    "To copy SalesLT data from shortcuts to your bronze lakehouse, follow these steps in Microsoft Fabric:\n",
    "\n",
    "### 1. **Open Target Lakehouse** \n",
    "- Navigate to `RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze`\n",
    "- This will be your **default lakehouse** (where data gets written)\n",
    "\n",
    "### 2. **Import This Notebook**\n",
    "- Import this notebook into the target lakehouse workspace\n",
    "- The notebook will write data to `Files/bronze/saleslt/` in the current lakehouse\n",
    "\n",
    "### 3. **Attach Source Lakehouse**\n",
    "- In the notebook, click **\"Add Lakehouse\"** \n",
    "- Select `Gaiye_Test_Lakehouse` as **additional lakehouse**\n",
    "- This gives access to the shortcuts for reading data\n",
    "\n",
    "### 4. **Run the Notebook**\n",
    "- Execute the configuration cells (cells 1-4) to verify setup\n",
    "- Run the export process to copy all SalesLT tables\n",
    "- Data will appear in `Files/bronze/saleslt/` in your bronze lakehouse\n",
    "\n",
    "### 5. **Verify Results**\n",
    "- Check `Files ‚Üí bronze ‚Üí saleslt` in the bronze lakehouse\n",
    "- You should see folders for each table (customer, product, etc.)\n",
    "- Each folder contains parquet files with your SalesLT data\n",
    "\n",
    "### Expected Result:\n",
    "```\n",
    "RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze/\n",
    "‚îî‚îÄ‚îÄ Files/\n",
    "    ‚îî‚îÄ‚îÄ bronze/\n",
    "        ‚îî‚îÄ‚îÄ saleslt/\n",
    "            ‚îú‚îÄ‚îÄ address/\n",
    "            ‚îú‚îÄ‚îÄ customer/\n",
    "            ‚îú‚îÄ‚îÄ customeraddress/\n",
    "            ‚îú‚îÄ‚îÄ product/\n",
    "            ‚îú‚îÄ‚îÄ productcategory/\n",
    "            ‚îú‚îÄ‚îÄ productdescription/\n",
    "            ‚îú‚îÄ‚îÄ productmodel/\n",
    "            ‚îú‚îÄ‚îÄ productmodelproductdescription/\n",
    "            ‚îú‚îÄ‚îÄ salesorderdetail/\n",
    "            ‚îî‚îÄ‚îÄ salesorderheader/\n",
    "```\n",
    "\n",
    "**‚ö° The key is running this notebook FROM the bronze lakehouse with shortcuts attached!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d34a89",
   "metadata": {},
   "source": [
    "## Step 4 - Setup Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3592a3e",
   "metadata": {},
   "source": [
    "# Export SalesLT Tables to Retail Data Model Bronze Layer\n",
    "\n",
    "This notebook dynamically discovers and exports all tables from the SalesLT schema in Gaiye-SQL-DB to the retail data model bronze layer.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Fabric workspace with access to Gaiye-SQL-DB\n",
    "- Retail data model lakehouse attached to this notebook\n",
    "- Appropriate permissions for SQL database and lakehouse access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749177a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "# Import required libraries (Fabric-compatible only)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Export started at: {datetime.now()}\")\n",
    "print(\"üîß Using Fabric-native connectivity (no external dependencies)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2620fd0",
   "metadata": {},
   "source": [
    "## Step 5 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Environment Diagnostic & Connectivity Check\n",
    "\n",
    "# This cell validates the Fabric environment and ensures all prerequisites are met\n",
    "# Key validations:\n",
    "# ‚úÖ Spark session is active and functional\n",
    "# ‚úÖ Database connectivity (lists all available databases in workspace)\n",
    "# ‚úÖ Lakehouse is properly attached to this notebook\n",
    "# ‚úÖ File system access for bronze layer creation\n",
    "# ‚ö†Ô∏è Troubleshooting: Fixes common column name issues and auto-creates directories\n",
    "\n",
    "# Check if we're running in Fabric environment first\n",
    "try:\n",
    "    spark_version = spark.version\n",
    "    print(f\"üè≠ Fabric Spark session detected: {spark_version}\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è  Spark session not available - ensure you're running in Fabric\")\n",
    "    print(\"‚ùå Cannot proceed without Spark session\")\n",
    "    raise Exception(\"Spark session required for this notebook\")\n",
    "\n",
    "# Fabric Environment Diagnostic Check\n",
    "print(\"\\nüîç FABRIC ENVIRONMENT DIAGNOSTIC\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check 1: Spark session\n",
    "try:\n",
    "    print(f\"‚úÖ Spark session active: {spark.version}\")\n",
    "except:\n",
    "    print(\"‚ùå Spark session not available\")\n",
    "\n",
    "# Check 2: Available databases\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\").toPandas()\n",
    "    print(f\"‚úÖ Available databases ({len(databases)}):\")\n",
    "    # Handle different possible column names\n",
    "    db_column = 'namespace' if 'namespace' in databases.columns else 'databaseName'\n",
    "    for db in databases[db_column]:\n",
    "        print(f\"   üìÅ {db}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cannot list databases: {str(e)}\")\n",
    "\n",
    "# Check 3: Lakehouse attachment\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    print(f\"‚úÖ Lakehouse tables visible ({len(tables)}):\")\n",
    "    for table in tables['tableName'][:5]:  # Show first 5\n",
    "        print(f\"   üìä {table}\")\n",
    "    if len(tables) > 5:\n",
    "        print(f\"   ... and {len(tables)-5} more\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cannot access lakehouse tables: {str(e)}\")\n",
    "\n",
    "# Check 4: File system access\n",
    "try:\n",
    "    files_path = \"Files\"\n",
    "    if os.path.exists(files_path):\n",
    "        print(f\"‚úÖ File system access: {files_path} exists\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  File system: {files_path} not found - will be created during export\")\n",
    "        # Create the Files directory\n",
    "        os.makedirs(files_path, exist_ok=True)\n",
    "        print(f\"‚úÖ Created {files_path} directory\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå File system access failed: {str(e)}\")\n",
    "\n",
    "# Check 5: SQL database access (will be checked after database name is defined)\n",
    "print(\"‚è≥ SQL Database access check will run after database configuration...\")\n",
    "\n",
    "print(\"\\nüìã Diagnostic complete. Address any ‚ùå issues before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa461633",
   "metadata": {},
   "source": [
    "## Step 6 - Environment Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Azure SQL Authentication & Connection Methods\n",
    "\n",
    "# Based on Fabric community guidance, we'll try multiple authentication approaches\n",
    "# Reference: https://community.fabric.microsoft.com/t5/Data-Engineering/Connecting-to-Azure-SQL-Server-from-a-Fabric-Compute-Notebook/m-p/4414950\n",
    "\n",
    "print(\"\udd10 AZURE SQL AUTHENTICATION & CONNECTION SETUP\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Found Fabric connections (for Data Factory):\")\n",
    "print(\"   1. FabricSql gazho (Fabric SQL database)\")\n",
    "print(\"   2. gaiye-sql-db.sql.fabric.microsoft;SalesLT gazho (SQL Server)\")\n",
    "print()\n",
    "print(\"üí° Testing authentication methods based on community guidance...\")\n",
    "\n",
    "schema_name = \"SalesLT\"\n",
    "working_connection = None\n",
    "working_syntax = None\n",
    "working_auth_method = None\n",
    "\n",
    "# Server details for your Gaiye SQL DB\n",
    "server_name = \"gaiye-sql-db.sql.fabric.microsoft.com\"\n",
    "database_name = \"Gaiye-SQL-DB\"\n",
    "\n",
    "print(\"üéØ Testing Fabric SQL authentication methods:\")\n",
    "\n",
    "# Method 1: Check if there are any shortcuts or external tables already set up\n",
    "print(\"\\nüìç Method 1: Checking for existing shortcuts/external tables\")\n",
    "try:\n",
    "    existing_tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    \n",
    "    # Standard SalesLT table names (shortcuts import without schema prefix)\n",
    "    expected_saleslt_tables = [\n",
    "        'address', 'customer', 'customeraddress', 'product', \n",
    "        'productcategory', 'productdescription', 'productmodel',\n",
    "        'productmodelproductdescription', 'salesorderdetail', 'salesorderheader'\n",
    "    ]\n",
    "    \n",
    "    # Look for exact matches first\n",
    "    saleslt_tables = []\n",
    "    for _, row in existing_tables.iterrows():\n",
    "        table_name = row['tableName'].lower()\n",
    "        if table_name in expected_saleslt_tables:\n",
    "            saleslt_tables.append(row['tableName'])\n",
    "    \n",
    "    # If no exact matches, look for partial matches\n",
    "    if not saleslt_tables:\n",
    "        for _, row in existing_tables.iterrows():\n",
    "            table_name = row['tableName'].lower()\n",
    "            if any(expected in table_name or table_name in expected for expected in expected_saleslt_tables):\n",
    "                saleslt_tables.append(row['tableName'])\n",
    "    \n",
    "    if saleslt_tables:\n",
    "        print(f\"   ‚úÖ Found {len(saleslt_tables)} existing SalesLT tables in lakehouse:\")\n",
    "        for table in saleslt_tables:\n",
    "            print(f\"      üîó {table}\")\n",
    "        print(\"   üéØ These appear to be lakehouse shortcuts - perfect!\")\n",
    "        working_syntax = \"existing_tables\"\n",
    "        working_connection = \"lakehouse_shortcuts\"\n",
    "        working_auth_method = \"lakehouse_native\"\n",
    "    else:\n",
    "        print(\"   ‚ùå No SalesLT tables found in lakehouse shortcuts\")\n",
    "        print(\"   üí° Expected tables: Address, Customer, CustomerAddress, Product,\")\n",
    "        print(\"       ProductCategory, ProductDescription, ProductModel, etc.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error checking existing tables: {str(e)[:100]}...\")\n",
    "\n",
    "# Method 2: Spark JDBC with Workspace Identity (recommended approach from community)\n",
    "if not working_connection:\n",
    "    print(\"\\nüìç Method 2: Spark JDBC with Workspace Identity\")\n",
    "    \n",
    "    try:\n",
    "        # Build JDBC URL with ActiveDirectoryMSI authentication\n",
    "        jdbc_url = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};encrypt=true;trustServerCertificate=false;Authentication=ActiveDirectoryMSI\"\n",
    "        \n",
    "        print(f\"   üîç Trying Spark JDBC with MSI: {server_name}\")\n",
    "        print(f\"   üìù JDBC URL: {jdbc_url}\")\n",
    "        \n",
    "        # Test with a simple query to INFORMATION_SCHEMA\n",
    "        test_query = f\"(SELECT COUNT(*) as table_count FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{schema_name}' AND TABLE_TYPE = 'BASE TABLE') AS test\"\n",
    "        \n",
    "        # Use Spark JDBC connector\n",
    "        df_test = spark.read \\\n",
    "            .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", test_query) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "        \n",
    "        # Execute the query\n",
    "        result = df_test.collect()\n",
    "        table_count = result[0]['table_count']\n",
    "        \n",
    "        print(f\"   ‚úÖ SUCCESS! Spark JDBC with MSI connected, found {table_count} tables\")\n",
    "        working_connection = jdbc_url\n",
    "        working_syntax = \"spark_jdbc_msi\"\n",
    "        working_auth_method = \"workspace_identity\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Spark JDBC MSI failed: {str(e)[:100]}...\")\n",
    "\n",
    "# Method 3: Try with mssparkutils token (though community reports issues)\n",
    "if not working_connection:\n",
    "    print(\"\\nüìç Method 3: Spark JDBC with Token Authentication\")\n",
    "    \n",
    "    try:\n",
    "        # Try to get token for Azure SQL\n",
    "        print(\"   üîë Attempting to get Azure SQL token...\")\n",
    "        \n",
    "        # Get token for Azure SQL Database\n",
    "        token = mssparkutils.credentials.getToken(audience=\"https://database.windows.net/\")\n",
    "        print(f\"   üìã Token retrieved (length: {len(token) if token else 0})\")\n",
    "        \n",
    "        # Build JDBC URL without authentication\n",
    "        jdbc_url_token = f\"jdbc:sqlserver://{server_name}:1433;database={database_name};encrypt=true;trustServerCertificate=false\"\n",
    "        \n",
    "        print(f\"   üîç Trying Spark JDBC with token: {server_name}\")\n",
    "        \n",
    "        # Test with token - this is tricky with Spark JDBC, let's try a different approach\n",
    "        # Note: Community reports this often returns user token instead of workspace token\n",
    "        test_query = f\"(SELECT COUNT(*) as table_count FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{schema_name}' AND TABLE_TYPE = 'BASE TABLE') AS test\"\n",
    "        \n",
    "        # Try to use the token in connection properties\n",
    "        connection_properties = {\n",
    "            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "            \"accessToken\": token\n",
    "        }\n",
    "        \n",
    "        df_test = spark.read.jdbc(\n",
    "            url=jdbc_url_token,\n",
    "            table=test_query,\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        \n",
    "        result = df_test.collect()\n",
    "        table_count = result[0]['table_count']\n",
    "        \n",
    "        print(f\"   ‚úÖ SUCCESS! Spark JDBC with token connected, found {table_count} tables\")\n",
    "        working_connection = jdbc_url_token\n",
    "        working_syntax = \"spark_jdbc_token\"\n",
    "        working_auth_method = \"token_based\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Spark JDBC token failed: {str(e)[:100]}...\")\n",
    "\n",
    "# Method 4: Check for Fabric SQL Database connections\n",
    "if not working_connection:\n",
    "    print(\"\\nüìç Method 4: Fabric SQL Database Direct Connection\")\n",
    "    \n",
    "    fabric_sql_variations = [\n",
    "        \"FabricSql_gazho\",\n",
    "        \"FabricSqlgazho\", \n",
    "        \"FabricSql\",\n",
    "        \"gaiye_sql_db\"\n",
    "    ]\n",
    "    \n",
    "    for conn_name in fabric_sql_variations:\n",
    "        try:\n",
    "            test_query = f\"\"\"\n",
    "            SELECT COUNT(*) as table_count \n",
    "            FROM {conn_name}.INFORMATION_SCHEMA.TABLES \n",
    "            WHERE TABLE_SCHEMA = '{schema_name}' \n",
    "            AND TABLE_TYPE = 'BASE TABLE'\n",
    "            \"\"\"\n",
    "            \n",
    "            print(f\"   üîç Trying Fabric SQL connection: {conn_name}\")\n",
    "            result = spark.sql(test_query).toPandas()\n",
    "            table_count = result.iloc[0]['table_count']\n",
    "            \n",
    "            print(f\"   ‚úÖ SUCCESS! Connected via {conn_name}, found {table_count} tables\")\n",
    "            working_connection = conn_name\n",
    "            working_syntax = \"fabric_sql_direct\"\n",
    "            working_auth_method = \"fabric_native\"\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed {conn_name}: {str(e)[:80]}...\")\n",
    "            continue\n",
    "\n",
    "# Method 5: Check for mirrored databases (fallback)\n",
    "if not working_connection:\n",
    "    print(\"\\nüìç Method 5: Check for mirrored SQL databases\")\n",
    "    \n",
    "    try:\n",
    "        databases = spark.sql(\"SHOW DATABASES\").toPandas()\n",
    "        db_column = 'namespace' if 'namespace' in databases.columns else 'databaseName'\n",
    "        \n",
    "        sql_candidates = []\n",
    "        for db in databases[db_column]:\n",
    "            if any(keyword in db.lower() for keyword in ['gaiye', 'sql', 'server', 'saleslt']):\n",
    "                sql_candidates.append(db)\n",
    "        \n",
    "        print(f\"   üìÅ Found potential SQL-related databases: {sql_candidates}\")\n",
    "        \n",
    "        for candidate_db in sql_candidates:\n",
    "            try:\n",
    "                tables_df = spark.sql(f\"SHOW TABLES IN {candidate_db}\").toPandas()\n",
    "                \n",
    "                if 'tableName' in tables_df.columns:\n",
    "                    saleslt_in_db = tables_df[tables_df['tableName'].str.contains('saleslt|customer|product|address', case=False, na=False)]\n",
    "                else:\n",
    "                    col_name = tables_df.columns[0] if len(tables_df.columns) > 0 else 'table'\n",
    "                    saleslt_in_db = tables_df[tables_df[col_name].str.contains('saleslt|customer|product|address', case=False, na=False)]\n",
    "                \n",
    "                if not saleslt_in_db.empty:\n",
    "                    table_names_in_db = saleslt_in_db.iloc[:, 0].tolist()\n",
    "                    saleslt_indicators = ['customer', 'product', 'address', 'salesorder']\n",
    "                    found_indicators = [name for name in table_names_in_db if any(ind in name.lower() for ind in saleslt_indicators)]\n",
    "                    \n",
    "                    if len(found_indicators) >= 2:\n",
    "                        print(f\"   üéØ {candidate_db} looks like SalesLT source: {found_indicators}\")\n",
    "                        working_connection = candidate_db\n",
    "                        working_syntax = \"mirrored_database\"\n",
    "                        working_auth_method = \"mirrored_native\"\n",
    "                        break\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error checking mirrored databases: {str(e)[:100]}...\")\n",
    "\n",
    "# Summary\n",
    "if working_connection:\n",
    "    print(f\"\\nüéâ AUTHENTICATION & CONNECTION SUCCESSFUL!\")\n",
    "    print(f\"   ‚úÖ Working method: {working_syntax}\")\n",
    "    print(f\"   ‚úÖ Connection/Database: {working_connection}\")\n",
    "    print(f\"   ‚úÖ Authentication: {working_auth_method}\")\n",
    "    print(f\"   ‚úÖ Schema: {schema_name}\")\n",
    "    \n",
    "    # Store for use in subsequent cells\n",
    "    connection_name = working_connection\n",
    "    connection_method = working_syntax\n",
    "    auth_method = working_auth_method\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  NO AUTHENTICATION METHOD WORKED\")\n",
    "    print(\"üí° Based on community feedback, this is a known limitation\")\n",
    "    print(\"üìã Recommended solutions:\")\n",
    "    print(\"   1. üîß Create lakehouse shortcuts to SQL Server tables\")\n",
    "    print(\"   2. üîß Use Data Factory with existing connections\")\n",
    "    print(\"   3. üîß Set up SQL database mirroring in Fabric\")\n",
    "    print(\"   4. üîß Request workspace identity permissions on SQL Server\")\n",
    "    print(\"   5. üìã Use fallback with standard SalesLT table list\")\n",
    "    \n",
    "    # Set defaults for fallback approach\n",
    "    connection_name = \"manual_setup_required\" \n",
    "    connection_method = \"use_data_factory_or_shortcuts\"\n",
    "    auth_method = \"manual_authentication_needed\"\n",
    "\n",
    "print(f\"\\nüìã Configuration for subsequent cells:\")\n",
    "print(f\"   Method: {connection_method}\")\n",
    "print(f\"   Connection: {connection_name}\")\n",
    "print(f\"   Authentication: {auth_method}\")\n",
    "print(f\"   Schema: {schema_name}\")\n",
    "\n",
    "# Additional guidance based on authentication results\n",
    "if working_auth_method == \"workspace_identity\":\n",
    "    print(f\"\\nüí° WORKSPACE IDENTITY SUCCESS:\")\n",
    "    print(f\"   Your Fabric workspace has proper SQL Server access\")\n",
    "    print(f\"   This is the recommended approach for production use\")\n",
    "elif working_auth_method == \"token_based\":\n",
    "    print(f\"\\n‚ö†Ô∏è  TOKEN-BASED CONNECTION:\")\n",
    "    print(f\"   Note: Community reports token may be user-based, not workspace-based\")\n",
    "    print(f\"   Monitor for any permission issues during export\")\n",
    "elif working_auth_method == \"fabric_native\":\n",
    "    print(f\"\\nüí° FABRIC SQL DATABASE DETECTED:\")\n",
    "    print(f\"   Direct connection to Fabric SQL Database established\")\n",
    "elif auth_method == \"manual_authentication_needed\":\n",
    "    print(f\"\\nüîß AUTHENTICATION SETUP REQUIRED:\")\n",
    "    print(f\"   1. Option A: Grant workspace identity access to SQL Server\")\n",
    "    print(f\"      - In Azure portal, go to your SQL Server\")\n",
    "    print(f\"      - Add your Fabric workspace as SQL admin or db_datareader\")\n",
    "    print(f\"   2. Option B: Create lakehouse shortcuts\")\n",
    "    print(f\"      - Go to your lakehouse\")\n",
    "    print(f\"      - Create shortcuts to SQL Server tables\")\n",
    "    print(f\"   3. Option C: Use Data Factory\")\n",
    "    print(f\"      - Your existing connections work in Data Factory\")\n",
    "    print(f\"      - Create copy activities for each table\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40dc35",
   "metadata": {},
   "source": [
    "## Step 7 - Authentication Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb666754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Discover SalesLT Tables with Authentication-Aware Methods\n",
    "\n",
    "print(\"üîç DISCOVERING SALESLT TABLES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìã Using authentication method: {auth_method}\")\n",
    "print(f\"\udccb Connection: {connection_name}\")\n",
    "print(f\"\udccb Method: {connection_method}\")\n",
    "print()\n",
    "\n",
    "tables_info = []\n",
    "schema_name = \"SalesLT\"\n",
    "\n",
    "# Discovery method based on successful authentication\n",
    "if connection_method == \"spark_jdbc_msi\":\n",
    "    print(\"üí° Using Spark JDBC with Workspace Identity\")\n",
    "    try:\n",
    "        # Query INFORMATION_SCHEMA using established JDBC connection\n",
    "        query = f\"\"\"\n",
    "        (SELECT \n",
    "            TABLE_NAME,\n",
    "            TABLE_TYPE,\n",
    "            CASE \n",
    "                WHEN TABLE_NAME LIKE '%Customer%' THEN 'Customer Data'\n",
    "                WHEN TABLE_NAME LIKE '%Product%' THEN 'Product Catalog'\n",
    "                WHEN TABLE_NAME LIKE '%SalesOrder%' THEN 'Sales Transactions'\n",
    "                WHEN TABLE_NAME LIKE '%Address%' THEN 'Address Information'\n",
    "                ELSE 'Reference Data'\n",
    "            END as category\n",
    "         FROM INFORMATION_SCHEMA.TABLES \n",
    "         WHERE TABLE_SCHEMA = '{schema_name}' \n",
    "         AND TABLE_TYPE = 'BASE TABLE'\n",
    "         ORDER BY TABLE_NAME) AS tables_query\"\"\"\n",
    "        \n",
    "        df_tables = spark.read \\\n",
    "            .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "            .option(\"url\", working_connection) \\\n",
    "            .option(\"dbtable\", query) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .load()\n",
    "        \n",
    "        tables_df = df_tables.toPandas()\n",
    "        \n",
    "        for _, row in tables_df.iterrows():\n",
    "            tables_info.append({\n",
    "                'table_name': row['TABLE_NAME'],\n",
    "                'full_name': f\"{schema_name}.{row['TABLE_NAME']}\",\n",
    "                'type': row['TABLE_TYPE'],\n",
    "                'category': row['category'],\n",
    "                'source': 'jdbc_msi'\n",
    "            })\n",
    "            \n",
    "        print(f\"   ‚úÖ Discovered {len(tables_info)} tables via JDBC MSI\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå JDBC MSI table discovery failed: {str(e)[:100]}...\")\n",
    "\n",
    "elif connection_method == \"spark_jdbc_token\":\n",
    "    print(\"\udca1 Using Spark JDBC with Token Authentication\")\n",
    "    try:\n",
    "        # Query INFORMATION_SCHEMA using token-based JDBC\n",
    "        query = f\"\"\"\n",
    "        (SELECT \n",
    "            TABLE_NAME,\n",
    "            TABLE_TYPE,\n",
    "            CASE \n",
    "                WHEN TABLE_NAME LIKE '%Customer%' THEN 'Customer Data'\n",
    "                WHEN TABLE_NAME LIKE '%Product%' THEN 'Product Catalog'\n",
    "                WHEN TABLE_NAME LIKE '%SalesOrder%' THEN 'Sales Transactions'\n",
    "                WHEN TABLE_NAME LIKE '%Address%' THEN 'Address Information'\n",
    "                ELSE 'Reference Data'\n",
    "            END as category\n",
    "         FROM INFORMATION_SCHEMA.TABLES \n",
    "         WHERE TABLE_SCHEMA = '{schema_name}' \n",
    "         AND TABLE_TYPE = 'BASE TABLE'\n",
    "         ORDER BY TABLE_NAME) AS tables_query\"\"\"\n",
    "        \n",
    "        # Get fresh token\n",
    "        token = mssparkutils.credentials.getToken(audience=\"https://database.windows.net/\")\n",
    "        \n",
    "        connection_properties = {\n",
    "            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "            \"accessToken\": token\n",
    "        }\n",
    "        \n",
    "        df_tables = spark.read.jdbc(\n",
    "            url=working_connection,\n",
    "            table=query,\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        \n",
    "        tables_df = df_tables.toPandas()\n",
    "        \n",
    "        for _, row in tables_df.iterrows():\n",
    "            tables_info.append({\n",
    "                'table_name': row['TABLE_NAME'],\n",
    "                'full_name': f\"{schema_name}.{row['TABLE_NAME']}\",\n",
    "                'type': row['TABLE_TYPE'],\n",
    "                'category': row['category'],\n",
    "                'source': 'jdbc_token'\n",
    "            })\n",
    "            \n",
    "        print(f\"   ‚úÖ Discovered {len(tables_info)} tables via JDBC Token\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå JDBC Token table discovery failed: {str(e)[:100]}...\")\n",
    "\n",
    "elif connection_method == \"fabric_sql_direct\":\n",
    "    print(\"üí° Using Fabric SQL Database Direct Connection\")\n",
    "    try:\n",
    "        # Query using Fabric SQL connection name\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            TABLE_NAME,\n",
    "            TABLE_TYPE,\n",
    "            CASE \n",
    "                WHEN TABLE_NAME LIKE '%Customer%' THEN 'Customer Data'\n",
    "                WHEN TABLE_NAME LIKE '%Product%' THEN 'Product Catalog'\n",
    "                WHEN TABLE_NAME LIKE '%SalesOrder%' THEN 'Sales Transactions'\n",
    "                WHEN TABLE_NAME LIKE '%Address%' THEN 'Address Information'\n",
    "                ELSE 'Reference Data'\n",
    "            END as category\n",
    "        FROM {connection_name}.INFORMATION_SCHEMA.TABLES \n",
    "        WHERE TABLE_SCHEMA = '{schema_name}' \n",
    "        AND TABLE_TYPE = 'BASE TABLE'\n",
    "        ORDER BY TABLE_NAME\n",
    "        \"\"\"\n",
    "        \n",
    "        tables_df = spark.sql(query).toPandas()\n",
    "        \n",
    "        for _, row in tables_df.iterrows():\n",
    "            tables_info.append({\n",
    "                'table_name': row['TABLE_NAME'],\n",
    "                'full_name': f\"{connection_name}.{schema_name}.{row['TABLE_NAME']}\",\n",
    "                'type': row['TABLE_TYPE'],\n",
    "                'category': row['category'],\n",
    "                'source': 'fabric_sql'\n",
    "            })\n",
    "            \n",
    "        print(f\"   ‚úÖ Discovered {len(tables_info)} tables via Fabric SQL\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Fabric SQL table discovery failed: {str(e)[:100]}...\")\n",
    "\n",
    "elif connection_method == \"existing_tables\":\n",
    "    print(\"üí° Using Existing Lakehouse Tables/Shortcuts\")\n",
    "    try:\n",
    "        existing_tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "        \n",
    "        # Standard SalesLT table names (without schema prefix in shortcuts)\n",
    "        expected_saleslt_tables = [\n",
    "            'address', 'customer', 'customeraddress', 'product', \n",
    "            'productcategory', 'productdescription', 'productmodel',\n",
    "            'productmodelproductdescription', 'salesorderdetail', 'salesorderheader'\n",
    "        ]\n",
    "        \n",
    "        # Look for exact matches and partial matches\n",
    "        print(f\"   üîç Scanning {len(existing_tables)} tables in lakehouse...\")\n",
    "        \n",
    "        for _, row in existing_tables.iterrows():\n",
    "            table_name = row['tableName'].lower()\n",
    "            original_name = row['tableName']\n",
    "            \n",
    "            # Check if this is a SalesLT table (exact match or contains expected name)\n",
    "            is_saleslt_table = False\n",
    "            category = 'Reference Data'\n",
    "            \n",
    "            if table_name in expected_saleslt_tables:\n",
    "                is_saleslt_table = True\n",
    "            else:\n",
    "                # Check for partial matches (e.g., table might have prefix/suffix)\n",
    "                for expected in expected_saleslt_tables:\n",
    "                    if expected in table_name or table_name in expected:\n",
    "                        is_saleslt_table = True\n",
    "                        break\n",
    "            \n",
    "            if is_saleslt_table:\n",
    "                # Determine category\n",
    "                if any(x in table_name for x in ['customer', 'customeraddress']):\n",
    "                    category = 'Customer Data'\n",
    "                elif any(x in table_name for x in ['product', 'productcategory', 'productdescription', 'productmodel']):\n",
    "                    category = 'Product Catalog'\n",
    "                elif any(x in table_name for x in ['salesorder', 'order']):\n",
    "                    category = 'Sales Transactions'\n",
    "                elif 'address' in table_name:\n",
    "                    category = 'Address Information'\n",
    "                \n",
    "                tables_info.append({\n",
    "                    'table_name': original_name,\n",
    "                    'full_name': original_name,\n",
    "                    'type': 'TABLE',\n",
    "                    'category': category,\n",
    "                    'source': 'lakehouse_shortcuts'\n",
    "                })\n",
    "                \n",
    "                print(f\"   ‚úÖ Found SalesLT table: {original_name} ({category})\")\n",
    "        \n",
    "        print(f\"   üìä Total SalesLT tables found: {len(tables_info)}\")\n",
    "        \n",
    "        if len(tables_info) == 0:\n",
    "            print(\"   ‚ö†Ô∏è  No SalesLT tables found in shortcuts\")\n",
    "            print(\"   üí° Expected tables: Address, Customer, CustomerAddress, Product,\")\n",
    "            print(\"       ProductCategory, ProductDescription, ProductModel,\")\n",
    "            print(\"       ProductModelProductDescription, SalesOrderDetail, SalesOrderHeader\")\n",
    "            print(\"\\n   üîß To fix this:\")\n",
    "            print(\"   1. Go to your lakehouse ‚Üí Tables\")\n",
    "            print(\"   2. Create shortcuts from Azure SQL Database\")\n",
    "            print(\"   3. Select all SalesLT schema tables\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Existing tables discovery failed: {str(e)[:100]}...\")\n",
    "\n",
    "elif connection_method == \"mirrored_database\":\n",
    "    print(\"üí° Using Mirrored Database\")\n",
    "    try:\n",
    "        tables_query = f\"SHOW TABLES IN {connection_name}\"\n",
    "        all_tables = spark.sql(tables_query).toPandas()\n",
    "        \n",
    "        # Filter for SalesLT-like tables\n",
    "        saleslt_indicators = ['customer', 'product', 'address', 'salesorder']\n",
    "        \n",
    "        for _, row in all_tables.iterrows():\n",
    "            table_name = row['tableName'] if 'tableName' in row else row.iloc[0]\n",
    "            \n",
    "            if any(indicator in table_name.lower() for indicator in saleslt_indicators):\n",
    "                category = 'Unknown'\n",
    "                if 'customer' in table_name.lower():\n",
    "                    category = 'Customer Data'\n",
    "                elif 'product' in table_name.lower():\n",
    "                    category = 'Product Catalog'\n",
    "                elif 'salesorder' in table_name.lower():\n",
    "                    category = 'Sales Transactions'\n",
    "                elif 'address' in table_name.lower():\n",
    "                    category = 'Address Information'\n",
    "                \n",
    "                tables_info.append({\n",
    "                    'table_name': table_name,\n",
    "                    'full_name': f\"{connection_name}.{table_name}\",\n",
    "                    'type': 'TABLE',\n",
    "                    'category': category,\n",
    "                    'source': 'mirrored_db'\n",
    "                })\n",
    "        \n",
    "        print(f\"   ‚úÖ Found {len(tables_info)} SalesLT-like tables in mirrored database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Mirrored database discovery failed: {str(e)[:100]}...\")\n",
    "\n",
    "# Fallback: Use standard SalesLT table list\n",
    "if not tables_info:\n",
    "    print(\"üí° Using Standard SalesLT Table List (Fallback)\")\n",
    "    print(\"   ‚ö†Ô∏è  Authentication failed - using expected table structure\")\n",
    "    \n",
    "    standard_saleslt_tables = [\n",
    "        ('Address', 'Address Information'),\n",
    "        ('Customer', 'Customer Data'),\n",
    "        ('CustomerAddress', 'Customer Data'),\n",
    "        ('Product', 'Product Catalog'),\n",
    "        ('ProductCategory', 'Product Catalog'),\n",
    "        ('ProductDescription', 'Product Catalog'),\n",
    "        ('ProductModel', 'Product Catalog'),\n",
    "        ('ProductModelProductDescription', 'Product Catalog'),\n",
    "        ('SalesOrderDetail', 'Sales Transactions'),\n",
    "        ('SalesOrderHeader', 'Sales Transactions')\n",
    "    ]\n",
    "    \n",
    "    for table_name, category in standard_saleslt_tables:\n",
    "        tables_info.append({\n",
    "            'table_name': table_name,\n",
    "            'full_name': f\"{schema_name}.{table_name}\",\n",
    "            'type': 'BASE TABLE',\n",
    "            'category': category,\n",
    "            'source': 'standard_list'\n",
    "        })\n",
    "    \n",
    "    print(f\"   üìã Using standard list of {len(tables_info)} SalesLT tables\")\n",
    "\n",
    "# Display discovered tables\n",
    "print(f\"\\nüìä DISCOVERED TABLES SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if tables_info:\n",
    "    # Group by category\n",
    "    from collections import defaultdict\n",
    "    by_category = defaultdict(list)\n",
    "    \n",
    "    for table in tables_info:\n",
    "        by_category[table['category']].append(table)\n",
    "    \n",
    "    total_tables = len(tables_info)\n",
    "    print(f\"üìã Total Tables: {total_tables}\")\n",
    "    print(f\"üìã Source: {tables_info[0]['source']}\")\n",
    "    print()\n",
    "    \n",
    "    for category, tables in by_category.items():\n",
    "        print(f\"üìÅ {category} ({len(tables)} tables):\")\n",
    "        for table in tables:\n",
    "            print(f\"   üîó {table['table_name']}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Table discovery completed successfully!\")\n",
    "    print(f\"üìã Ready to export {total_tables} tables to Bronze layer\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå NO TABLES DISCOVERED\")\n",
    "    print(\"üí° This indicates authentication/connection issues\")\n",
    "    print(\"üîß Consider setting up lakehouse shortcuts or Data Factory pipelines\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5a1b3",
   "metadata": {},
   "source": [
    "## Step 8 - Table Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48900e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Authentication-Aware Export Function\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_table_to_bronze(table_info, connection_method, connection_name, working_connection=None):\n",
    "    \"\"\"\n",
    "    Export a single table to bronze layer using the appropriate authentication method\n",
    "    Based on Fabric community guidance for SQL Server connectivity\n",
    "    \"\"\"\n",
    "    table_name = table_info['table_name']\n",
    "    full_name = table_info['full_name']\n",
    "    category = table_info['category']\n",
    "    source = table_info['source']\n",
    "    \n",
    "    print(f\"\udd04 Exporting {table_name} ({category})\")\n",
    "    print(f\"   üìã Method: {connection_method}\")\n",
    "    print(f\"   üîó Source: {source}\")\n",
    "    \n",
    "    try:\n",
    "        # Read data based on authentication method\n",
    "        if connection_method == \"spark_jdbc_msi\":\n",
    "            print(f\"   \udd10 Using Spark JDBC with Workspace Identity\")\n",
    "            \n",
    "            # Use JDBC with MSI authentication\n",
    "            df = spark.read \\\n",
    "                .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "                .option(\"url\", working_connection) \\\n",
    "                .option(\"dbtable\", f\"SalesLT.{table_name}\") \\\n",
    "                .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "                .load()\n",
    "                \n",
    "        elif connection_method == \"spark_jdbc_token\":\n",
    "            print(f\"   üîê Using Spark JDBC with Token Authentication\")\n",
    "            \n",
    "            # Get fresh token for this operation\n",
    "            token = mssparkutils.credentials.getToken(audience=\"https://database.windows.net/\")\n",
    "            \n",
    "            connection_properties = {\n",
    "                \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "                \"accessToken\": token\n",
    "            }\n",
    "            \n",
    "            df = spark.read.jdbc(\n",
    "                url=working_connection,\n",
    "                table=f\"SalesLT.{table_name}\",\n",
    "                properties=connection_properties\n",
    "            )\n",
    "            \n",
    "        elif connection_method == \"fabric_sql_direct\":\n",
    "            print(f\"   \udd10 Using Fabric SQL Database Direct\")\n",
    "            \n",
    "            # Use Fabric SQL connection directly\n",
    "            df = spark.sql(f\"SELECT * FROM {connection_name}.SalesLT.{table_name}\")\n",
    "            \n",
    "        elif connection_method == \"existing_tables\":\n",
    "            print(f\"   üîê Using Existing Lakehouse Tables\")\n",
    "            \n",
    "            # Read from existing lakehouse table\n",
    "            df = spark.sql(f\"SELECT * FROM {table_name}\")\n",
    "            \n",
    "        elif connection_method == \"mirrored_database\":\n",
    "            print(f\"   \udd10 Using Mirrored Database\")\n",
    "            \n",
    "            # Read from mirrored database\n",
    "            df = spark.sql(f\"SELECT * FROM {connection_name}.{table_name}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Using manual data setup - creating placeholder\")\n",
    "            \n",
    "            # Create a placeholder DataFrame for fallback\n",
    "            df = spark.createDataFrame(\n",
    "                [{\"status\": \"authentication_required\", \"table\": table_name, \"timestamp\": str(datetime.now())}]\n",
    "            )\n",
    "        \n",
    "        # Get row count and basic info\n",
    "        row_count = df.count()\n",
    "        columns = df.columns\n",
    "        \n",
    "        print(f\"   üìä Loaded: {row_count:,} rows, {len(columns)} columns\")\n",
    "        \n",
    "        # Create bronze layer path using configured target\n",
    "        try:\n",
    "            # Use configured bronze target path from Step 3\n",
    "            base_path = globals().get('BRONZE_TARGET_PATH', 'Files/bronze/saleslt')\n",
    "            bronze_path = f\"{base_path}/{table_name.lower()}\"\n",
    "        except:\n",
    "            # Fallback to default path\n",
    "            bronze_path = f\"Files/bronze/saleslt/{table_name.lower()}\"\n",
    "        \n",
    "        # Add metadata columns\n",
    "        df_with_metadata = df \\\n",
    "            .withColumn(\"_bronze_load_date\", lit(datetime.now().strftime(\"%Y-%m-%d\"))) \\\n",
    "            .withColumn(\"_bronze_load_timestamp\", lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))) \\\n",
    "            .withColumn(\"_source_system\", lit(\"SalesLT\")) \\\n",
    "            .withColumn(\"_source_table\", lit(table_name)) \\\n",
    "            .withColumn(\"_source_schema\", lit(\"SalesLT\")) \\\n",
    "            .withColumn(\"_auth_method\", lit(connection_method)) \\\n",
    "            .withColumn(\"_load_method\", lit(source))\n",
    "        \n",
    "        # Write to bronze layer as Parquet\n",
    "        print(f\"   üíæ Writing to: {bronze_path}\")\n",
    "        \n",
    "        df_with_metadata.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .parquet(bronze_path)\n",
    "        \n",
    "        # Create metadata file\n",
    "        metadata = {\n",
    "            \"table_name\": table_name,\n",
    "            \"category\": category,\n",
    "            \"source_system\": \"SalesLT\",\n",
    "            \"row_count\": row_count,\n",
    "            \"column_count\": len(columns),\n",
    "            \"columns\": columns,\n",
    "            \"load_timestamp\": datetime.now().isoformat(),\n",
    "            \"load_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            \"auth_method\": connection_method,\n",
    "            \"connection_source\": source,\n",
    "            \"bronze_path\": bronze_path,\n",
    "            \"format\": \"parquet\"\n",
    "        }\n",
    "        \n",
    "        # Write metadata as JSON\n",
    "        try:\n",
    "            base_path = globals().get('BRONZE_TARGET_PATH', 'Files/bronze/saleslt')\n",
    "            metadata_path = f\"{base_path}/_metadata/{table_name.lower()}_metadata.json\"\n",
    "        except:\n",
    "            metadata_path = f\"Files/bronze/saleslt/_metadata/{table_name.lower()}_metadata.json\"\n",
    "        \n",
    "        # Create metadata DataFrame and save\n",
    "        metadata_df = spark.createDataFrame([metadata])\n",
    "        \n",
    "        try:\n",
    "            base_path = globals().get('BRONZE_TARGET_PATH', 'Files/bronze/saleslt')\n",
    "            metadata_write_path = f\"{base_path}/_metadata/{table_name.lower()}\"\n",
    "        except:\n",
    "            metadata_write_path = f\"Files/bronze/saleslt/_metadata/{table_name.lower()}\"\n",
    "            \n",
    "        metadata_df.coalesce(1).write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .json(metadata_write_path)\n",
    "        \n",
    "        print(f\"   ‚úÖ Export completed: {row_count:,} rows\")\n",
    "        print(f\"   üìÅ Data: {bronze_path}\")\n",
    "        print(f\"   üìã Metadata: {metadata_write_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_count\": row_count,\n",
    "            \"column_count\": len(columns),\n",
    "            \"bronze_path\": bronze_path,\n",
    "            \"auth_method\": connection_method\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"   ‚ùå Export failed: {error_msg[:100]}...\")\n",
    "        \n",
    "        # For authentication failures, provide specific guidance\n",
    "        if any(auth_keyword in error_msg.lower() for auth_keyword in ['login', 'authentication', 'permission', 'access', 'token']):\n",
    "            print(f\"   üí° Authentication error detected\")\n",
    "            print(f\"   üîß Try: Create lakehouse shortcuts or use Data Factory\")\n",
    "            \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"table_name\": table_name,\n",
    "            \"error\": error_msg,\n",
    "            \"auth_method\": connection_method,\n",
    "            \"recommendation\": \"Check authentication or use lakehouse shortcuts\"\n",
    "        }\n",
    "\n",
    "print(\"\udee0Ô∏è  EXPORT FUNCTION READY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Authentication-aware export function loaded\")\n",
    "print(\"‚úÖ Supports multiple connection methods\")\n",
    "print(\"‚úÖ Includes community-recommended approaches\")\n",
    "print(\"‚úÖ Bronze layer with metadata tracking\")\n",
    "print(\"‚úÖ Authentication error handling\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba9cd2",
   "metadata": {},
   "source": [
    "## Step 9 - Export Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Execute Export with Authentication Handling\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "import time\n",
    "\n",
    "print(\"üöÄ STARTING SALESLT TO BRONZE EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\udcc5 Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üîê Authentication Method: {auth_method}\")\n",
    "print(f\"üìã Connection Method: {connection_method}\")\n",
    "print(f\"üîó Connection/Database: {connection_name}\")\n",
    "print(f\"\udcca Tables to Export: {len(tables_info)}\")\n",
    "print()\n",
    "\n",
    "# Check if we need to handle authentication issues\n",
    "if auth_method == \"manual_authentication_needed\":\n",
    "    print(\"‚ö†Ô∏è  AUTHENTICATION SETUP REQUIRED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üí° Based on community guidance, here are your options:\")\n",
    "    print()\n",
    "    print(\"üîß OPTION 1: Setup Lakehouse Shortcuts (Recommended)\")\n",
    "    print(\"   1. Go to your lakehouse in Fabric\")\n",
    "    print(\"   2. Click 'New shortcut' in Tables section\")\n",
    "    print(\"   3. Select 'Azure SQL Database'\")\n",
    "    print(\"   4. Use server: gaiye-sql-db.sql.fabric.microsoft.com\")\n",
    "    print(\"   5. Database: Gaiye-SQL-DB\")\n",
    "    print(\"   6. Select all SalesLT tables\")\n",
    "    print(\"   7. Re-run this notebook - it will detect shortcuts automatically\")\n",
    "    print()\n",
    "    print(\"\udd27 OPTION 2: Use Data Factory\")\n",
    "    print(\"   1. Create Data Factory pipeline\")\n",
    "    print(\"   2. Use existing connections: 'FabricSql gazho' or 'gaiye-sql-db...'\")\n",
    "    print(\"   3. Create copy activities for each SalesLT table\")\n",
    "    print(\"   4. Target: Lakehouse Files/bronze/saleslt/\")\n",
    "    print()\n",
    "    print(\"üîß OPTION 3: Grant Workspace Identity Access\")\n",
    "    print(\"   1. Go to Azure Portal > SQL Server > gaiye-sql-db\")\n",
    "    print(\"   2. Add your Fabric workspace identity as SQL admin\")\n",
    "    print(\"   3. Or grant db_datareader role on Gaiye-SQL-DB\")\n",
    "    print(\"   4. Re-run this notebook\")\n",
    "    print()\n",
    "    print(\"üí° After setup, re-run from Step 3 to detect working authentication\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Proceed with export if we have a working connection\n",
    "export_results = []\n",
    "successful_exports = 0\n",
    "failed_exports = 0\n",
    "\n",
    "if tables_info and auth_method != \"manual_authentication_needed\":\n",
    "    print(\"üíæ STARTING TABLE EXPORTS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create bronze directory structure\n",
    "    try:\n",
    "        # Ensure bronze directories exist\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "        print(\"‚úÖ Bronze database structure ready\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Export each table\n",
    "    for i, table_info in enumerate(tables_info, 1):\n",
    "        print(f\"\\n[{i}/{len(tables_info)}] {table_info['table_name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Pass the working_connection variable for JDBC methods\n",
    "        connection_to_pass = working_connection if 'working_connection' in locals() else connection_name\n",
    "        \n",
    "        result = export_table_to_bronze(\n",
    "            table_info, \n",
    "            connection_method, \n",
    "            connection_name,\n",
    "            connection_to_pass\n",
    "        )\n",
    "        \n",
    "        export_results.append(result)\n",
    "        \n",
    "        if result['success']:\n",
    "            successful_exports += 1\n",
    "        else:\n",
    "            failed_exports += 1\n",
    "            \n",
    "        # Small delay to avoid overwhelming the connection\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nüéâ EXPORT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚è±Ô∏è  Total Duration: {duration:.1f} seconds\")\n",
    "    print(f\"‚úÖ Successful Exports: {successful_exports}\")\n",
    "    print(f\"‚ùå Failed Exports: {failed_exports}\")\n",
    "    print(f\"üìä Total Tables: {len(tables_info)}\")\n",
    "    \n",
    "    if successful_exports > 0:\n",
    "        print(f\"\\n\udcc1 BRONZE LAYER STRUCTURE:\")\n",
    "        print(f\"   Files/bronze/saleslt/\")\n",
    "        \n",
    "        # Show exported tables by category\n",
    "        from collections import defaultdict\n",
    "        by_category = defaultdict(list)\n",
    "        \n",
    "        for result in export_results:\n",
    "            if result['success']:\n",
    "                # Find table info for this result\n",
    "                table_info = next((t for t in tables_info if t['table_name'] == result['table_name']), None)\n",
    "                if table_info:\n",
    "                    by_category[table_info['category']].append(result)\n",
    "        \n",
    "        for category, results in by_category.items():\n",
    "            print(f\"   üìÇ {category}:\")\n",
    "            for result in results:\n",
    "                print(f\"      üìÑ {result['table_name'].lower()}/ ({result['row_count']:,} rows)\")\n",
    "        \n",
    "        print(f\"\\nüìã METADATA LOCATION:\")\n",
    "        print(f\"   Files/bronze/saleslt/_metadata/\")\n",
    "        \n",
    "        print(f\"\\n\udca1 NEXT STEPS:\")\n",
    "        print(f\"   1. üîç Explore data: Use notebook or SQL endpoint\")\n",
    "        print(f\"   2. üìä Create Silver layer: Clean and transform data\")\n",
    "        print(f\"   3. üîó Build relationships: Join tables for analysis\")\n",
    "        print(f\"   4. üìà Create reports: Use Power BI or Fabric reports\")\n",
    "    \n",
    "    if failed_exports > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  EXPORT ISSUES:\")\n",
    "        for result in export_results:\n",
    "            if not result['success']:\n",
    "                print(f\"   ‚ùå {result['table_name']}: {result.get('error', 'Unknown error')[:80]}...\")\n",
    "        \n",
    "        print(f\"\\nüîß TROUBLESHOOTING:\")\n",
    "        print(f\"   - Check workspace identity permissions on SQL Server\")\n",
    "        print(f\"   - Consider using lakehouse shortcuts instead\")\n",
    "        print(f\"   - Use Data Factory with existing connections\")\n",
    "\n",
    "elif tables_info and auth_method == \"manual_authentication_needed\":\n",
    "    print(\"‚ö†Ô∏è  EXPORT BLOCKED - AUTHENTICATION REQUIRED\")\n",
    "    print(\"üìã Tables identified but authentication failed\")\n",
    "    print(\"üîß Complete authentication setup first, then re-run\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå NO TABLES TO EXPORT\")\n",
    "    print(\"üîß Check connection and authentication setup\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üìã Export process completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Final guidance based on authentication method\n",
    "if auth_method == \"workspace_identity\":\n",
    "    print(f\"\udca1 SUCCESS: Workspace Identity authentication working!\")\n",
    "    print(f\"üéØ This is the recommended production approach\")\n",
    "elif auth_method == \"token_based\":\n",
    "    print(f\"‚ö†Ô∏è  TOKEN-BASED: Monitor for token expiration issues\")\n",
    "    print(f\"üí° Consider switching to Workspace Identity for reliability\")\n",
    "elif auth_method == \"fabric_native\":\n",
    "    print(f\"üí° FABRIC SQL: Direct connection established\")\n",
    "elif auth_method == \"manual_authentication_needed\":\n",
    "    print(f\"üîß SETUP REQUIRED: Follow authentication guidance above\")\n",
    "\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe0da1",
   "metadata": {},
   "source": [
    "## Step 10 - Execute Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Troubleshooting & Next Steps\n",
    "\n",
    "print(\"\udd27 TROUBLESHOOTING & NEXT STEPS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üìã Based on Fabric community guidance and authentication results\")\n",
    "print()\n",
    "\n",
    "# Check if export was successful\n",
    "export_success = 'successful_exports' in locals() and successful_exports > 0\n",
    "auth_issues = auth_method == \"manual_authentication_needed\"\n",
    "\n",
    "if export_success:\n",
    "    print(\"üéâ EXPORT SUCCESSFUL!\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"‚úÖ SalesLT data successfully exported to Bronze layer\")\n",
    "    print(\"‚úÖ Metadata tracking in place\")\n",
    "    print(\"‚úÖ Authentication method working\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üìä VERIFY YOUR DATA:\")\n",
    "    print(\"   # List exported tables\")\n",
    "    print(\"   spark.sql('SHOW TABLES').show()\")\n",
    "    print()\n",
    "    print(\"   # Check a sample table\")\n",
    "    print(\"   spark.sql('SELECT * FROM bronze.customer LIMIT 10').show()\")\n",
    "    print()\n",
    "    print(\"   # Check row counts\")\n",
    "    print(\"   spark.sql('SELECT COUNT(*) FROM bronze.customer').show()\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üîÑ NEXT STEPS:\")\n",
    "    print(\"1. ü•à Create Silver Layer:\")\n",
    "    print(\"   - Clean and standardize data\")\n",
    "    print(\"   - Create business-friendly views\")\n",
    "    print(\"   - Implement data quality checks\")\n",
    "    print()\n",
    "    print(\"2. ü•á Create Gold Layer:\")\n",
    "    print(\"   - Build aggregated tables\")\n",
    "    print(\"   - Create star schema for analytics\")\n",
    "    print(\"   - Optimize for reporting\")\n",
    "    print()\n",
    "    print(\"3. üìä Build Analytics:\")\n",
    "    print(\"   - Connect Power BI to Gold layer\")\n",
    "    print(\"   - Create semantic models\")\n",
    "    print(\"   - Build dashboards and reports\")\n",
    "\n",
    "elif auth_issues:\n",
    "    print(\"üîê AUTHENTICATION SETUP NEEDED\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"‚ö†Ô∏è  Based on community forum insights, authentication with Azure SQL\")\n",
    "    print(\"   from Fabric notebooks has known limitations\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üîß RECOMMENDED SOLUTIONS (in order of preference):\")\n",
    "    print()\n",
    "    print(\"1. üéØ LAKEHOUSE SHORTCUTS (Easiest)\")\n",
    "    print(\"   ‚úÖ No authentication issues\")\n",
    "    print(\"   ‚úÖ Data appears as native lakehouse tables\")\n",
    "    print(\"   ‚úÖ Automatic refresh capabilities\")\n",
    "    print()\n",
    "    print(\"   Steps:\")\n",
    "    print(\"   a) Go to your lakehouse > Tables section\")\n",
    "    print(\"   b) Click 'New shortcut' > 'Azure SQL Database'\")\n",
    "    print(\"   c) Server: gaiye-sql-db.sql.fabric.microsoft.com\")\n",
    "    print(\"   d) Database: Gaiye-SQL-DB\")\n",
    "    print(\"   e) Select schema: SalesLT\")\n",
    "    print(\"   f) Select all tables or specific ones\")\n",
    "    print(\"   g) Re-run this notebook - it will detect them automatically\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. üîÑ DATA FACTORY PIPELINES\")\n",
    "    print(\"   ‚úÖ Your existing connections work in Data Factory\")\n",
    "    print(\"   ‚úÖ Scheduled refresh capabilities\")\n",
    "    print(\"   ‚úÖ Full ETL/ELT capabilities\")\n",
    "    print()\n",
    "    print(\"   Steps:\")\n",
    "    print(\"   a) Create new Data Factory\")\n",
    "    print(\"   b) Use connection: 'gaiye-sql-db.sql.fabric.microsoft;SalesLT gazho'\")\n",
    "    print(\"   c) Create copy activities for each table\")\n",
    "    print(\"   d) Target: Lakehouse Files/bronze/saleslt/\")\n",
    "    print(\"   e) Schedule for regular updates\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. üîë WORKSPACE IDENTITY SETUP\")\n",
    "    print(\"   ‚ö†Ô∏è  Requires Azure admin permissions\")\n",
    "    print(\"   ‚ö†Ô∏è  Community reports mixed success\")\n",
    "    print()\n",
    "    print(\"   Steps:\")\n",
    "    print(\"   a) Get your Fabric workspace identity name\")\n",
    "    print(\"   b) In Azure Portal: SQL Server > Security > Entra ID admin\")\n",
    "    print(\"   c) Add workspace identity as admin or db_datareader\")\n",
    "    print(\"   d) Test connection in this notebook\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üìã COMMUNITY INSIGHTS:\")\n",
    "    print(\"   - Workspace Identity more reliable than Managed Identity\")\n",
    "    print(\"   - Token-based auth often returns user tokens vs workspace tokens\")\n",
    "    print(\"   - JDBC connector issues common in Fabric notebooks\")\n",
    "    print(\"   - Data Factory works better for SQL Server connections\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùì MIXED RESULTS\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Some tables may have exported successfully\")\n",
    "    print(\"Check the export summary above for details\")\n",
    "\n",
    "print()\n",
    "print(\"üîó HELPFUL RESOURCES:\")\n",
    "print(\"   üìö Fabric Documentation: https://docs.microsoft.com/fabric\")\n",
    "print(\"   üè† Community Forum: https://community.fabric.microsoft.com\")\n",
    "print(\"   üìñ Lakehouse Shortcuts: Search 'Create shortcuts to Azure SQL'\")\n",
    "print(\"   üè≠ Data Factory: Search 'Copy data from Azure SQL to Lakehouse'\")\n",
    "\n",
    "print()\n",
    "print(\"üí° PRO TIPS:\")\n",
    "print(\"   - Lakehouse shortcuts are often easier than notebook connections\")\n",
    "print(\"   - Data Factory pipelines handle authentication better\")\n",
    "print(\"   - Test with small tables first\")\n",
    "print(\"   - Monitor workspace identity permissions carefully\")\n",
    "print(\"   - Consider hybrid approach: shortcuts + notebooks for processing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã Notebook completed successfully!\")\n",
    "print(\"\udd27 Choose your preferred authentication method and proceed\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a60629",
   "metadata": {},
   "source": [
    "## Step 11 - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLICIT TARGET LAKEHOUSE VERIFICATION\n",
    "print(\"üîç EXPLICIT BRONZE LAKEHOUSE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "TARGET_LAKEHOUSE = \"RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze\"\n",
    "print(f\"üéØ Target Lakehouse: {TARGET_LAKEHOUSE}\")\n",
    "\n",
    "# Get the configured bronze path\n",
    "bronze_base = globals().get('BRONZE_TARGET_PATH', 'Files/bronze/saleslt')\n",
    "print(f\"üìÅ Configured Bronze Path: {bronze_base}\")\n",
    "\n",
    "# Method 1: Check via mount points\n",
    "print(f\"\\nüîç METHOD 1: Mount Point Analysis\")\n",
    "try:\n",
    "    mounts = dbutils.fs.mounts()\n",
    "    print(f\"üìÇ All available mounts:\")\n",
    "    \n",
    "    target_found = False\n",
    "    for mount in mounts:\n",
    "        mount_point = mount.mountPoint\n",
    "        source = mount.source\n",
    "        print(f\"   üîó {mount_point} ‚Üí {source}\")\n",
    "        \n",
    "        if TARGET_LAKEHOUSE.lower() in source.lower():\n",
    "            target_found = True\n",
    "            print(f\"   ‚úÖ TARGET FOUND: {mount_point}\")\n",
    "            \n",
    "            # Check if bronze data exists in this mount\n",
    "            try:\n",
    "                bronze_check = f\"{mount_point}bronze/saleslt\"\n",
    "                files = dbutils.fs.ls(bronze_check)\n",
    "                print(f\"   üìä Bronze data found: {len(files)} items in {bronze_check}\")\n",
    "                for file in files[:5]:  # Show first 5\n",
    "                    print(f\"      üìÑ {file.name}\")\n",
    "                if len(files) > 5:\n",
    "                    print(f\"      ... and {len(files)-5} more\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è No bronze data yet in {bronze_check}: {str(e)[:50]}...\")\n",
    "    \n",
    "    if not target_found:\n",
    "        print(f\"   ‚ùå Target lakehouse not found in mounts\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Mount analysis failed: {str(e)}\")\n",
    "\n",
    "# Method 2: Check current lakehouse Files directory\n",
    "print(f\"\\nüîç METHOD 2: Current Lakehouse Files Check\")\n",
    "try:\n",
    "    files_content = dbutils.fs.ls(\"Files/\")\n",
    "    print(f\"üìÅ Current lakehouse Files/ contains:\")\n",
    "    for item in files_content:\n",
    "        print(f\"   üìÇ {item.name}\")\n",
    "        \n",
    "        if item.name.rstrip('/') == 'bronze':\n",
    "            print(f\"   üéØ Bronze folder found! Checking contents...\")\n",
    "            try:\n",
    "                bronze_content = dbutils.fs.ls(\"Files/bronze/\")\n",
    "                for bronze_item in bronze_content:\n",
    "                    print(f\"      üìÇ {bronze_item.name}\")\n",
    "                    \n",
    "                    if bronze_item.name.rstrip('/') == 'saleslt':\n",
    "                        print(f\"      üéâ SalesLT folder found! Checking tables...\")\n",
    "                        try:\n",
    "                            saleslt_content = dbutils.fs.ls(\"Files/bronze/saleslt/\")\n",
    "                            print(f\"      üìä Found {len(saleslt_content)} items:\")\n",
    "                            for table_item in saleslt_content:\n",
    "                                print(f\"         üìÑ {table_item.name}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"      ‚ö†Ô∏è Could not read saleslt: {str(e)[:50]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Could not read bronze: {str(e)[:50]}...\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Files check failed: {str(e)}\")\n",
    "\n",
    "# Method 3: Direct path check using configured path\n",
    "print(f\"\\nüîç METHOD 3: Configured Path Direct Check\")\n",
    "try:\n",
    "    # Check the exact path we're supposed to write to\n",
    "    check_path = bronze_base\n",
    "    print(f\"üìÅ Checking configured path: {check_path}\")\n",
    "    \n",
    "    content = dbutils.fs.ls(check_path)\n",
    "    print(f\"‚úÖ Path accessible! Contains {len(content)} items:\")\n",
    "    for item in content[:10]:  # Show first 10\n",
    "        print(f\"   üìÑ {item.name}\")\n",
    "        \n",
    "        # If it's a directory, show row count\n",
    "        if item.isDir() and not item.name.startswith('_'):\n",
    "            table_name = item.name.rstrip('/')\n",
    "            try:\n",
    "                table_path = f\"{check_path}/{table_name}\"\n",
    "                df = spark.read.parquet(table_path)\n",
    "                row_count = df.count()\n",
    "                print(f\"      üìä {row_count:,} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Could not read: {str(e)[:30]}...\")\n",
    "    \n",
    "    if len(content) > 10:\n",
    "        print(f\"   ... and {len(content)-10} more items\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configured path check failed: {str(e)}\")\n",
    "    print(f\"üí° This suggests data was not written to the intended location\")\n",
    "\n",
    "# Method 4: Workspace context check\n",
    "print(f\"\\nüîç METHOD 4: Workspace Context Analysis\")\n",
    "try:\n",
    "    # Check current database/lakehouse context\n",
    "    current_db = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "    print(f\"üìä Current database context: {current_db}\")\n",
    "    \n",
    "    # Check if we're in the right workspace\n",
    "    if TARGET_LAKEHOUSE.lower() in current_db.lower():\n",
    "        print(f\"‚úÖ We appear to be in the target lakehouse context!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Current context may not be the target lakehouse\")\n",
    "        print(f\"üí° Expected: {TARGET_LAKEHOUSE}\")\n",
    "        print(f\"üí° Current: {current_db}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Context check failed: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéØ SUMMARY:\")\n",
    "print(f\"   üìã Target: {TARGET_LAKEHOUSE}\")\n",
    "print(f\"   üìÅ Path: {bronze_base}\")\n",
    "print(f\"   üîç Use the above methods to locate your exported data\")\n",
    "print(f\"   üí° If data not found, re-run Step 3 configuration and Step 10 export\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Verify files exist in bronze layer\n",
    "print(\"üîç VALIDATION: Checking bronze layer contents...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "bronze_base_path = \"Files/bronze\"\n",
    "\n",
    "try:\n",
    "    # List bronze layer directories\n",
    "    if os.path.exists(bronze_base_path):\n",
    "        bronze_contents = os.listdir(bronze_base_path)\n",
    "        \n",
    "        print(f\"üìÅ Bronze layer contains {len(bronze_contents)} items:\")\n",
    "        for item in sorted(bronze_contents):\n",
    "            item_path = os.path.join(bronze_base_path, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                # Check for parquet files in the directory\n",
    "                files = [f for f in os.listdir(item_path) if f.endswith('.parquet')]\n",
    "                print(f\"   üìÇ {item}/ ({len(files)} parquet files)\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {item}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Bronze layer path not found: {bronze_base_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error validating bronze layer: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Validation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FABRIC LAKEHOUSE VALIDATION - Check bronze layer in Fabric\n",
    "print(\"üîç FABRIC LAKEHOUSE VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Check if bronze layer exists in Fabric lakehouse\n",
    "    bronze_tables = spark.sql(\"SHOW TABLES\").filter(\"tableName LIKE '*bronze*' OR tableName LIKE '*saleslt*'\").toPandas()\n",
    "    \n",
    "    if len(bronze_tables) > 0:\n",
    "        print(f\"‚úÖ Found {len(bronze_tables)} bronze/saleslt related tables:\")\n",
    "        for _, row in bronze_tables.iterrows():\n",
    "            print(f\"   üìä {row['tableName']}\")\n",
    "    \n",
    "    # Check Files directory using Fabric file system\n",
    "    try:\n",
    "        # Try to list bronze directory using dbutils (Fabric file system)\n",
    "        files_list = dbutils.fs.ls(\"Files/bronze/saleslt/\")\n",
    "        print(f\"\\n‚úÖ Bronze layer found with {len(files_list)} items:\")\n",
    "        for file_info in files_list:\n",
    "            print(f\"   üìÇ {file_info.name}\")\n",
    "            \n",
    "    except Exception as fs_error:\n",
    "        # Alternative: Check using mssparkutils\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            files_list = mssparkutils.fs.ls(\"Files/bronze/saleslt/\")\n",
    "            print(f\"\\n‚úÖ Bronze layer found with {len(files_list)} items:\")\n",
    "            for file_info in files_list:\n",
    "                print(f\"   üìÇ {file_info.name}\")\n",
    "        except:\n",
    "            print(f\"\\n‚ö†Ô∏è Could not access Files/bronze/saleslt/ via file system\")\n",
    "            print(f\"   üí° Check manually in Fabric lakehouse Files section\")\n",
    "    \n",
    "    # Quick data sample if customer table exists\n",
    "    try:\n",
    "        customer_df = spark.read.parquet(\"Files/bronze/saleslt/customer\")\n",
    "        customer_count = customer_df.count()\n",
    "        print(f\"\\nüìä SAMPLE DATA CHECK:\")\n",
    "        print(f\"   ‚úÖ Customer table: {customer_count:,} rows\")\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(f\"   üìã Sample columns: {', '.join(customer_df.columns[:5])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nüìä SAMPLE DATA CHECK:\")\n",
    "        print(f\"   ‚ö†Ô∏è Could not read customer data: {str(e)[:100]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation error: {str(e)}\")\n",
    "\n",
    "print(\"\\nüéØ TO VERIFY SUCCESS:\")\n",
    "print(\"   1. Go to your Fabric lakehouse\")\n",
    "print(\"   2. Check Files > bronze > saleslt > [table folders]\")\n",
    "print(\"   3. Each folder should contain .parquet files\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ef76d",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this notebook, you should have:\n",
    "\n",
    "1. **Exported Data**: All SalesLT tables saved as Parquet files in the bronze layer\n",
    "2. **Metadata**: Each file includes source tracking and extraction timestamps\n",
    "3. **Summary Report**: JSON file with complete export details\n",
    "4. **File Organization**: Each table in its own folder within `/Files/bronze/`\n",
    "\n",
    "### Recommended Next Actions:\n",
    "\n",
    "- **Review the data**: Check the bronze layer files to ensure data quality\n",
    "- **Create silver layer transformations**: Build data pipelines to clean and standardize the data\n",
    "- **Map to retail model**: Align the SalesLT schema with your retail data model requirements\n",
    "- **Set up monitoring**: Create alerts for data freshness and quality\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "- **Connection issues**: Verify your Fabric workspace has access to the SQL database\n",
    "- **Permission errors**: Ensure you have read access to SalesLT schema and write access to the lakehouse\n",
    "- **Large tables**: For very large tables, consider implementing chunked processing\n",
    "- **Data types**: Some SQL Server data types may need special handling during export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d9584",
   "metadata": {},
   "source": [
    "## Data Factory Pipeline Alternative\n",
    "\n",
    "If you prefer using **Data Factory** instead of this notebook, here's how to create a dynamic pipeline:\n",
    "\n",
    "### 1. Create Parameterized Pipeline\n",
    "\n",
    "Create a Data Factory pipeline with these components:\n",
    "\n",
    "**Pipeline Parameters:**\n",
    "- `sourceSchema` = \"SalesLT\"\n",
    "- `targetPath` = \"Files/bronze/saleslt\"\n",
    "\n",
    "**Activities:**\n",
    "1. **Lookup Activity**: Get table list from `INFORMATION_SCHEMA.TABLES`\n",
    "2. **ForEach Activity**: Loop through discovered tables  \n",
    "3. **Copy Data Activity**: Copy each table dynamically\n",
    "\n",
    "### 2. Connection Configuration\n",
    "\n",
    "**Source (Fabric SQL Database):**\n",
    "- **Server**: `Gaiye-SQL-DB.sql.fabric.microsoft.com`\n",
    "- **Database**: `Gaiye-SQL-DB`\n",
    "- **Authentication**: `Organizational Account`\n",
    "\n",
    "**Sink (Lakehouse):**\n",
    "- **Path**: `@concat(pipeline().parameters.targetPath, '/', toLower(item().TABLE_NAME), '/', toLower(item().TABLE_NAME), '.parquet')`\n",
    "- **Format**: `Parquet`\n",
    "\n",
    "### 3. Dynamic Query\n",
    "\n",
    "**Lookup Query:**\n",
    "```sql\n",
    "SELECT TABLE_NAME \n",
    "FROM INFORMATION_SCHEMA.TABLES \n",
    "WHERE TABLE_SCHEMA = '@{pipeline().parameters.sourceSchema}' \n",
    "AND TABLE_TYPE = 'BASE TABLE'\n",
    "```\n",
    "\n",
    "**Copy Data Source Query:**\n",
    "```sql\n",
    "SELECT * FROM [@{pipeline().parameters.sourceSchema}].[@{item().TABLE_NAME}]\n",
    "```\n",
    "\n",
    "This approach gives you the same dynamic table discovery but through Data Factory's visual interface."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
