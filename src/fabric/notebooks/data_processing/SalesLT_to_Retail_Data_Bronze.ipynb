{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4864afcf",
   "metadata": {},
   "source": [
    "# SalesLT to Retail Data Bronze Layer\n",
    "\n",
    "**Objective**: Copy all SalesLT tables to bronze layer in Retail Data lakehouse using simplified Fabric PySpark approach\n",
    "\n",
    "**Prerequisites**:\n",
    "- Microsoft Fabric environment with PySpark runtime\n",
    "- Access to SalesLT tables (via shortcuts or direct tables)\n",
    "- Write permissions to target bronze lakehouse\n",
    "- Source lakehouse shortcut configured in bronze lakehouse\n",
    "\n",
    "**Setup Strategy**:\n",
    "1. **Current Context**: Running in bronze lakehouse (`RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze`)\n",
    "2. **Source Access**: Access SalesLT tables from `Gaiye_Test_Lakehouse` via shortcuts\n",
    "3. **Data Flow**: Shortcut tables â†’ Bronze Files + Tables with metadata enrichment\n",
    "\n",
    "**Expected Tables**: address, customer, customeraddress, product, productcategory, productdescription, productmodel, productmodelproductdescription, salesorderdetail, salesorderheader\n",
    "\n",
    "**Workflow Options**:\n",
    "- **First Run**: Execute all steps (1-5) for initial setup and validation\n",
    "- **Subsequent Runs**: Execute steps 1, 4, 5 only (Setup â†’ Process â†’ Validate) for regular data refreshes\n",
    "- **Quick Refresh**: Steps 2-3 can be skipped once environment is validated and working\n",
    "- **Optimized Refresh**: Execute steps 1, 4 only (Setup â†’ Process) for fastest data updates once pipeline is proven reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdef79f",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ae949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Configuration\n",
    "BRONZE_TARGET_PATH = \"Files/SalesLT/\"\n",
    "SOURCE_SYSTEM = \"SalesLT\"\n",
    "SOURCE_DATABASE = \"Gaiye_Test_Lakehouse\"\n",
    "LOAD_TIMESTAMP = datetime.now().isoformat()\n",
    "LOAD_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Expected SalesLT tables\n",
    "EXPECTED_TABLES = [\n",
    "    'address', 'customer', 'customeraddress', 'product', \n",
    "    'productcategory', 'productdescription', 'productmodel',\n",
    "    'productmodelproductdescription', 'salesorderdetail', 'salesorderheader'\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ SalesLT to Retail Data Bronze Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"âœ… Libraries imported\")\n",
    "print(f\"ğŸ“… Load timestamp: {LOAD_TIMESTAMP}\")\n",
    "print(f\"ğŸ¯ Target path: {BRONZE_TARGET_PATH}\")\n",
    "print(f\"ğŸ“¥ Source database: {SOURCE_DATABASE}\")\n",
    "print(f\"ğŸ“Š Expected tables: {len(EXPECTED_TABLES)}\")\n",
    "print(f\"âœ… Microsoft Fabric PySpark environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b215c",
   "metadata": {},
   "source": [
    "## Step 2: Discover Available Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover source tables from Gaiye_Test_Lakehouse\n",
    "print(\"ğŸ” DISCOVERING SOURCE TABLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check source lakehouse context\n",
    "print(f\"ğŸ  Current context: Bronze lakehouse (target)\")\n",
    "print(f\"ğŸ“¥ Source database: {SOURCE_DATABASE}\")\n",
    "print(f\"ğŸ“¤ Target path: {BRONZE_TARGET_PATH}\")\n",
    "print(f\"ğŸ“‹ Expected table names: {', '.join(EXPECTED_TABLES)}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Get tables from source database specifically\n",
    "    print(f\"ğŸ” Querying tables from {SOURCE_DATABASE}...\")\n",
    "    source_tables_df = spark.sql(f\"SHOW TABLES IN {SOURCE_DATABASE}\").toPandas()\n",
    "    \n",
    "    print(f\"âœ… Total tables found in source: {len(source_tables_df)}\")\n",
    "    print(f\"âœ… Spark SQL connection confirmed\")\n",
    "    \n",
    "    if len(source_tables_df) > 0:\n",
    "        # Handle flexible column naming\n",
    "        table_column = None\n",
    "        for possible_col in ['tableName', 'table_name', 'name']:\n",
    "            if possible_col in source_tables_df.columns:\n",
    "                table_column = possible_col\n",
    "                break\n",
    "        \n",
    "        if table_column is None:\n",
    "            table_column = source_tables_df.columns[0]\n",
    "            print(f\"ğŸ” Using column '{table_column}' as table name\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ All available tables in {SOURCE_DATABASE}:\")\n",
    "        for _, row in source_tables_df.iterrows():\n",
    "            table_name = row[table_column]\n",
    "            # Check if this matches our expected tables\n",
    "            is_expected = any(expected.lower() == table_name.lower() for expected in EXPECTED_TABLES)\n",
    "            marker = \"ğŸ¯\" if is_expected else \"ğŸ“‹\"\n",
    "            print(f\"   {marker} {table_name}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ No tables found in {SOURCE_DATABASE}!\")\n",
    "        print(\"ğŸ’¡ Verify the source lakehouse contains data\")\n",
    "    \n",
    "    # Find matching tables from our expected list\n",
    "    available_tables = []\n",
    "    missing_tables = []\n",
    "    table_mapping = {}  # Map expected names to actual names\n",
    "    \n",
    "    if len(source_tables_df) > 0:\n",
    "        # Get actual table names (case-sensitive)\n",
    "        actual_table_names = source_tables_df[table_column].tolist()\n",
    "        actual_table_names_lower = [name.lower() for name in actual_table_names]\n",
    "        \n",
    "        print(f\"\\nğŸ” Matching expected tables with available tables:\")\n",
    "        for expected_table in EXPECTED_TABLES:\n",
    "            # Find case-insensitive match\n",
    "            matching_indices = [i for i, name in enumerate(actual_table_names_lower) \n",
    "                              if name == expected_table.lower()]\n",
    "            \n",
    "            if matching_indices:\n",
    "                # Use the actual table name (with correct case)\n",
    "                actual_table_name = actual_table_names[matching_indices[0]]\n",
    "                available_tables.append(actual_table_name)\n",
    "                table_mapping[expected_table] = actual_table_name\n",
    "                print(f\"   âœ… Found: {expected_table} â†’ {actual_table_name}\")\n",
    "            else:\n",
    "                missing_tables.append(expected_table)\n",
    "                print(f\"   âŒ Missing: {expected_table}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DISCOVERY SUMMARY\")\n",
    "    print(f\"âœ… Available tables found: {len(available_tables)}\")\n",
    "    print(f\"âŒ Missing tables: {len(missing_tables)}\")\n",
    "    \n",
    "    if len(available_tables) == 0:\n",
    "        print(f\"\\nâš ï¸ No expected tables found in {SOURCE_DATABASE}!\")\n",
    "        print(\"ğŸ’¡ Check that the source lakehouse contains the SalesLT tables\")\n",
    "        print(\"ğŸ’¡ Verify table names match expected format\")\n",
    "        print()\n",
    "        print(\"ğŸ”§ TROUBLESHOOTING:\")\n",
    "        print(\"1. Ensure you have a shortcut to the source lakehouse\")\n",
    "        print(\"2. Refresh the lakehouse view in Fabric\")\n",
    "        print(\"3. Check the source lakehouse contains data\")\n",
    "        \n",
    "        # Show what was actually found for debugging\n",
    "        if len(source_tables_df) > 0:\n",
    "            print(f\"\\nğŸ” Debug - Available table names in source:\")\n",
    "            for _, row in source_tables_df.iterrows():\n",
    "                print(f\"   ğŸ“‹ '{row[table_column]}'\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ‰ Ready to process {len(available_tables)} tables!\")\n",
    "        \n",
    "        # Store for next steps (use actual table names with correct casing)\n",
    "        TABLES_TO_PROCESS = available_tables\n",
    "        TABLE_MAPPING = table_mapping\n",
    "        print(f\"ğŸ“ Tables to process: {', '.join(TABLES_TO_PROCESS)}\")\n",
    "        print(f\"ğŸš€ Source: {SOURCE_DATABASE} â†’ Target: {BRONZE_TARGET_PATH}\")\n",
    "        \n",
    "        if missing_tables:\n",
    "            print(f\"\\nâš ï¸ Missing tables (will be skipped): {', '.join(missing_tables)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to discover tables: {str(e)}\")\n",
    "    print()\n",
    "    print(\"ğŸ”§ TROUBLESHOOTING:\")\n",
    "    print(f\"1. Ensure {SOURCE_DATABASE} is accessible from this lakehouse\")\n",
    "    print(\"2. Check lakehouse attachment/shortcut configuration\")\n",
    "    print(\"3. Refresh the lakehouse view in Fabric\")\n",
    "    print(\"4. Verify source lakehouse permissions\")\n",
    "    TABLES_TO_PROCESS = []\n",
    "    TABLE_MAPPING = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a49d7",
   "metadata": {},
   "source": [
    "## Step 3: Test Bronze Layer Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae119fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test write access to bronze layer\n",
    "print(\"ğŸ§ª TESTING BRONZE LAYER ACCESS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_path = f\"{BRONZE_TARGET_PATH}_test_access\"\n",
    "\n",
    "try:\n",
    "    # Create test data\n",
    "    test_data = [(\"access_test\", LOAD_TIMESTAMP, \"success\")]\n",
    "    test_df = spark.createDataFrame(test_data, [\"test_type\", \"timestamp\", \"status\"])\n",
    "    \n",
    "    # Test write to bronze location\n",
    "    print(f\"ğŸ“ Testing write to: {test_path}\")\n",
    "    test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "    \n",
    "    # Verify read access\n",
    "    verify_df = spark.read.parquet(test_path)\n",
    "    test_count = verify_df.count()\n",
    "    \n",
    "    print(f\"âœ… Write access confirmed\")\n",
    "    print(f\"âœ… Read access confirmed ({test_count} test records)\")\n",
    "    print(f\"ğŸ¯ Target path ready: {BRONZE_TARGET_PATH}\")\n",
    "    \n",
    "    # Display test data to confirm\n",
    "    print(\"\\nğŸ“‹ Test data sample:\")\n",
    "    verify_df.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Bronze layer access test failed: {str(e)}\")\n",
    "    print(\"ğŸ’¡ Ensure you have write permissions to the current lakehouse\")\n",
    "    print(\"ğŸ’¡ Check Files directory structure and permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09179308",
   "metadata": {},
   "source": [
    "## Step 4: Process SalesLT Tables to Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy SalesLT tables to bronze layer with metadata enrichment\n",
    "print(\"ğŸš€ PROCESSING SALESLT TABLES TO BRONZE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n",
    "    print(\"âŒ No tables to process. Run previous steps first.\")\n",
    "else:\n",
    "    print(f\"ğŸ“‹ Processing {len(TABLES_TO_PROCESS)} tables\")\n",
    "    print(f\"ğŸ“¥ Source: {SOURCE_DATABASE}\")\n",
    "    print(f\"ğŸ“¤ Target: {BRONZE_TARGET_PATH}\")\n",
    "    print(f\"ğŸ“… Load date: {LOAD_DATE}\")\n",
    "    print()\n",
    "    \n",
    "    # Processing results tracking\n",
    "    results = []\n",
    "    total_rows_processed = 0\n",
    "    \n",
    "    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n",
    "        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Processing {table_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read source table using qualified name\n",
    "            print(f\"   ğŸ“– Reading from {SOURCE_DATABASE}.{table_name}...\")\n",
    "            source_df = spark.sql(f\"SELECT * FROM {SOURCE_DATABASE}.{table_name}\")\n",
    "            row_count = source_df.count()\n",
    "            \n",
    "            print(f\"   âœ… Source data loaded: {row_count:,} rows\")\n",
    "            \n",
    "            # Add bronze layer metadata columns\n",
    "            print(f\"   ğŸ·ï¸ Adding metadata columns...\")\n",
    "            bronze_df = source_df \\\n",
    "                .withColumn(\"_load_date\", lit(LOAD_DATE)) \\\n",
    "                .withColumn(\"_load_timestamp\", lit(LOAD_TIMESTAMP)) \\\n",
    "                .withColumn(\"_source_system\", lit(SOURCE_SYSTEM)) \\\n",
    "                .withColumn(\"_source_table\", lit(table_name)) \\\n",
    "                .withColumn(\"_processing_timestamp\", current_timestamp()) \\\n",
    "                .withColumn(\"_record_source\", lit(\"cross_lakehouse_copy\")) \\\n",
    "                .withColumn(\"_load_method\", lit(\"spark_sql_full_extract\")) \\\n",
    "                .withColumn(\"_source_database\", lit(SOURCE_DATABASE)) \\\n",
    "                .withColumn(\"_target_path\", lit(f\"{BRONZE_TARGET_PATH}{table_name}\"))\n",
    "            \n",
    "            # Write to bronze layer as files\n",
    "            table_target_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n",
    "            print(f\"   ğŸ’¾ Writing to Files: {table_target_path}\")\n",
    "            \n",
    "            bronze_df.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .parquet(table_target_path)\n",
    "            \n",
    "            print(f\"   âœ… Files saved: {row_count:,} rows\")\n",
    "            \n",
    "            # Also create lakehouse table (data in memory, efficient to do both)\n",
    "            lakehouse_table_name = f\"bronze_{table_name}\"\n",
    "            print(f\"   ğŸ¢ Creating lakehouse table: {lakehouse_table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Create/replace lakehouse table\n",
    "                bronze_df.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"overwriteSchema\", \"true\") \\\n",
    "                    .saveAsTable(lakehouse_table_name)\n",
    "                \n",
    "                print(f\"   âœ… Table created: {lakehouse_table_name}\")\n",
    "                table_creation_status = \"success\"\n",
    "                \n",
    "            except Exception as table_error:\n",
    "                table_error_msg = str(table_error)[:60]\n",
    "                print(f\"   âš ï¸ Table creation failed: {table_error_msg}...\")\n",
    "                table_creation_status = \"file_only\"\n",
    "            \n",
    "            # Success tracking\n",
    "            total_rows_processed += row_count\n",
    "            results.append({\n",
    "                \"table\": table_name,\n",
    "                \"rows\": row_count,\n",
    "                \"status\": \"success\",\n",
    "                \"target_path\": table_target_path,\n",
    "                \"lakehouse_table\": lakehouse_table_name,\n",
    "                \"table_status\": table_creation_status\n",
    "            })\n",
    "            \n",
    "            print(f\"   ğŸ‰ Successfully processed {row_count:,} rows (Files + Table)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:100]\n",
    "            results.append({\n",
    "                \"table\": table_name,\n",
    "                \"rows\": 0,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg\n",
    "            })\n",
    "            print(f\"   âŒ Failed: {error_msg}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Processing summary\n",
    "    successful = [r for r in results if r[\"status\"] == \"success\"]\n",
    "    failed = [r for r in results if r[\"status\"] == \"failed\"]\n",
    "    tables_created = [r for r in successful if r.get(\"table_status\") == \"success\"]\n",
    "    files_only = [r for r in successful if r.get(\"table_status\") == \"file_only\"]\n",
    "    \n",
    "    print(\"ğŸ‰ PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… Successfully processed: {len(successful)} tables\")\n",
    "    print(f\"âŒ Failed processing: {len(failed)} tables\")\n",
    "    print(f\"ğŸ“Š Total rows processed: {total_rows_processed:,}\")\n",
    "    print(f\"ğŸ“ Files created: {len(successful)} (all)\")\n",
    "    print(f\"ğŸ¢ Lakehouse tables created: {len(tables_created)}\")\n",
    "    print(f\"âš ï¸ Files only (table creation failed): {len(files_only)}\")\n",
    "    print(f\"ğŸ“… Processing completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if successful:\n",
    "        print(f\"\\nğŸ“ Bronze layer file structure:\")\n",
    "        print(f\"{BRONZE_TARGET_PATH}\")\n",
    "        for result in successful:\n",
    "            table_marker = \"ğŸ¢\" if result.get(\"table_status\") == \"success\" else \"ğŸ“\"\n",
    "            print(f\"â”œâ”€â”€ {result['table']}/ ({result['rows']:,} rows) {table_marker}\")\n",
    "    \n",
    "    if tables_created:\n",
    "        print(f\"\\nğŸ¢ Lakehouse tables created:\")\n",
    "        for result in tables_created:\n",
    "            print(f\"âœ… {result['lakehouse_table']} ({result['rows']:,} rows)\")\n",
    "    \n",
    "    if files_only:\n",
    "        print(f\"\\nğŸ“ Files only (table creation issues):\")\n",
    "        for result in files_only:\n",
    "            print(f\"âš ï¸ {result['table']} â†’ Files saved, table creation failed\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nâš ï¸ Processing failures:\")\n",
    "        for result in failed:\n",
    "            print(f\"âŒ {result['table']}: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Bronze data ready for downstream processing!\")\n",
    "    print(f\"ğŸ“ Files stored in: {BRONZE_TARGET_PATH}\")\n",
    "    if tables_created:\n",
    "        print(f\"ğŸ¢ Tables available in: {len(tables_created)} lakehouse tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b1ba4",
   "metadata": {},
   "source": [
    "## Step 5: Validate Bronze Layer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate bronze layer data quality and completeness\n",
    "print(\"ğŸ” VALIDATING BRONZE LAYER DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n",
    "    print(\"âŒ No tables to validate. Run previous steps first.\")\n",
    "else:\n",
    "    validation_results = []\n",
    "    total_bronze_rows = 0\n",
    "    \n",
    "    print(f\"ğŸ“‹ Validating {len(TABLES_TO_PROCESS)} bronze files\")\n",
    "    print(f\"ğŸ“ Target location: {BRONZE_TARGET_PATH}\")\n",
    "    print()\n",
    "    \n",
    "    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n",
    "        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Validating {table_name}...\")\n",
    "        \n",
    "        try:\n",
    "            file_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n",
    "            lakehouse_table_name = f\"bronze_{table_name}\"\n",
    "            \n",
    "            # Read bronze data files\n",
    "            bronze_df = spark.read.parquet(file_path)\n",
    "            bronze_count = bronze_df.count()\n",
    "            \n",
    "            # Check if lakehouse table exists and validate\n",
    "            table_count = 0\n",
    "            table_exists = False\n",
    "            try:\n",
    "                table_df = spark.table(lakehouse_table_name)\n",
    "                table_count = table_df.count()\n",
    "                table_exists = True\n",
    "            except Exception:\n",
    "                table_exists = False\n",
    "            \n",
    "            # Check metadata columns\n",
    "            sample_row = bronze_df.select(\n",
    "                \"_load_date\", \n",
    "                \"_source_system\", \n",
    "                \"_source_table\",\n",
    "                \"_load_method\"\n",
    "            ).first()\n",
    "            \n",
    "            # Get column count for schema validation\n",
    "            column_count = len(bronze_df.columns)\n",
    "            metadata_columns = [col for col in bronze_df.columns if col.startswith('_')]\n",
    "            business_columns = [col for col in bronze_df.columns if not col.startswith('_')]\n",
    "            \n",
    "            total_bronze_rows += bronze_count\n",
    "            validation_results.append({\n",
    "                \"table\": table_name,\n",
    "                \"bronze_rows\": bronze_count,\n",
    "                \"table_rows\": table_count,\n",
    "                \"table_exists\": table_exists,\n",
    "                \"total_columns\": column_count,\n",
    "                \"business_columns\": len(business_columns),\n",
    "                \"metadata_columns\": len(metadata_columns),\n",
    "                \"load_date\": sample_row._load_date if sample_row else \"Unknown\",\n",
    "                \"source_system\": sample_row._source_system if sample_row else \"Unknown\",\n",
    "                \"load_method\": sample_row._load_method if sample_row else \"Unknown\",\n",
    "                \"lakehouse_table\": lakehouse_table_name,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "            \n",
    "            print(f\"   âœ… Files: {bronze_count:,} rows validated\")\n",
    "            if table_exists:\n",
    "                print(f\"   ğŸ¢ Table: {table_count:,} rows validated\")\n",
    "                row_match = \"âœ…\" if bronze_count == table_count else \"âš ï¸\"\n",
    "                print(f\"   {row_match} Row count match: Files={bronze_count:,}, Table={table_count:,}\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ Lakehouse table not found: {lakehouse_table_name}\")\n",
    "            print(f\"   ğŸ“Š Columns: {len(business_columns)} business + {len(metadata_columns)} metadata\")\n",
    "            print(f\"   ğŸ“… Load date: {sample_row._load_date if sample_row else 'Unknown'}\")\n",
    "            print(f\"   ğŸ·ï¸ Source: {sample_row._source_system if sample_row else 'Unknown'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:80]\n",
    "            validation_results.append({\n",
    "                \"table\": table_name,\n",
    "                \"bronze_rows\": 0,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg\n",
    "            })\n",
    "            print(f\"   âŒ Validation failed: {error_msg}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Validation summary\n",
    "    successful_validations = [r for r in validation_results if r[\"status\"] == \"success\"]\n",
    "    failed_validations = [r for r in validation_results if r[\"status\"] == \"failed\"]\n",
    "    tables_available = [r for r in successful_validations if r.get(\"table_exists\", False)]\n",
    "    files_only = [r for r in successful_validations if not r.get(\"table_exists\", False)]\n",
    "    \n",
    "    print(\"ğŸ¯ VALIDATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ… Successfully validated: {len(successful_validations)} files\")\n",
    "    print(f\"âŒ Failed validations: {len(failed_validations)} files\")\n",
    "    print(f\"ğŸ“Š Total bronze rows: {total_bronze_rows:,}\")\n",
    "    print(f\"ğŸ“ Files available: {len(successful_validations)}\")\n",
    "    print(f\"ğŸ¢ Lakehouse tables available: {len(tables_available)}\")\n",
    "    print(f\"âš ï¸ Files only (no table): {len(files_only)}\")\n",
    "    print(f\"ğŸ·ï¸ Metadata enrichment: Load tracking added\")\n",
    "    \n",
    "    if successful_validations:\n",
    "        print(f\"\\nğŸ“‹ Bronze layer inventory:\")\n",
    "        for result in successful_validations:\n",
    "            table_marker = \"ğŸ¢+ğŸ“\" if result.get(\"table_exists\") else \"ğŸ“\"\n",
    "            table_info = f\" | Table: {result.get('table_rows', 0):,}\" if result.get(\"table_exists\") else \"\"\n",
    "            print(f\"  â€¢ {result['table']}: {result['bronze_rows']:,} rows | {result['total_columns']} columns | {result['load_date']} {table_marker}{table_info}\")\n",
    "    \n",
    "    if tables_available:\n",
    "        print(f\"\\nğŸ¢ Available lakehouse tables:\")\n",
    "        for result in tables_available:\n",
    "            print(f\"  âœ… {result['lakehouse_table']}: {result.get('table_rows', 0):,} rows\")\n",
    "    \n",
    "    if files_only:\n",
    "        print(f\"\\nğŸ“ Files only (tables not created):\")\n",
    "        for result in files_only:\n",
    "            print(f\"  âš ï¸ {result['table']}: File available, no lakehouse table\")\n",
    "    \n",
    "    if failed_validations:\n",
    "        print(f\"\\nâš ï¸ Validation failures:\")\n",
    "        for result in failed_validations:\n",
    "            print(f\"  âŒ {result['table']}: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Bronze layer validation complete!\")\n",
    "    print(f\"ğŸ“ Files location: {BRONZE_TARGET_PATH}\")\n",
    "    if tables_available:\n",
    "        print(f\"ğŸ¢ Tables accessible via: SELECT * FROM bronze_[tablename]\")\n",
    "    print(f\"ğŸš€ Ready for silver layer processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f8e6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides a **simplified, streamlined approach** for copying SalesLT tables to bronze layer in Microsoft Fabric with **dual storage**:\n",
    "\n",
    "### âœ… **Key Features**:\n",
    "- **Pure SQL approach** - No dbutils dependencies\n",
    "- **Dual storage strategy** - Both Files and managed Tables\n",
    "- **Metadata enrichment** - Adds bronze layer tracking columns\n",
    "- **Error handling** - Graceful failure handling and reporting\n",
    "- **Comprehensive validation** - Files and Tables validation\n",
    "- **Streamlined workflow** - Essential steps only for team adoption\n",
    "\n",
    "### ğŸ¯ **Output**:\n",
    "- **Files**: Bronze layer data in `Files/SalesLT/` directory structure\n",
    "- **Tables**: Managed lakehouse tables with `bronze_` prefix\n",
    "- Each table saved as parquet files with load metadata\n",
    "- Queryable tables for SQL analytics\n",
    "- Validation reports for data quality assurance\n",
    "- Ready for immediate downstream processing\n",
    "\n",
    "### ğŸ’¡ **Usage Patterns**:\n",
    "- **File-based processing**: Use `spark.read.parquet(\"Files/SalesLT/tablename\")`\n",
    "- **SQL analytics**: Use `SELECT * FROM bronze_tablename`\n",
    "- **Cross-lakehouse queries**: Join with other lakehouse data\n",
    "- **Data quality checks**: Built-in validation and metadata tracking\n",
    "\n",
    "### ğŸš€ **Next Steps**:\n",
    "- Silver layer transformations\n",
    "- Data quality rules implementation\n",
    "- Incremental load patterns\n",
    "- Gold layer aggregations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
