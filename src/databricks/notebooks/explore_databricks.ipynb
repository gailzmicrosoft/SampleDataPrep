{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306a04be",
   "metadata": {},
   "source": [
    "# Azure Databricks Unity Catalog Exploration\n",
    "\n",
    "This notebook demonstrates how to explore data using Azure Databricks with Unity Catalog enabled. We'll cover:\n",
    "- Unity Catalog basics\n",
    "- Data discovery and exploration\n",
    "- Sample data analysis\n",
    "- Best practices for data governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa21c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30009746",
   "metadata": {},
   "source": [
    "## 1. Unity Catalog Overview\n",
    "\n",
    "Unity Catalog provides centralized governance for data and AI assets across Azure Databricks workspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b45add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current catalog and schema\n",
    "print(\"Current catalog:\", spark.catalog.currentCatalog())\n",
    "print(\"Current database/schema:\", spark.catalog.currentDatabase())\n",
    "\n",
    "# List available catalogs\n",
    "print(\"\\nAvailable catalogs:\")\n",
    "catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "for catalog in catalogs:\n",
    "    print(f\"- {catalog.catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ba4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List schemas in current catalog\n",
    "print(\"Available schemas in current catalog:\")\n",
    "schemas = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "for schema in schemas:\n",
    "    print(f\"- {schema.databaseName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845e0a7",
   "metadata": {},
   "source": [
    "## 2. Data Discovery\n",
    "\n",
    "Let's explore what data is available in our Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80198dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore tables in a schema\n",
    "def explore_schema(catalog_name, schema_name):\n",
    "    \"\"\"Explore tables and views in a given schema\"\"\"\n",
    "    try:\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name}\").collect()\n",
    "        print(f\"\\nTables in {catalog_name}.{schema_name}:\")\n",
    "        for table in tables:\n",
    "            print(f\"- {table.tableName} ({table.tableType})\")\n",
    "        return [table.tableName for table in tables]\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring schema {catalog_name}.{schema_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Explore main catalog schemas\n",
    "current_catalog = spark.catalog.currentCatalog()\n",
    "for schema in schemas[:3]:  # Limit to first 3 schemas\n",
    "    schema_name = schema.databaseName\n",
    "    explore_schema(current_catalog, schema_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df85d8",
   "metadata": {},
   "source": [
    "## 3. Sample Data Creation\n",
    "\n",
    "Let's create some sample data for exploration if no existing data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data\n",
    "sample_data = [\n",
    "    (\"2024-01-01\", \"Product_A\", \"Electronics\", 1200.50, 2, \"North\"),\n",
    "    (\"2024-01-02\", \"Product_B\", \"Clothing\", 450.75, 3, \"South\"),\n",
    "    (\"2024-01-03\", \"Product_C\", \"Electronics\", 890.00, 1, \"East\"),\n",
    "    (\"2024-01-04\", \"Product_A\", \"Electronics\", 1200.50, 1, \"West\"),\n",
    "    (\"2024-01-05\", \"Product_D\", \"Home\", 320.25, 4, \"North\"),\n",
    "    (\"2024-01-06\", \"Product_B\", \"Clothing\", 450.75, 2, \"South\"),\n",
    "    (\"2024-01-07\", \"Product_E\", \"Sports\", 780.90, 1, \"East\"),\n",
    "    (\"2024-01-08\", \"Product_C\", \"Electronics\", 890.00, 3, \"West\"),\n",
    "    (\"2024-01-09\", \"Product_F\", \"Books\", 45.99, 10, \"North\"),\n",
    "    (\"2024-01-10\", \"Product_A\", \"Electronics\", 1200.50, 1, \"Central\")\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_sales = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "# Convert date string to date type\n",
    "df_sales = df_sales.withColumn(\"sale_date\", to_date(\"sale_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "print(\"Sample sales data created successfully!\")\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4190c",
   "metadata": {},
   "source": [
    "## 4. Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data info\n",
    "print(\"Dataset Shape:\", (df_sales.count(), len(df_sales.columns)))\n",
    "print(\"\\nSchema:\")\n",
    "df_sales.printSchema()\n",
    "\n",
    "print(\"\\nSample records:\")\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1412394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "df_sales.describe().show()\n",
    "\n",
    "# Check for null values\n",
    "print(\"\\nNull value counts:\")\n",
    "for col in df_sales.columns:\n",
    "    null_count = df_sales.filter(df_sales[col].isNull()).count()\n",
    "    print(f\"{col}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ceb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values in categorical columns\n",
    "categorical_cols = [\"product_name\", \"category\", \"region\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unique_count = df_sales.select(col).distinct().count()\n",
    "    print(f\"\\nUnique values in {col}: {unique_count}\")\n",
    "    df_sales.select(col).distinct().orderBy(col).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30700b",
   "metadata": {},
   "source": [
    "## 5. Data Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b7a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total revenue\n",
    "df_sales_with_revenue = df_sales.withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
    "\n",
    "# Revenue by category\n",
    "revenue_by_category = df_sales_with_revenue.groupBy(\"category\").agg(\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    avg(\"revenue\").alias(\"avg_revenue\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(\"Revenue by Category:\")\n",
    "revenue_by_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c58c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by region\n",
    "revenue_by_region = df_sales_with_revenue.groupBy(\"region\").agg(\n",
    "    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    count(\"*\").alias(\"transaction_count\")\n",
    ").orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(\"Revenue by Region:\")\n",
    "revenue_by_region.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for visualization\n",
    "df_pandas = df_sales_with_revenue.toPandas()\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Revenue by Category\n",
    "category_revenue = df_pandas.groupby('category')['revenue'].sum().sort_values(ascending=False)\n",
    "axes[0,0].bar(category_revenue.index, category_revenue.values)\n",
    "axes[0,0].set_title('Total Revenue by Category')\n",
    "axes[0,0].set_xlabel('Category')\n",
    "axes[0,0].set_ylabel('Revenue')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Revenue by Region\n",
    "region_revenue = df_pandas.groupby('region')['revenue'].sum().sort_values(ascending=False)\n",
    "axes[0,1].bar(region_revenue.index, region_revenue.values, color='orange')\n",
    "axes[0,1].set_title('Total Revenue by Region')\n",
    "axes[0,1].set_xlabel('Region')\n",
    "axes[0,1].set_ylabel('Revenue')\n",
    "\n",
    "# Price distribution\n",
    "axes[1,0].hist(df_pandas['price'], bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[1,0].set_title('Price Distribution')\n",
    "axes[1,0].set_xlabel('Price')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Revenue over time\n",
    "df_pandas['sale_date'] = pd.to_datetime(df_pandas['sale_date'])\n",
    "daily_revenue = df_pandas.groupby('sale_date')['revenue'].sum()\n",
    "axes[1,1].plot(daily_revenue.index, daily_revenue.values, marker='o')\n",
    "axes[1,1].set_title('Daily Revenue Trend')\n",
    "axes[1,1].set_xlabel('Date')\n",
    "axes[1,1].set_ylabel('Revenue')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb968e60",
   "metadata": {},
   "source": [
    "## 6. Working with Unity Catalog Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eaa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to Unity Catalog (if you have write permissions)\n",
    "# This creates a managed table in Unity Catalog\n",
    "table_name = \"sample_sales_data\"\n",
    "\n",
    "try:\n",
    "    # Write to Unity Catalog\n",
    "    df_sales_with_revenue.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"default.{table_name}\")\n",
    "    \n",
    "    print(f\"Table {table_name} created successfully in Unity Catalog!\")\n",
    "    \n",
    "    # Verify table creation\n",
    "    spark.sql(f\"DESCRIBE TABLE default.{table_name}\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not create table in Unity Catalog. This might be due to permissions.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Creating temporary view instead...\")\n",
    "    df_sales_with_revenue.createOrReplaceTempView(table_name)\n",
    "    print(f\"Temporary view {table_name} created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94622ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data using SQL\n",
    "sql_query = f\"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    region,\n",
    "    COUNT(*) as transaction_count,\n",
    "    SUM(revenue) as total_revenue,\n",
    "    AVG(revenue) as avg_revenue,\n",
    "    MAX(revenue) as max_revenue\n",
    "FROM {table_name}\n",
    "GROUP BY category, region\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "print(\"SQL Query Results:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409278d8",
   "metadata": {},
   "source": [
    "## 7. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "def data_quality_report(df, table_name):\n",
    "    \"\"\"Generate a data quality report\"\"\"\n",
    "    print(f\"=== Data Quality Report for {table_name} ===\")\n",
    "    \n",
    "    # Row count\n",
    "    row_count = df.count()\n",
    "    print(f\"Total rows: {row_count}\")\n",
    "    \n",
    "    # Column count\n",
    "    col_count = len(df.columns)\n",
    "    print(f\"Total columns: {col_count}\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    distinct_count = df.distinct().count()\n",
    "    duplicate_count = row_count - distinct_count\n",
    "    print(f\"Duplicate rows: {duplicate_count}\")\n",
    "    \n",
    "    # Null checks per column\n",
    "    print(\"\\nNull value analysis:\")\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(df[col_name].isNull()).count()\n",
    "        null_percentage = (null_count / row_count) * 100\n",
    "        print(f\"  {col_name}: {null_count} nulls ({null_percentage:.2f}%)\")\n",
    "    \n",
    "    # Data type validation\n",
    "    print(f\"\\nData types:\")\n",
    "    for field in df.schema.fields:\n",
    "        print(f\"  {field.name}: {field.dataType}\")\n",
    "\n",
    "# Run data quality report\n",
    "data_quality_report(df_sales_with_revenue, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea1a2b",
   "metadata": {},
   "source": [
    "## 8. Next Steps and Best Practices\n",
    "\n",
    "### Unity Catalog Best Practices:\n",
    "1. **Data Governance**: Use proper naming conventions for catalogs, schemas, and tables\n",
    "2. **Access Control**: Implement fine-grained permissions using Unity Catalog\n",
    "3. **Data Lineage**: Unity Catalog automatically tracks data lineage\n",
    "4. **Data Discovery**: Use Unity Catalog's search and discovery features\n",
    "5. **Delta Lake Integration**: Use Delta Lake for ACID transactions and time travel\n",
    "\n",
    "### Recommended Actions:\n",
    "- Set up proper catalog structure for your organization\n",
    "- Implement data quality monitoring\n",
    "- Create documentation for your datasets\n",
    "- Set up automated data pipelines\n",
    "- Use Delta Live Tables for production workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (optional)\n",
    "print(\"Exploration completed successfully!\")\n",
    "print(\"You can now:\")\n",
    "print(\"1. Create your own datasets in Unity Catalog\")\n",
    "print(\"2. Set up data pipelines using Delta Live Tables\")\n",
    "print(\"3. Implement proper governance and access controls\")\n",
    "print(\"4. Explore advanced analytics and ML features\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
