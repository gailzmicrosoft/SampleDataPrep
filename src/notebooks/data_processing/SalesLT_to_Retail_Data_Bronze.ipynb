{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4864afcf",
   "metadata": {},
   "source": [
    "# SalesLT to Retail Data Bronze Layer\n",
    "\n",
    "**Objective**: Copy all SalesLT tables to bronze layer in Retail Data lakehouse using simplified Fabric PySpark approach\n",
    "\n",
    "**Prerequisites**:\n",
    "- Microsoft Fabric environment with PySpark runtime\n",
    "- Access to SalesLT tables (via shortcuts or direct tables)\n",
    "- Write permissions to target bronze lakehouse\n",
    "- Source lakehouse shortcut configured in bronze lakehouse\n",
    "\n",
    "**Setup Strategy**:\n",
    "1. **Current Context**: Running in bronze lakehouse (`RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze`)\n",
    "2. **Source Access**: Access SalesLT tables from `Gaiye_Test_Lakehouse` via shortcuts\n",
    "3. **Data Flow**: Shortcut tables ‚Üí Bronze Files + Tables with metadata enrichment\n",
    "\n",
    "**Expected Tables**: address, customer, customeraddress, product, productcategory, productdescription, productmodel, productmodelproductdescription, salesorderdetail, salesorderheader\n",
    "\n",
    "**Workflow Options**:\n",
    "- **First Run**: Execute all steps (1-5) for initial setup and validation\n",
    "- **Subsequent Runs**: Execute steps 1, 4, 5 only (Setup ‚Üí Process ‚Üí Validate) for regular data refreshes\n",
    "- **Quick Refresh**: Steps 2-3 can be skipped once environment is validated and working\n",
    "- **Optimized Refresh**: Execute steps 1, 4 only (Setup ‚Üí Process) for fastest data updates once pipeline is proven reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdef79f",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ae949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Configuration\n",
    "BRONZE_TARGET_PATH = \"Files/SalesLT/\"\n",
    "SOURCE_SYSTEM = \"SalesLT\"\n",
    "SOURCE_DATABASE = \"Gaiye_Test_Lakehouse\"\n",
    "LOAD_TIMESTAMP = datetime.now().isoformat()\n",
    "LOAD_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Expected SalesLT tables\n",
    "EXPECTED_TABLES = [\n",
    "    'address', 'customer', 'customeraddress', 'product', \n",
    "    'productcategory', 'productdescription', 'productmodel',\n",
    "    'productmodelproductdescription', 'salesorderdetail', 'salesorderheader'\n",
    "]\n",
    "\n",
    "print(\"üöÄ SalesLT to Retail Data Bronze Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úÖ Libraries imported\")\n",
    "print(f\"üìÖ Load timestamp: {LOAD_TIMESTAMP}\")\n",
    "print(f\"üéØ Target path: {BRONZE_TARGET_PATH}\")\n",
    "print(f\"üì• Source database: {SOURCE_DATABASE}\")\n",
    "print(f\"üìä Expected tables: {len(EXPECTED_TABLES)}\")\n",
    "print(f\"‚úÖ Microsoft Fabric PySpark environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b215c",
   "metadata": {},
   "source": [
    "## Step 2: Discover Available Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover source tables from Gaiye_Test_Lakehouse\n",
    "print(\"üîç DISCOVERING SOURCE TABLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check source lakehouse context\n",
    "print(f\"üè† Current context: Bronze lakehouse (target)\")\n",
    "print(f\"üì• Source database: {SOURCE_DATABASE}\")\n",
    "print(f\"üì§ Target path: {BRONZE_TARGET_PATH}\")\n",
    "print(f\"üìã Expected table names: {', '.join(EXPECTED_TABLES)}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Get tables from source database specifically\n",
    "    print(f\"üîç Querying tables from {SOURCE_DATABASE}...\")\n",
    "    source_tables_df = spark.sql(f\"SHOW TABLES IN {SOURCE_DATABASE}\").toPandas()\n",
    "    \n",
    "    print(f\"‚úÖ Total tables found in source: {len(source_tables_df)}\")\n",
    "    print(f\"‚úÖ Spark SQL connection confirmed\")\n",
    "    \n",
    "    if len(source_tables_df) > 0:\n",
    "        # Handle flexible column naming\n",
    "        table_column = None\n",
    "        for possible_col in ['tableName', 'table_name', 'name']:\n",
    "            if possible_col in source_tables_df.columns:\n",
    "                table_column = possible_col\n",
    "                break\n",
    "        \n",
    "        if table_column is None:\n",
    "            table_column = source_tables_df.columns[0]\n",
    "            print(f\"üîç Using column '{table_column}' as table name\")\n",
    "        \n",
    "        print(f\"\\nüìã All available tables in {SOURCE_DATABASE}:\")\n",
    "        for _, row in source_tables_df.iterrows():\n",
    "            table_name = row[table_column]\n",
    "            # Check if this matches our expected tables\n",
    "            is_expected = any(expected.lower() == table_name.lower() for expected in EXPECTED_TABLES)\n",
    "            marker = \"üéØ\" if is_expected else \"üìã\"\n",
    "            print(f\"   {marker} {table_name}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è No tables found in {SOURCE_DATABASE}!\")\n",
    "        print(\"üí° Verify the source lakehouse contains data\")\n",
    "    \n",
    "    # Find matching tables from our expected list\n",
    "    available_tables = []\n",
    "    missing_tables = []\n",
    "    table_mapping = {}  # Map expected names to actual names\n",
    "    \n",
    "    if len(source_tables_df) > 0:\n",
    "        # Get actual table names (case-sensitive)\n",
    "        actual_table_names = source_tables_df[table_column].tolist()\n",
    "        actual_table_names_lower = [name.lower() for name in actual_table_names]\n",
    "        \n",
    "        print(f\"\\nüîç Matching expected tables with available tables:\")\n",
    "        for expected_table in EXPECTED_TABLES:\n",
    "            # Find case-insensitive match\n",
    "            matching_indices = [i for i, name in enumerate(actual_table_names_lower) \n",
    "                              if name == expected_table.lower()]\n",
    "            \n",
    "            if matching_indices:\n",
    "                # Use the actual table name (with correct case)\n",
    "                actual_table_name = actual_table_names[matching_indices[0]]\n",
    "                available_tables.append(actual_table_name)\n",
    "                table_mapping[expected_table] = actual_table_name\n",
    "                print(f\"   ‚úÖ Found: {expected_table} ‚Üí {actual_table_name}\")\n",
    "            else:\n",
    "                missing_tables.append(expected_table)\n",
    "                print(f\"   ‚ùå Missing: {expected_table}\")\n",
    "    \n",
    "    print(f\"\\nüìä DISCOVERY SUMMARY\")\n",
    "    print(f\"‚úÖ Available tables found: {len(available_tables)}\")\n",
    "    print(f\"‚ùå Missing tables: {len(missing_tables)}\")\n",
    "    \n",
    "    if len(available_tables) == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è No expected tables found in {SOURCE_DATABASE}!\")\n",
    "        print(\"üí° Check that the source lakehouse contains the SalesLT tables\")\n",
    "        print(\"üí° Verify table names match expected format\")\n",
    "        print()\n",
    "        print(\"üîß TROUBLESHOOTING:\")\n",
    "        print(\"1. Ensure you have a shortcut to the source lakehouse\")\n",
    "        print(\"2. Refresh the lakehouse view in Fabric\")\n",
    "        print(\"3. Check the source lakehouse contains data\")\n",
    "        \n",
    "        # Show what was actually found for debugging\n",
    "        if len(source_tables_df) > 0:\n",
    "            print(f\"\\nüîç Debug - Available table names in source:\")\n",
    "            for _, row in source_tables_df.iterrows():\n",
    "                print(f\"   üìã '{row[table_column]}'\")\n",
    "    else:\n",
    "        print(f\"\\nüéâ Ready to process {len(available_tables)} tables!\")\n",
    "        \n",
    "        # Store for next steps (use actual table names with correct casing)\n",
    "        TABLES_TO_PROCESS = available_tables\n",
    "        TABLE_MAPPING = table_mapping\n",
    "        print(f\"üìù Tables to process: {', '.join(TABLES_TO_PROCESS)}\")\n",
    "        print(f\"üöÄ Source: {SOURCE_DATABASE} ‚Üí Target: {BRONZE_TARGET_PATH}\")\n",
    "        \n",
    "        if missing_tables:\n",
    "            print(f\"\\n‚ö†Ô∏è Missing tables (will be skipped): {', '.join(missing_tables)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to discover tables: {str(e)}\")\n",
    "    print()\n",
    "    print(\"üîß TROUBLESHOOTING:\")\n",
    "    print(f\"1. Ensure {SOURCE_DATABASE} is accessible from this lakehouse\")\n",
    "    print(\"2. Check lakehouse attachment/shortcut configuration\")\n",
    "    print(\"3. Refresh the lakehouse view in Fabric\")\n",
    "    print(\"4. Verify source lakehouse permissions\")\n",
    "    TABLES_TO_PROCESS = []\n",
    "    TABLE_MAPPING = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a49d7",
   "metadata": {},
   "source": [
    "## Step 3: Test Bronze Layer Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae119fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test write access to bronze layer\n",
    "print(\"üß™ TESTING BRONZE LAYER ACCESS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_path = f\"{BRONZE_TARGET_PATH}_test_access\"\n",
    "\n",
    "try:\n",
    "    # Create test data\n",
    "    test_data = [(\"access_test\", LOAD_TIMESTAMP, \"success\")]\n",
    "    test_df = spark.createDataFrame(test_data, [\"test_type\", \"timestamp\", \"status\"])\n",
    "    \n",
    "    # Test write to bronze location\n",
    "    print(f\"üìù Testing write to: {test_path}\")\n",
    "    test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "    \n",
    "    # Verify read access\n",
    "    verify_df = spark.read.parquet(test_path)\n",
    "    test_count = verify_df.count()\n",
    "    \n",
    "    print(f\"‚úÖ Write access confirmed\")\n",
    "    print(f\"‚úÖ Read access confirmed ({test_count} test records)\")\n",
    "    print(f\"üéØ Target path ready: {BRONZE_TARGET_PATH}\")\n",
    "    \n",
    "    # Display test data to confirm\n",
    "    print(\"\\nüìã Test data sample:\")\n",
    "    verify_df.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Bronze layer access test failed: {str(e)}\")\n",
    "    print(\"üí° Ensure you have write permissions to the current lakehouse\")\n",
    "    print(\"üí° Check Files directory structure and permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09179308",
   "metadata": {},
   "source": [
    "## Step 4: Process SalesLT Tables to Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy SalesLT tables to bronze layer with metadata enrichment\n",
    "print(\"üöÄ PROCESSING SALESLT TABLES TO BRONZE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n",
    "    print(\"‚ùå No tables to process. Run previous steps first.\")\n",
    "else:\n",
    "    print(f\"üìã Processing {len(TABLES_TO_PROCESS)} tables\")\n",
    "    print(f\"üì• Source: {SOURCE_DATABASE}\")\n",
    "    print(f\"üì§ Target: {BRONZE_TARGET_PATH}\")\n",
    "    print(f\"üìÖ Load date: {LOAD_DATE}\")\n",
    "    print()\n",
    "    \n",
    "    # Processing results tracking\n",
    "    results = []\n",
    "    total_rows_processed = 0\n",
    "    \n",
    "    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n",
    "        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Processing {table_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read source table using qualified name\n",
    "            print(f\"   üìñ Reading from {SOURCE_DATABASE}.{table_name}...\")\n",
    "            source_df = spark.sql(f\"SELECT * FROM {SOURCE_DATABASE}.{table_name}\")\n",
    "            row_count = source_df.count()\n",
    "            \n",
    "            print(f\"   ‚úÖ Source data loaded: {row_count:,} rows\")\n",
    "            \n",
    "            # Add bronze layer metadata columns\n",
    "            print(f\"   üè∑Ô∏è Adding metadata columns...\")\n",
    "            bronze_df = source_df \\\n",
    "                .withColumn(\"_load_date\", lit(LOAD_DATE)) \\\n",
    "                .withColumn(\"_load_timestamp\", lit(LOAD_TIMESTAMP)) \\\n",
    "                .withColumn(\"_source_system\", lit(SOURCE_SYSTEM)) \\\n",
    "                .withColumn(\"_source_table\", lit(table_name)) \\\n",
    "                .withColumn(\"_processing_timestamp\", current_timestamp()) \\\n",
    "                .withColumn(\"_record_source\", lit(\"cross_lakehouse_copy\")) \\\n",
    "                .withColumn(\"_load_method\", lit(\"spark_sql_full_extract\")) \\\n",
    "                .withColumn(\"_source_database\", lit(SOURCE_DATABASE)) \\\n",
    "                .withColumn(\"_target_path\", lit(f\"{BRONZE_TARGET_PATH}{table_name}\"))\n",
    "            \n",
    "            # Write to bronze layer as files\n",
    "            table_target_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n",
    "            print(f\"   üíæ Writing to Files: {table_target_path}\")\n",
    "            \n",
    "            bronze_df.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .parquet(table_target_path)\n",
    "            \n",
    "            print(f\"   ‚úÖ Files saved: {row_count:,} rows\")\n",
    "            \n",
    "            # Also create lakehouse table (data in memory, efficient to do both)\n",
    "            lakehouse_table_name = f\"bronze_{table_name}\"\n",
    "            print(f\"   üè¢ Creating lakehouse table: {lakehouse_table_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Create/replace lakehouse table\n",
    "                bronze_df.write \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"overwriteSchema\", \"true\") \\\n",
    "                    .saveAsTable(lakehouse_table_name)\n",
    "                \n",
    "                print(f\"   ‚úÖ Table created: {lakehouse_table_name}\")\n",
    "                table_creation_status = \"success\"\n",
    "                \n",
    "            except Exception as table_error:\n",
    "                table_error_msg = str(table_error)[:60]\n",
    "                print(f\"   ‚ö†Ô∏è Table creation failed: {table_error_msg}...\")\n",
    "                table_creation_status = \"file_only\"\n",
    "            \n",
    "            # Success tracking\n",
    "            total_rows_processed += row_count\n",
    "            results.append({\n",
    "                \"table\": table_name,\n",
    "                \"rows\": row_count,\n",
    "                \"status\": \"success\",\n",
    "                \"target_path\": table_target_path,\n",
    "                \"lakehouse_table\": lakehouse_table_name,\n",
    "                \"table_status\": table_creation_status\n",
    "            })\n",
    "            \n",
    "            print(f\"   üéâ Successfully processed {row_count:,} rows (Files + Table)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:100]\n",
    "            results.append({\n",
    "                \"table\": table_name,\n",
    "                \"rows\": 0,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg\n",
    "            })\n",
    "            print(f\"   ‚ùå Failed: {error_msg}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Processing summary\n",
    "    successful = [r for r in results if r[\"status\"] == \"success\"]\n",
    "    failed = [r for r in results if r[\"status\"] == \"failed\"]\n",
    "    tables_created = [r for r in successful if r.get(\"table_status\") == \"success\"]\n",
    "    files_only = [r for r in successful if r.get(\"table_status\") == \"file_only\"]\n",
    "    \n",
    "    print(\"üéâ PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Successfully processed: {len(successful)} tables\")\n",
    "    print(f\"‚ùå Failed processing: {len(failed)} tables\")\n",
    "    print(f\"üìä Total rows processed: {total_rows_processed:,}\")\n",
    "    print(f\"üìÅ Files created: {len(successful)} (all)\")\n",
    "    print(f\"üè¢ Lakehouse tables created: {len(tables_created)}\")\n",
    "    print(f\"‚ö†Ô∏è Files only (table creation failed): {len(files_only)}\")\n",
    "    print(f\"üìÖ Processing completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if successful:\n",
    "        print(f\"\\nüìÅ Bronze layer file structure:\")\n",
    "        print(f\"{BRONZE_TARGET_PATH}\")\n",
    "        for result in successful:\n",
    "            table_marker = \"üè¢\" if result.get(\"table_status\") == \"success\" else \"üìÅ\"\n",
    "            print(f\"‚îú‚îÄ‚îÄ {result['table']}/ ({result['rows']:,} rows) {table_marker}\")\n",
    "    \n",
    "    if tables_created:\n",
    "        print(f\"\\nüè¢ Lakehouse tables created:\")\n",
    "        for result in tables_created:\n",
    "            print(f\"‚úÖ {result['lakehouse_table']} ({result['rows']:,} rows)\")\n",
    "    \n",
    "    if files_only:\n",
    "        print(f\"\\nüìÅ Files only (table creation issues):\")\n",
    "        for result in files_only:\n",
    "            print(f\"‚ö†Ô∏è {result['table']} ‚Üí Files saved, table creation failed\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n‚ö†Ô∏è Processing failures:\")\n",
    "        for result in failed:\n",
    "            print(f\"‚ùå {result['table']}: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Bronze data ready for downstream processing!\")\n",
    "    print(f\"üìÅ Files stored in: {BRONZE_TARGET_PATH}\")\n",
    "    if tables_created:\n",
    "        print(f\"üè¢ Tables available in: {len(tables_created)} lakehouse tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b1ba4",
   "metadata": {},
   "source": [
    "## Step 5: Validate Bronze Layer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate bronze layer data quality and completeness\n",
    "print(\"üîç VALIDATING BRONZE LAYER DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n",
    "    print(\"‚ùå No tables to validate. Run previous steps first.\")\n",
    "else:\n",
    "    validation_results = []\n",
    "    total_bronze_rows = 0\n",
    "    \n",
    "    print(f\"üìã Validating {len(TABLES_TO_PROCESS)} bronze files\")\n",
    "    print(f\"üìç Target location: {BRONZE_TARGET_PATH}\")\n",
    "    print()\n",
    "    \n",
    "    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n",
    "        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Validating {table_name}...\")\n",
    "        \n",
    "        try:\n",
    "            file_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n",
    "            lakehouse_table_name = f\"bronze_{table_name}\"\n",
    "            \n",
    "            # Read bronze data files\n",
    "            bronze_df = spark.read.parquet(file_path)\n",
    "            bronze_count = bronze_df.count()\n",
    "            \n",
    "            # Check if lakehouse table exists and validate\n",
    "            table_count = 0\n",
    "            table_exists = False\n",
    "            try:\n",
    "                table_df = spark.table(lakehouse_table_name)\n",
    "                table_count = table_df.count()\n",
    "                table_exists = True\n",
    "            except Exception:\n",
    "                table_exists = False\n",
    "            \n",
    "            # Check metadata columns\n",
    "            sample_row = bronze_df.select(\n",
    "                \"_load_date\", \n",
    "                \"_source_system\", \n",
    "                \"_source_table\",\n",
    "                \"_load_method\"\n",
    "            ).first()\n",
    "            \n",
    "            # Get column count for schema validation\n",
    "            column_count = len(bronze_df.columns)\n",
    "            metadata_columns = [col for col in bronze_df.columns if col.startswith('_')]\n",
    "            business_columns = [col for col in bronze_df.columns if not col.startswith('_')]\n",
    "            \n",
    "            total_bronze_rows += bronze_count\n",
    "            validation_results.append({\n",
    "                \"table\": table_name,\n",
    "                \"bronze_rows\": bronze_count,\n",
    "                \"table_rows\": table_count,\n",
    "                \"table_exists\": table_exists,\n",
    "                \"total_columns\": column_count,\n",
    "                \"business_columns\": len(business_columns),\n",
    "                \"metadata_columns\": len(metadata_columns),\n",
    "                \"load_date\": sample_row._load_date if sample_row else \"Unknown\",\n",
    "                \"source_system\": sample_row._source_system if sample_row else \"Unknown\",\n",
    "                \"load_method\": sample_row._load_method if sample_row else \"Unknown\",\n",
    "                \"lakehouse_table\": lakehouse_table_name,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚úÖ Files: {bronze_count:,} rows validated\")\n",
    "            if table_exists:\n",
    "                print(f\"   üè¢ Table: {table_count:,} rows validated\")\n",
    "                row_match = \"‚úÖ\" if bronze_count == table_count else \"‚ö†Ô∏è\"\n",
    "                print(f\"   {row_match} Row count match: Files={bronze_count:,}, Table={table_count:,}\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Lakehouse table not found: {lakehouse_table_name}\")\n",
    "            print(f\"   üìä Columns: {len(business_columns)} business + {len(metadata_columns)} metadata\")\n",
    "            print(f\"   üìÖ Load date: {sample_row._load_date if sample_row else 'Unknown'}\")\n",
    "            print(f\"   üè∑Ô∏è Source: {sample_row._source_system if sample_row else 'Unknown'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:80]\n",
    "            validation_results.append({\n",
    "                \"table\": table_name,\n",
    "                \"bronze_rows\": 0,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": error_msg\n",
    "            })\n",
    "            print(f\"   ‚ùå Validation failed: {error_msg}...\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Validation summary\n",
    "    successful_validations = [r for r in validation_results if r[\"status\"] == \"success\"]\n",
    "    failed_validations = [r for r in validation_results if r[\"status\"] == \"failed\"]\n",
    "    tables_available = [r for r in successful_validations if r.get(\"table_exists\", False)]\n",
    "    files_only = [r for r in successful_validations if not r.get(\"table_exists\", False)]\n",
    "    \n",
    "    print(\"üéØ VALIDATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Successfully validated: {len(successful_validations)} files\")\n",
    "    print(f\"‚ùå Failed validations: {len(failed_validations)} files\")\n",
    "    print(f\"üìä Total bronze rows: {total_bronze_rows:,}\")\n",
    "    print(f\"üìÅ Files available: {len(successful_validations)}\")\n",
    "    print(f\"üè¢ Lakehouse tables available: {len(tables_available)}\")\n",
    "    print(f\"‚ö†Ô∏è Files only (no table): {len(files_only)}\")\n",
    "    print(f\"üè∑Ô∏è Metadata enrichment: Load tracking added\")\n",
    "    \n",
    "    if successful_validations:\n",
    "        print(f\"\\nüìã Bronze layer inventory:\")\n",
    "        for result in successful_validations:\n",
    "            table_marker = \"üè¢+üìÅ\" if result.get(\"table_exists\") else \"üìÅ\"\n",
    "            table_info = f\" | Table: {result.get('table_rows', 0):,}\" if result.get(\"table_exists\") else \"\"\n",
    "            print(f\"  ‚Ä¢ {result['table']}: {result['bronze_rows']:,} rows | {result['total_columns']} columns | {result['load_date']} {table_marker}{table_info}\")\n",
    "    \n",
    "    if tables_available:\n",
    "        print(f\"\\nüè¢ Available lakehouse tables:\")\n",
    "        for result in tables_available:\n",
    "            print(f\"  ‚úÖ {result['lakehouse_table']}: {result.get('table_rows', 0):,} rows\")\n",
    "    \n",
    "    if files_only:\n",
    "        print(f\"\\nüìÅ Files only (tables not created):\")\n",
    "        for result in files_only:\n",
    "            print(f\"  ‚ö†Ô∏è {result['table']}: File available, no lakehouse table\")\n",
    "    \n",
    "    if failed_validations:\n",
    "        print(f\"\\n‚ö†Ô∏è Validation failures:\")\n",
    "        for result in failed_validations:\n",
    "            print(f\"  ‚ùå {result['table']}: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Bronze layer validation complete!\")\n",
    "    print(f\"üìÅ Files location: {BRONZE_TARGET_PATH}\")\n",
    "    if tables_available:\n",
    "        print(f\"üè¢ Tables accessible via: SELECT * FROM bronze_[tablename]\")\n",
    "    print(f\"üöÄ Ready for silver layer processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f8e6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides a **simplified, streamlined approach** for copying SalesLT tables to bronze layer in Microsoft Fabric with **dual storage**:\n",
    "\n",
    "### ‚úÖ **Key Features**:\n",
    "- **Pure SQL approach** - No dbutils dependencies\n",
    "- **Dual storage strategy** - Both Files and managed Tables\n",
    "- **Metadata enrichment** - Adds bronze layer tracking columns\n",
    "- **Error handling** - Graceful failure handling and reporting\n",
    "- **Comprehensive validation** - Files and Tables validation\n",
    "- **Streamlined workflow** - Essential steps only for team adoption\n",
    "\n",
    "### üéØ **Output**:\n",
    "- **Files**: Bronze layer data in `Files/SalesLT/` directory structure\n",
    "- **Tables**: Managed lakehouse tables with `bronze_` prefix\n",
    "- Each table saved as parquet files with load metadata\n",
    "- Queryable tables for SQL analytics\n",
    "- Validation reports for data quality assurance\n",
    "- Ready for immediate downstream processing\n",
    "\n",
    "### üí° **Usage Patterns**:\n",
    "- **File-based processing**: Use `spark.read.parquet(\"Files/SalesLT/tablename\")`\n",
    "- **SQL analytics**: Use `SELECT * FROM bronze_tablename`\n",
    "- **Cross-lakehouse queries**: Join with other lakehouse data\n",
    "- **Data quality checks**: Built-in validation and metadata tracking\n",
    "\n",
    "### üöÄ **Next Steps**:\n",
    "- Silver layer transformations\n",
    "- Data quality rules implementation\n",
    "- Incremental load patterns\n",
    "- Gold layer aggregations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
