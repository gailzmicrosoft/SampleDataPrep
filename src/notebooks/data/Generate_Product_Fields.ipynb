{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d4f224",
   "metadata": {},
   "source": [
    "# Generate Product Fileds - Output Not Utilized \n",
    "\n",
    "## Overview\n",
    "This notebook generates sample data for the Product table with specific field distributions and business rules.\n",
    "\n",
    "## Output\n",
    "- File: `C:\\temp\\samples\\Product_Special_Fields_NotUsed.csv`\n",
    "- Contains selected Product table fields with realistic distributions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdffb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 GENERATING PRODUCT SAMPLE DATA\n",
      "Sample Size: 295\n",
      "Output: C:\\temp\\samples\\Product_Special_Fields.csv\n",
      "==================================================\n",
      "🔄 Generating field data...\n",
      "✅ Data generation complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# Set seed for reproducible results\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_SIZE = 295  # Number of product records to generate\n",
    "OUTPUT_FOLDER = \"C:\\\\temp\\\\samples\"\n",
    "OUTPUT_FILE = \"Product_Special_Fields_NotUsed.csv\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Remove existing output file if it exists\n",
    "output_path = os.path.join(OUTPUT_FOLDER, OUTPUT_FILE)\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    print(f\"🗑️ Removed existing file: {output_path}\")\n",
    "\n",
    "print(f\"🎯 GENERATING PRODUCT SAMPLE DATA\")\n",
    "print(f\"Sample Size: {SAMPLE_SIZE}\")\n",
    "print(f\"Output: {OUTPUT_FOLDER}\\\\{OUTPUT_FILE}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Placeholder functions - will be updated based on your field requirements\n",
    "def generate_product_id(num_records):\n",
    "    \"\"\"Generate unique product IDs\"\"\"\n",
    "    return [f\"P-{i+1:03d}\" for i in range(num_records)]\n",
    "\n",
    "def generate_product_name(num_records):\n",
    "    \"\"\"Generate sample product names\"\"\"\n",
    "    product_names = [f\"Product {i+1}\" for i in range(num_records)]\n",
    "    return product_names\n",
    "\n",
    "# 3. Generate BrandName (70% Fabrikam, 30% Alpine Ski House)\n",
    "def generate_brand_name(num_records):\n",
    "    \"\"\"Generate BrandName with specified distribution\"\"\"\n",
    "    brand_names = ['Fabrikam', 'Alpine Ski House']\n",
    "    probabilities = [0.7, 0.3]\n",
    "    return np.random.choice(brand_names, size=num_records, p=probabilities)\n",
    "\n",
    "# 4. Generate CreatedDate (Jan 1, 2020 to Dec 31, 2020)\n",
    "def generate_created_date(num_records):\n",
    "    \"\"\"Generate CreatedDate uniformly distributed in 2020\"\"\"\n",
    "    start_date = date(2020, 1, 1)\n",
    "    end_date = date(2020, 12, 31)\n",
    "    \n",
    "    # Calculate total days\n",
    "    total_days = (end_date - start_date).days\n",
    "    \n",
    "    # Generate random days offset\n",
    "    random_days = np.random.randint(0, total_days + 1, num_records)\n",
    "    \n",
    "    # Convert to dates\n",
    "    created_dates = [start_date + timedelta(days=int(days)) for days in random_days]\n",
    "    \n",
    "    return created_dates\n",
    "\n",
    "# 5. Generate SellStartDate (Jan 1, 2020 to Sept 30, 2021, must be later than CreatedDate)\n",
    "def generate_sell_start_date(created_dates):\n",
    "    \"\"\"Generate SellStartDate that is later than CreatedDate\"\"\"\n",
    "    sell_start_dates = []\n",
    "    \n",
    "    sell_start_min = date(2020, 1, 1)\n",
    "    sell_start_max = date(2021, 9, 30)\n",
    "    \n",
    "    for created_date in created_dates:\n",
    "        # SellStartDate must be later than CreatedDate\n",
    "        # Use the later of CreatedDate or sell_start_min as the minimum\n",
    "        min_start_date = max(created_date, sell_start_min)\n",
    "        \n",
    "        # If min_start_date is after sell_start_max, use sell_start_max\n",
    "        if min_start_date > sell_start_max:\n",
    "            sell_start_date = sell_start_max\n",
    "        else:\n",
    "            # Generate random date between min_start_date and sell_start_max\n",
    "            days_range = (sell_start_max - min_start_date).days\n",
    "            if days_range > 0:\n",
    "                random_days = np.random.randint(0, days_range + 1)\n",
    "                sell_start_date = min_start_date + timedelta(days=int(random_days))\n",
    "            else:\n",
    "                sell_start_date = min_start_date\n",
    "        \n",
    "        sell_start_dates.append(sell_start_date)\n",
    "    \n",
    "    return sell_start_dates\n",
    "\n",
    "# 6. Generate SellEndDate (Less than 5% discontinued, Jan 1, 2022 to March 31, 2023)\n",
    "def generate_sell_end_date(num_records):\n",
    "    \"\"\"Generate SellEndDate for less than 5% of products (discontinued products)\"\"\"\n",
    "    sell_end_dates = []\n",
    "    \n",
    "    start_date = date(2022, 1, 1)\n",
    "    end_date = date(2023, 3, 31)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        # Less than 5% chance of being discontinued\n",
    "        if np.random.random() < 0.05:\n",
    "            # Calculate total days for discontinued date range\n",
    "            total_days = (end_date - start_date).days\n",
    "            random_days = np.random.randint(0, total_days + 1)\n",
    "            sell_end_date = start_date + timedelta(days=int(random_days))\n",
    "            sell_end_dates.append(sell_end_date)\n",
    "        else:\n",
    "            # Product is still active, empty string instead of None\n",
    "            sell_end_dates.append(\"\")\n",
    "    \n",
    "    return sell_end_dates\n",
    "\n",
    "# 7. Generate ProductStatus (96% active, 3% inactive, 1% discontinued)\n",
    "def generate_product_status(num_records):\n",
    "    \"\"\"Generate ProductStatus with specified distribution\"\"\"\n",
    "    statuses = ['active', 'inactive', 'discontinued']\n",
    "    probabilities = [0.96, 0.03, 0.01]\n",
    "    return np.random.choice(statuses, size=num_records, p=probabilities)\n",
    "\n",
    "print(\"🔄 Generating field data...\")\n",
    "\n",
    "# Generate all fields\n",
    "product_ids = generate_product_id(SAMPLE_SIZE)\n",
    "product_names = generate_product_name(SAMPLE_SIZE)\n",
    "brand_names = generate_brand_name(SAMPLE_SIZE)\n",
    "created_dates = generate_created_date(SAMPLE_SIZE)\n",
    "sell_start_dates = generate_sell_start_date(created_dates)\n",
    "sell_end_dates = generate_sell_end_date(SAMPLE_SIZE)\n",
    "product_statuses = generate_product_status(SAMPLE_SIZE)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'ProductId': product_ids,\n",
    "    'ProductName': product_names,\n",
    "    'BrandName': brand_names,\n",
    "    'CreatedDate': created_dates,\n",
    "    'SellStartDate': sell_start_dates,\n",
    "    'SellEndDate': sell_end_dates,\n",
    "    'ProductStatus': product_statuses\n",
    "})\n",
    "\n",
    "print(\"✅ Data generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c7180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 DATA DISTRIBUTION ANALYSIS\n",
      "==================================================\n",
      "\n",
      "🎯 BrandName Distribution:\n",
      "  Fabrikam       : 205 ( 69.5%)\n",
      "  Alpine Ski House:  90 ( 30.5%)\n",
      "\n",
      "🎯 ProductStatus Distribution:\n",
      "  active      : 280 ( 94.9%)\n",
      "  inactive    :  11 (  3.7%)\n",
      "  discontinued:   4 (  1.4%)\n",
      "\n",
      "🎯 CreatedDate Range:\n",
      "  Earliest: 2020-01-01\n",
      "  Latest  : 2020-12-30\n",
      "\n",
      "🎯 SellStartDate Range:\n",
      "  Earliest: 2020-02-09\n",
      "  Latest  : 2021-09-27\n",
      "  Invalid dates (SellStart <= Created): 2\n",
      "\n",
      "🎯 SellEndDate Distribution:\n",
      "  Discontinued:  11 (  3.7%)\n",
      "  Active      : 284 ( 96.3%)\n",
      "  End Date Range: 2022-01-14 to 2023-03-11\n",
      "\n",
      "📋 First 10 Sample Records:\n",
      "ProductId ProductName        BrandName CreatedDate SellStartDate SellEndDate ProductStatus\n",
      "    P-001   Product 1         Fabrikam  2020-09-20    2021-05-17                    active\n",
      "    P-002   Product 2 Alpine Ski House  2020-02-22    2021-09-01                    active\n",
      "    P-003   Product 3 Alpine Ski House  2020-02-29    2021-08-28                    active\n",
      "    P-004   Product 4         Fabrikam  2020-04-17    2020-12-25                    active\n",
      "    P-005   Product 5         Fabrikam  2020-01-05    2021-01-28                    active\n",
      "    P-006   Product 6         Fabrikam  2020-04-12    2020-12-23                    active\n",
      "    P-007   Product 7         Fabrikam  2020-07-14    2021-01-26                    active\n",
      "    P-008   Product 8 Alpine Ski House  2020-09-18    2021-07-17                    active\n",
      "    P-009   Product 9         Fabrikam  2020-12-30    2021-01-08                    active\n",
      "    P-010  Product 10 Alpine Ski House  2020-12-15    2021-09-26                    active\n",
      "\n",
      "💾 SAVED TO: C:\\temp\\samples\\Product_Special_Fields.csv\n",
      "📊 Total Records: 295\n",
      "📈 Columns: ProductId, ProductName, BrandName, CreatedDate, SellStartDate, SellEndDate, ProductStatus\n",
      "\n",
      "✅ Product sample data generation complete!\n"
     ]
    }
   ],
   "source": [
    "# filepath: c:\\Repos\\Code\\SampleDataPrep\\src\\notebooks\\data\\Generate_Product_Samples.ipynb\n",
    "# Display distributions and statistics\n",
    "print(\"\\n📊 DATA DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# BrandName Distribution\n",
    "print(\"\\n🎯 BrandName Distribution:\")\n",
    "brand_dist = df['BrandName'].value_counts()\n",
    "brand_pct = df['BrandName'].value_counts(normalize=True) * 100\n",
    "for brand in ['Fabrikam', 'Alpine Ski House']:\n",
    "    count = brand_dist.get(brand, 0)\n",
    "    percent = brand_pct.get(brand, 0)\n",
    "    print(f\"  {brand:15}: {count:3d} ({percent:5.1f}%)\")\n",
    "\n",
    "# ProductStatus Distribution\n",
    "print(\"\\n🎯 ProductStatus Distribution:\")\n",
    "status_dist = df['ProductStatus'].value_counts()\n",
    "status_pct = df['ProductStatus'].value_counts(normalize=True) * 100\n",
    "for status in ['active', 'inactive', 'discontinued']:\n",
    "    count = status_dist.get(status, 0)\n",
    "    percent = status_pct.get(status, 0)\n",
    "    print(f\"  {status:12}: {count:3d} ({percent:5.1f}%)\")\n",
    "\n",
    "# Date Range Analysis\n",
    "print(\"\\n🎯 CreatedDate Range:\")\n",
    "min_created = df['CreatedDate'].min()\n",
    "max_created = df['CreatedDate'].max()\n",
    "print(f\"  Earliest: {min_created}\")\n",
    "print(f\"  Latest  : {max_created}\")\n",
    "\n",
    "print(\"\\n🎯 SellStartDate Range:\")\n",
    "min_sell_start = df['SellStartDate'].min()\n",
    "max_sell_start = df['SellStartDate'].max()\n",
    "print(f\"  Earliest: {min_sell_start}\")\n",
    "print(f\"  Latest  : {max_sell_start}\")\n",
    "\n",
    "# Validate SellStartDate > CreatedDate\n",
    "invalid_dates = df[df['SellStartDate'] <= df['CreatedDate']]\n",
    "print(f\"  Invalid dates (SellStart <= Created): {len(invalid_dates)}\")\n",
    "\n",
    "print(\"\\n🎯 SellEndDate Distribution:\")\n",
    "discontinued_count = len([d for d in df['SellEndDate'] if d != \"\"])\n",
    "active_count = len([d for d in df['SellEndDate'] if d == \"\"])\n",
    "print(f\"  Discontinued: {discontinued_count:3d} ({discontinued_count/len(df)*100:5.1f}%)\")\n",
    "print(f\"  Active      : {active_count:3d} ({active_count/len(df)*100:5.1f}%)\")\n",
    "\n",
    "if discontinued_count > 0:\n",
    "    discontinued_df = df[df['SellEndDate'] != \"\"]\n",
    "    min_end = discontinued_df['SellEndDate'].min()\n",
    "    max_end = discontinued_df['SellEndDate'].max()\n",
    "    print(f\"  End Date Range: {min_end} to {max_end}\")\n",
    "\n",
    "print(f\"\\n📋 First 10 Sample Records:\")\n",
    "print(df.head(10).to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(OUTPUT_FOLDER, OUTPUT_FILE)\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n💾 SAVED TO: {output_path}\")\n",
    "print(f\"📊 Total Records: {len(df)}\")\n",
    "print(f\"📈 Columns: {', '.join(df.columns)}\")\n",
    "print(\"\\n✅ Product sample data generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1f4829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TESTING FILE READ FROM WORKSPACE\n",
      "==================================================\n",
      "🎯 Attempting to read uploaded file...\n",
      "(Update the email address in the path below)\n",
      "📂 Trying path: /Workspace/Users/your_email/source_data/product_tents.csv\n",
      "❌ Error reading file: name 'spark' is not defined\n",
      "\n",
      "💡 To fix this:\n",
      "1. Check your workspace path in Azure Databricks\n",
      "2. Update the file_path variable above with your actual email\n",
      "3. Verify the file name is correct (product_tents.csv)\n",
      "4. Make sure the file was uploaded to source_data folder\n",
      "\n",
      "🔍 Alternative: Use dbutils to explore your workspace:\n",
      "   %fs ls /Workspace/Users/\n",
      "   %fs ls /Workspace/Users/your_actual_email/\n",
      "   %fs ls /Workspace/Users/your_actual_email/source_data/\n"
     ]
    }
   ],
   "source": [
    "# Test Reading Uploaded File from Azure Databricks Workspace\n",
    "print(\"🔍 TESTING FILE READ FROM WORKSPACE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Note: Replace 'your_email' with your actual email address from the workspace path\n",
    "# Example paths to try (update with your actual email):\n",
    "# \"/Workspace/Users/your.email@company.com/source_data/product_tents.csv\"\n",
    "\n",
    "# Test different possible paths\n",
    "possible_paths = [\n",
    "    \"/Workspace/Users/your_email/source_data/product_tents.csv\",  # Replace your_email\n",
    "    \"/Workspace/source_data/product_tents.csv\",\n",
    "    \"/source_data/product_tents.csv\"\n",
    "]\n",
    "\n",
    "print(\"🎯 Attempting to read uploaded file...\")\n",
    "print(\"(Update the email address in the path below)\")\n",
    "\n",
    "# Try to read the file - you'll need to update the path\n",
    "try:\n",
    "    # UPDATE THIS PATH with your actual email address:\n",
    "    file_path = \"/Workspace/Users/your_email/source_data/product_tents.csv\"\n",
    "    \n",
    "    print(f\"📂 Trying path: {file_path}\")\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    print(\"✅ File read successfully!\")\n",
    "    print(f\"📊 Shape: {df.count()} rows x {len(df.columns)} columns\")\n",
    "    print(f\"📋 Columns: {df.columns}\")\n",
    "    \n",
    "    print(\"\\n🔍 Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(\"\\n📖 First 10 rows:\")\n",
    "    df.show(10)\n",
    "    \n",
    "    print(\"\\n📈 Data Summary:\")\n",
    "    print(f\"  Total Records: {df.count()}\")\n",
    "    print(f\"  Brand Distribution:\")\n",
    "    df.groupBy(\"BrandName\").count().show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error reading file: {e}\")\n",
    "    print(\"\\n💡 To fix this:\")\n",
    "    print(\"1. Check your workspace path in Azure Databricks\")\n",
    "    print(\"2. Update the file_path variable above with your actual email\")\n",
    "    print(\"3. Verify the file name is correct (product_tents.csv)\")\n",
    "    print(\"4. Make sure the file was uploaded to source_data folder\")\n",
    "    \n",
    "    print(\"\\n🔍 Alternative: Use dbutils to explore your workspace:\")\n",
    "    print(\"   %fs ls /Workspace/Users/\")\n",
    "    print(\"   %fs ls /Workspace/Users/your_actual_email/\")\n",
    "    print(\"   %fs ls /Workspace/Users/your_actual_email/source_data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
