{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Databricks 2-Hour Crash Course\n",
    "\n",
    "## üéØ Learning Objectives (2 Hours Total)\n",
    "- **30 min:** Unity Catalog fundamentals and three-level namespace\n",
    "- **45 min:** Delta Lake creation, time travel, and MERGE operations\n",
    "- **30 min:** Data Engineering basics (Jobs, Workflows, External Storage)\n",
    "- **15 min:** Integration patterns for your solution accelerator\n",
    "\n",
    "## üìö Prerequisites\n",
    "- Azure Databricks workspace access\n",
    "- Basic SQL and Python knowledge\n",
    "- Sample data available\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Unity Catalog Crash Course (30 minutes)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-354e3eea\" language=\"python\">\n",
    "# üèõÔ∏è PART 1A: UNITY CATALOG EXPLORATION (10 minutes)\n",
    "print(\"üèõÔ∏è UNITY CATALOG EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check current catalog context\n",
    "print(\"\\nüìç Current Catalog Context:\")\n",
    "try:\n",
    "    print(f\"Current Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "    print(f\"Current Schema: {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error getting current context: {e}\")\n",
    "    print(\"üí° Unity Catalog may not be enabled\")\n",
    "\n",
    "# List all catalogs\n",
    "print(\"\\nüìö Available Catalogs:\")\n",
    "try:\n",
    "    catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "    for catalog in catalogs:\n",
    "        print(f\"  üìÅ {catalog.catalog}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error listing catalogs: {e}\")\n",
    "\n",
    "# List schemas in main catalog\n",
    "print(\"\\nüìã Schemas in 'main' catalog:\")\n",
    "try:\n",
    "    schemas = spark.sql(\"SHOW SCHEMAS IN CATALOG main\").collect()\n",
    "    for schema in schemas:\n",
    "        print(f\"  üìÇ main.{schema.databaseName}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Trying default schema listing...\")\n",
    "    try:\n",
    "        schemas = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "        for schema in schemas:\n",
    "            print(f\"  üìÇ {schema.databaseName}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Error with default schemas: {e2}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-8fa29ab2\" language=\"python\">\n",
    "# üèóÔ∏è PART 1B: CREATE YOUR OWN CATALOG STRUCTURE (10 minutes)\n",
    "print(\"\\nüèóÔ∏è CREATING CATALOG STRUCTURE FOR SOLUTION ACCELERATOR\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a catalog for your solution accelerator (if permissions allow)\n",
    "catalog_name = \"solution_accelerator\"\n",
    "schema_names = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "print(f\"\\nüìÅ Attempting to create catalog: {catalog_name}\")\n",
    "try:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "    print(\"  ‚úÖ Catalog created/exists\")\n",
    "    current_catalog = catalog_name\n",
    "    \n",
    "    # Create schemas for medallion architecture\n",
    "    for schema in schema_names:\n",
    "        print(f\"\\nüìÇ Creating schema: {catalog_name}.{schema}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "        print(f\"  ‚úÖ Schema {catalog_name}.{schema} created/exists\")\n",
    "        \n",
    "    # Set working catalog\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    print(f\"\\n‚úÖ Now working in catalog: {catalog_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Cannot create catalog (permissions): {e}\")\n",
    "    print(\"  üí° Using 'main' catalog instead\")\n",
    "    current_catalog = \"main\"\n",
    "    \n",
    "    # Try to create schemas in main catalog or use default\n",
    "    for schema in schema_names:\n",
    "        try:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "            print(f\"  ‚úÖ Schema {schema} created in main catalog\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not create schema {schema}: {e}\")\n",
    "            print(f\"     Will use 'default' schema\")\n",
    "\n",
    "print(f\"\\nüìç Working Context:\")\n",
    "try:\n",
    "    print(f\"   Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "    print(f\"   Schema:  {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "except:\n",
    "    print(f\"   Using default context\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-c8de4372\" language=\"python\">\n",
    "# üîç PART 1C: THREE-LEVEL NAMESPACE DEMO (10 minutes)\n",
    "print(\"\\nüîç THREE-LEVEL NAMESPACE DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Demonstrate three-level namespace: catalog.schema.table\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "except:\n",
    "    current_catalog = \"main\"\n",
    "    current_schema = \"default\"\n",
    "\n",
    "print(f\"üìç Three-Level Namespace Structure:\")\n",
    "print(f\"   Catalog: {current_catalog}\")\n",
    "print(f\"   Schema:  bronze, silver, gold (or {current_schema})\")\n",
    "print(f\"   Table:   customer, product, orders\")\n",
    "print(f\"   Full:    {current_catalog}.{current_schema}.customer\")\n",
    "\n",
    "# List tables in current schema\n",
    "print(f\"\\nüìä Tables in current schema:\")\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    if tables:\n",
    "        for table in tables[:5]:  # Show first 5\n",
    "            print(f\"   üìã {table.tableName}\")\n",
    "    else:\n",
    "        print(\"   üì≠ No tables found (expected for new schemas)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error listing tables: {e}\")\n",
    "\n",
    "print(\"\\nüí° Key Unity Catalog Concepts:\")\n",
    "print(\"   ‚Ä¢ Catalogs = Top-level containers (like databases)\")\n",
    "print(\"   ‚Ä¢ Schemas = Logical groupings within catalogs\")  \n",
    "print(\"   ‚Ä¢ Tables = Data assets within schemas\")\n",
    "print(\"   ‚Ä¢ Full path: catalog.schema.table\")\n",
    "print(\"   ‚Ä¢ Unity Catalog provides governance, lineage, and access control\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-c3721e4c\" language=\"markdown\">\n",
    "---\n",
    "\n",
    "# Part 2: Delta Lake Deep Dive (45 minutes)\n",
    "\n",
    "## What is Delta Lake?\n",
    "- **ACID transactions** on data lakes\n",
    "- **Time travel** - query historical versions\n",
    "- **Schema evolution** - safely modify table structure\n",
    "- **Optimizations** - Z-ordering, auto-compaction\n",
    "- **Merges/Upserts** - efficiently update data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-ce8a9ba3\" language=\"python\">\n",
    "# üöÄ PART 2A: CREATE SAMPLE DATA AND DELTA TABLES (15 minutes)\n",
    "print(\"üöÄ CREATING SAMPLE DATA FOR DELTA LAKE DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample customer data\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set seed for reproducible results\n",
    "random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "customers_data = []\n",
    "cities = ['Seattle', 'Portland', 'San Francisco', 'Los Angeles', 'Denver', 'Austin', 'Chicago', 'Boston']\n",
    "statuses = ['Active', 'Inactive', 'Pending', 'VIP']\n",
    "\n",
    "for i in range(100):\n",
    "    customers_data.append({\n",
    "        'customer_id': i + 1,\n",
    "        'customer_name': f'Customer_{i+1:03d}',\n",
    "        'email': f'customer{i+1}@example.com',\n",
    "        'city': random.choice(cities),\n",
    "        'registration_date': datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "        'status': random.choice(statuses),\n",
    "        'annual_spend': round(random.uniform(1000, 10000), 2)\n",
    "    })\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "try:\n",
    "    customers_df = spark.createDataFrame(customers_data)\n",
    "    print(\"üìä Sample Customer Data Created:\")\n",
    "    customers_df.show(5)\n",
    "    print(f\"   Total Records: {customers_df.count()}\")\n",
    "    print(\"   ‚úÖ Sample data generation successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating sample data: {e}\")\n",
    "    # Create a simple fallback dataset\n",
    "    print(\"Creating fallback dataset...\")\n",
    "    simple_data = [(1, \"Customer_001\", \"test@example.com\", \"Seattle\", datetime.now(), \"Active\", 5000.0)]\n",
    "    customers_df = spark.createDataFrame(simple_data, \n",
    "        ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status', 'annual_spend'])\n",
    "    print(\"‚úÖ Fallback data created\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-a3299514\" language=\"python\">\n",
    "# üèóÔ∏è PART 2B: CREATE DELTA TABLE (10 minutes)\n",
    "print(\"\\nüèóÔ∏è CREATING DELTA TABLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set working schema\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "    \n",
    "    # Try to use bronze schema if it exists\n",
    "    try:\n",
    "        spark.sql(\"USE SCHEMA bronze\")\n",
    "        current_schema = \"bronze\"\n",
    "        print(f\"‚úÖ Using schema: {current_catalog}.bronze\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Using current schema: {current_catalog}.{current_schema}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Using default context: {e}\")\n",
    "    current_catalog = \"main\"\n",
    "    current_schema = \"default\"\n",
    "\n",
    "# Create Delta table\n",
    "table_name = \"customers_delta\"\n",
    "\n",
    "print(f\"\\nüìã Creating Delta table: {table_name}\")\n",
    "print(f\"   Full path: {current_catalog}.{current_schema}.{table_name}\")\n",
    "\n",
    "try:\n",
    "    # Write as Delta table\n",
    "    customers_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    print(\"‚úÖ Delta table created successfully!\")\n",
    "\n",
    "    # Verify table creation\n",
    "    print(f\"\\nüîç Table Information:\")\n",
    "    table_info = spark.sql(f\"DESCRIBE EXTENDED {table_name}\")\n",
    "    print(f\"   Rows: {customers_df.count()}\")\n",
    "    print(f\"   Columns: {len(customers_df.columns)}\")\n",
    "    print(f\"   Format: Delta Lake\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating Delta table: {e}\")\n",
    "    print(\"üí° This might be due to permissions or catalog configuration\")\n",
    "    \n",
    "    # Try alternative approach - create in temp location\n",
    "    try:\n",
    "        print(\"\\nüîÑ Trying alternative approach...\")\n",
    "        temp_path = \"/tmp/customers_delta\"\n",
    "        customers_df.write.format(\"delta\").mode(\"overwrite\").save(temp_path)\n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{temp_path}'\")\n",
    "        print(\"‚úÖ Delta table created in temp location\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Alternative approach failed: {e2}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-3d4541d4\" language=\"python\">\n",
    "# ‚è∞ PART 2C: DELTA LAKE TIME TRAVEL (10 minutes)\n",
    "print(\"\\n‚è∞ DELTA LAKE TIME TRAVEL DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Show initial version\n",
    "    print(\"üìä Initial table state:\")\n",
    "    initial_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0][0]\n",
    "    print(f\"   Records: {initial_count}\")\n",
    "\n",
    "    # Make some updates to create versions\n",
    "    print(\"\\nüîÑ Creating new versions for time travel demo...\")\n",
    "\n",
    "    # Version 1: Update some customers to VIP status\n",
    "    print(\"Version 1: Updating customer status to VIP...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {table_name} \n",
    "        SET status = 'VIP', annual_spend = annual_spend * 1.5\n",
    "        WHERE customer_id <= 10\n",
    "    \"\"\")\n",
    "    \n",
    "    vip_count_v1 = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name} WHERE status = 'VIP'\").collect()[0][0]\n",
    "    print(f\"   VIP customers after update: {vip_count_v1}\")\n",
    "\n",
    "    # Version 2: Insert new customers  \n",
    "    print(\"\\nVersion 2: Adding new customers...\")\n",
    "    new_customers = spark.createDataFrame([\n",
    "        (101, 'Customer_101', 'customer101@example.com', 'Boston', datetime.now(), 'Active', 7500.0),\n",
    "        (102, 'Customer_102', 'customer102@example.com', 'Chicago', datetime.now(), 'Active', 8200.0)\n",
    "    ], ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status', 'annual_spend'])\n",
    "\n",
    "    new_customers.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "    \n",
    "    final_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0][0]\n",
    "    print(f\"   Total customers after insert: {final_count}\")\n",
    "\n",
    "    # Show table history\n",
    "    print(\"\\nüìö Delta Table History:\")\n",
    "    try:\n",
    "        history_df = spark.sql(f\"DESCRIBE HISTORY {table_name}\")\n",
    "        history_df.select(\"version\", \"timestamp\", \"operation\").show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not show history: {e}\")\n",
    "\n",
    "    # Time travel examples\n",
    "    print(\"\\n‚è∞ Time Travel Examples:\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\nüì∏ Version 0 (original):\")\n",
    "        v0_result = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as total_count, \n",
    "                   COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count \n",
    "            FROM {table_name} VERSION AS OF 0\n",
    "        \"\"\").collect()[0]\n",
    "        print(f\"   Total: {v0_result[0]}, VIP: {v0_result[1]}\")\n",
    "\n",
    "        print(\"\\nüì∏ Version 1 (after VIP updates):\")\n",
    "        v1_result = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as total_count, \n",
    "                   COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count \n",
    "            FROM {table_name} VERSION AS OF 1\n",
    "        \"\"\").collect()[0]\n",
    "        print(f\"   Total: {v1_result[0]}, VIP: {v1_result[1]}\")\n",
    "\n",
    "        print(\"\\nüì∏ Current version:\")\n",
    "        current_result = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as total_count, \n",
    "                   COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count \n",
    "            FROM {table_name}\n",
    "        \"\"\").collect()[0]\n",
    "        print(f\"   Total: {current_result[0]}, VIP: {current_result[1]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Time travel queries failed: {e}\")\n",
    "        print(\"üí° Time travel might not be available in this environment\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in time travel demo: {e}\")\n",
    "    print(\"üí° Make sure the Delta table was created successfully first\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-d63f8880\" language=\"python\">\n",
    "# üîÄ PART 2D: DELTA MERGE OPERATIONS (10 minutes)\n",
    "print(\"\\nüîÄ DELTA MERGE OPERATIONS DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Create updates dataset\n",
    "    updates_data = [\n",
    "        (5, 'Customer_005_Updated', 'updated5@example.com', 'Updated_City', datetime.now(), 'Premium', 12000.0),\n",
    "        (10, 'Customer_010_Updated', 'updated10@example.com', 'Updated_City', datetime.now(), 'Premium', 15000.0),\n",
    "        (103, 'Customer_103_New', 'customer103@example.com', 'Miami', datetime.now(), 'Active', 9500.0)  # New customer\n",
    "    ]\n",
    "\n",
    "    updates_df = spark.createDataFrame(updates_data, \n",
    "        ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status', 'annual_spend'])\n",
    "\n",
    "    print(\"üìä Updates to apply:\")\n",
    "    updates_df.show()\n",
    "\n",
    "    # Create temporary view for MERGE\n",
    "    updates_df.createOrReplaceTempView(\"customer_updates\")\n",
    "\n",
    "    # Perform MERGE operation\n",
    "    print(\"\\nüîÄ Executing MERGE operation...\")\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {table_name} as target\n",
    "    USING customer_updates as source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET \n",
    "            customer_name = source.customer_name,\n",
    "            email = source.email,\n",
    "            city = source.city,\n",
    "            status = source.status,\n",
    "            annual_spend = source.annual_spend\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, customer_name, email, city, registration_date, status, annual_spend)\n",
    "        VALUES (source.customer_id, source.customer_name, source.email, source.city, source.registration_date, source.status, source.annual_spend)\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(merge_sql)\n",
    "    print(\"‚úÖ MERGE operation completed!\")\n",
    "\n",
    "    # Verify results\n",
    "    print(\"\\nüìä Results after MERGE:\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "        SELECT customer_id, customer_name, city, status, annual_spend\n",
    "        FROM {table_name} \n",
    "        WHERE customer_id IN (5, 10, 103)\n",
    "        ORDER BY customer_id\n",
    "    \"\"\")\n",
    "    result_df.show()\n",
    "\n",
    "    print(\"\\nüìà Total count after MERGE:\")\n",
    "    final_count = spark.sql(f\"SELECT COUNT(*) as total_customers FROM {table_name}\").collect()[0][0]\n",
    "    print(f\"   Total customers: {final_count}\")\n",
    "    \n",
    "    print(\"\\nüí∞ Premium customers summary:\")\n",
    "    premium_summary = spark.sql(f\"\"\"\n",
    "        SELECT status, COUNT(*) as count, AVG(annual_spend) as avg_spend\n",
    "        FROM {table_name}\n",
    "        WHERE status IN ('Premium', 'VIP')\n",
    "        GROUP BY status\n",
    "        ORDER BY avg_spend DESC\n",
    "    \"\"\")\n",
    "    premium_summary.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in MERGE operation: {e}\")\n",
    "    print(\"üí° MERGE operations require Delta Lake table format\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-7fddcbf9\" language=\"markdown\">\n",
    "---\n",
    "\n",
    "# Part 3: Data Engineering Basics (30 minutes)\n",
    "\n",
    "## Key Concepts:\n",
    "- **Notebooks ‚Üí Jobs** (scheduled execution)\n",
    "- **Workflows** (multi-task orchestration)  \n",
    "- **External Storage** (Azure Data Lake integration)\n",
    "- **Delta Live Tables** (streaming ETL)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-10a1372e\" language=\"python\">\n",
    "# üîß PART 3A: EXTERNAL STORAGE CONNECTION (10 minutes)\n",
    "print(\"üîß EXTERNAL STORAGE CONNECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check existing mounts\n",
    "print(\"üìé Current mounted storage:\")\n",
    "try:\n",
    "    mounts = dbutils.fs.mounts()\n",
    "    if mounts:\n",
    "        for mount in mounts:\n",
    "            print(f\"   üîó {mount.mountPoint} ‚Üí {mount.source}\")\n",
    "    else:\n",
    "        print(\"   üì≠ No external storage currently mounted\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking mounts: {e}\")\n",
    "\n",
    "# Demo: How to mount Azure Data Lake (conceptual - requires credentials)\n",
    "print(f\"\\nüí° Mounting Azure Data Lake Storage (ADLS) - Conceptual Example:\")\n",
    "mount_example = '''\n",
    "# Mount ADLS Gen2 for your solution accelerator\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://container@storageaccount.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/solution-accelerator\",\n",
    "    extra_configs = {\n",
    "        \"fs.azure.account.auth.type.storageaccount.dfs.core.windows.net\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type.storageaccount.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id.storageaccount.dfs.core.windows.net\": \"application-id\",\n",
    "        \"fs.azure.account.oauth2.client.secret.storageaccount.dfs.core.windows.net\": \"service-credential-key\"\n",
    "    }\n",
    ")\n",
    "'''\n",
    "print(mount_example)\n",
    "\n",
    "# Show how to work with mounted storage\n",
    "print(\"\\nüìÅ Working with mounted storage:\")\n",
    "print(\"   ‚Ä¢ List files: dbutils.fs.ls('/mnt/solution-accelerator/')\")\n",
    "print(\"   ‚Ä¢ Read CSV: spark.read.csv('/mnt/solution-accelerator/data.csv')\")\n",
    "print(\"   ‚Ä¢ Write Delta: df.write.format('delta').save('/mnt/solution-accelerator/delta-table')\")\n",
    "\n",
    "# For your solution accelerator integration:\n",
    "print(f\"\\nüéØ For Your Solution Accelerator:\")\n",
    "print(\"   1. Mount Azure Data Lake where Fabric can access\")\n",
    "print(\"   2. Write Delta tables to mounted location\")  \n",
    "print(\"   3. Fabric reads from same Azure Data Lake location\")\n",
    "print(\"   4. Creates seamless Databricks ‚Üí Fabric pipeline\")\n",
    "\n",
    "# Demonstrate file operations that work without mounts\n",
    "print(f\"\\nüõ†Ô∏è Basic file operations (no mount required):\")\n",
    "try:\n",
    "    # Create a sample file in DBFS\n",
    "    sample_path = \"/tmp/sample_data.csv\"\n",
    "    sample_content = \"id,name,value\\n1,Sample,100\\n2,Test,200\"\n",
    "    dbutils.fs.put(sample_path, sample_content, overwrite=True)\n",
    "    print(f\"   ‚úÖ Created sample file: {sample_path}\")\n",
    "    \n",
    "    # Read it back as DataFrame\n",
    "    df_sample = spark.read.csv(sample_path, header=True, inferSchema=True)\n",
    "    df_sample.show()\n",
    "    \n",
    "    # Clean up\n",
    "    dbutils.fs.rm(sample_path)\n",
    "    print(\"   ‚úÖ Sample file removed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error in file operations: {e}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-26a8e5f7\" language=\"python\">\n",
    "# üìä PART 3B: JOBS AND WORKFLOWS CONCEPTS (10 minutes)\n",
    "print(\"\\nüìä JOBS AND WORKFLOWS CONCEPTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üîÑ Converting Notebooks to Jobs:\")\n",
    "print(\"   1. Save your notebook (this one!)\")\n",
    "print(\"   2. Go to 'Workflows' in Databricks sidebar\")\n",
    "print(\"   3. Click 'Create Job'\")\n",
    "print(\"   4. Select this notebook as the task\")\n",
    "print(\"   5. Configure cluster and schedule\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Job Configuration Example for Your Solution Accelerator:\")\n",
    "job_config = '''\n",
    "Job Name: \"Solution Accelerator Data Pipeline\"\n",
    "Tasks:\n",
    "‚îú‚îÄ‚îÄ Task 1: \"Data Ingestion\" \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Notebook: /Users/your-email/Data_Ingestion.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Cluster: Job cluster (auto-terminating)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Libraries: Delta Lake, pandas\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Schedule: Daily at 2 AM\n",
    "‚îú‚îÄ‚îÄ Task 2: \"Data Transformation\" (depends on Task 1)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Notebook: /Users/your-email/Data_Transformation.ipynb  \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Parameters: {\"source_table\": \"bronze.customers\"}\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Timeout: 30 minutes\n",
    "‚îî‚îÄ‚îÄ Task 3: \"Data Quality Check\" (depends on Task 2)\n",
    "    ‚îú‚îÄ‚îÄ Notebook: /Users/your-email/Data_Quality.ipynb\n",
    "    ‚îú‚îÄ‚îÄ Alerts: Email on failure\n",
    "    ‚îî‚îÄ‚îÄ Retry: 3 attempts with exponential backoff\n",
    "'''\n",
    "print(job_config)\n",
    "\n",
    "print(f\"\\nüìà Workflow Benefits:\")\n",
    "print(\"   ‚úÖ Automated execution (no manual intervention)\")\n",
    "print(\"   ‚úÖ Dependency management (tasks run in order)\") \n",
    "print(\"   ‚úÖ Error handling and retries (resilient)\")\n",
    "print(\"   ‚úÖ Monitoring and alerts (proactive)\")\n",
    "print(\"   ‚úÖ Parameter passing between tasks (flexible)\")\n",
    "print(\"   ‚úÖ Cost optimization (clusters auto-terminate)\")\n",
    "\n",
    "print(f\"\\nüéØ Real-world Use Cases:\")\n",
    "print(\"   ‚Ä¢ Daily data ingestion from source systems\")\n",
    "print(\"   ‚Ä¢ ETL pipelines with quality checks\")\n",
    "print(\"   ‚Ä¢ Model training and deployment\")\n",
    "print(\"   ‚Ä¢ Report generation and distribution\")\n",
    "print(\"   ‚Ä¢ Data validation and monitoring\")\n",
    "\n",
    "# Demonstrate job parameters concept\n",
    "print(f\"\\nüìù Job Parameters Example:\")\n",
    "print(\"   In this notebook, you could access job parameters like:\")\n",
    "print(\"   source_date = dbutils.widgets.get('source_date')\")\n",
    "print(\"   table_name = dbutils.widgets.get('table_name')\")\n",
    "print(\"   environment = dbutils.widgets.get('environment')\")\n",
    "\n",
    "# Show current notebook info that would be useful for jobs\n",
    "try:\n",
    "    context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    print(f\"\\nüìç Current Notebook Context:\")\n",
    "    print(f\"   User: {context.tags().get('user').get()}\")\n",
    "    print(f\"   Notebook: {context.notebookPath().get()}\")\n",
    "except:\n",
    "    print(f\"\\nüìç Notebook context not available in this environment\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-a0f12e84\" language=\"python\">\n",
    "# üåä PART 3C: DELTA LIVE TABLES OVERVIEW (10 minutes)\n",
    "print(\"\\nüåä DELTA LIVE TABLES (DLT) OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üí° What is Delta Live Tables?\")\n",
    "print(\"   ‚Ä¢ Declarative ETL framework\")\n",
    "print(\"   ‚Ä¢ Automatically manages dependencies\")\n",
    "print(\"   ‚Ä¢ Built-in data quality monitoring\")  \n",
    "print(\"   ‚Ä¢ Streaming and batch processing\")\n",
    "print(\"   ‚Ä¢ Automatic schema evolution\")\n",
    "print(\"   ‚Ä¢ Cost optimization with auto-scaling\")\n",
    "\n",
    "print(f\"\\nüìù DLT Example for Your Solution Accelerator:\")\n",
    "dlt_example = '''\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Bronze Layer - Raw data ingestion\n",
    "@dlt.table(\n",
    "    comment=\"Raw customer data from source systems\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/customers\")\n",
    "        .load(\"/mnt/raw-data/customers/\")\n",
    "    )\n",
    "\n",
    "# Silver Layer - Cleaned and validated data  \n",
    "@dlt.table(\n",
    "    comment=\"Cleaned customer data with quality checks\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect(\"valid_email\", \"email IS NOT NULL AND email RLIKE '^[^@]+@[^@]+\\\\\\\\.[^@]+$'\")\n",
    "@dlt.expect_or_drop(\"customer_id_not_null\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"reasonable_annual_spend\", \"annual_spend >= 0 AND annual_spend <= 100000\")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        dlt.read(\"bronze_customers\")\n",
    "        .select(\n",
    "            col(\"customer_id\").cast(\"int\"),\n",
    "            col(\"customer_name\"), \n",
    "            col(\"email\"), \n",
    "            col(\"city\"), \n",
    "            col(\"registration_date\").cast(\"date\"),\n",
    "            col(\"status\"),\n",
    "            col(\"annual_spend\").cast(\"double\")\n",
    "        )\n",
    "        .filter(\"customer_id IS NOT NULL\")\n",
    "        .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# Gold Layer - Business-ready aggregated data\n",
    "@dlt.table(\n",
    "    comment=\"Customer metrics for business analytics\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_customer_metrics():\n",
    "    return (\n",
    "        dlt.read(\"silver_customers\")\n",
    "        .groupBy(\"city\", \"status\")\n",
    "        .agg(\n",
    "            count(\"customer_id\").alias(\"customer_count\"),\n",
    "            avg(\"annual_spend\").alias(\"avg_annual_spend\"),\n",
    "            max(\"registration_date\").alias(\"latest_registration\"),\n",
    "            sum(\"annual_spend\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .withColumn(\"metric_date\", current_date())\n",
    "    )\n",
    "'''\n",
    "print(dlt_example)\n",
    "\n",
    "print(f\"\\nüéØ DLT Benefits for Your Architecture:\")\n",
    "print(\"   ‚úÖ Automatic Bronze ‚Üí Silver ‚Üí Gold pipeline\")\n",
    "print(\"   ‚úÖ Built-in data quality monitoring and alerts\")\n",
    "print(\"   ‚úÖ Real-time streaming capability\")\n",
    "print(\"   ‚úÖ Automatic dependency resolution\")\n",
    "print(\"   ‚úÖ Complete lineage tracking\")\n",
    "print(\"   ‚úÖ Schema evolution without breaking pipelines\")\n",
    "print(\"   ‚úÖ Automatic optimization and performance tuning\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è DLT vs Traditional ETL:\")\n",
    "comparison = '''\n",
    "Traditional ETL:                    Delta Live Tables:\n",
    "‚îú‚îÄ‚îÄ Manual dependency management    ‚îú‚îÄ‚îÄ Automatic dependency resolution\n",
    "‚îú‚îÄ‚îÄ Custom error handling          ‚îú‚îÄ‚îÄ Built-in quality expectations  \n",
    "‚îú‚îÄ‚îÄ Manual schema management       ‚îú‚îÄ‚îÄ Automatic schema evolution\n",
    "‚îú‚îÄ‚îÄ Complex orchestration          ‚îú‚îÄ‚îÄ Declarative pipeline definition\n",
    "‚îú‚îÄ‚îÄ Manual monitoring              ‚îú‚îÄ‚îÄ Built-in observability\n",
    "‚îî‚îÄ‚îÄ Infrastructure management      ‚îî‚îÄ‚îÄ Serverless execution\n",
    "'''\n",
    "print(comparison)\n",
    "\n",
    "print(f\"\\nüöÄ When to Use DLT:\")\n",
    "print(\"   ‚úÖ Complex ETL pipelines with multiple stages\")\n",
    "print(\"   ‚úÖ Streaming data processing requirements\")\n",
    "print(\"   ‚úÖ Strict data quality requirements\")\n",
    "print(\"   ‚úÖ Need for automatic dependency management\")\n",
    "print(\"   ‚úÖ Production-grade data pipelines\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-b30275c0\" language=\"markdown\">\n",
    "---\n",
    "\n",
    "# Part 4: Solution Accelerator Integration (15 minutes)\n",
    "\n",
    "## How This Applies to Your Project\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-8636aedc\" language=\"python\">\n",
    "# üéØ PART 4: SOLUTION ACCELERATOR INTEGRATION PATTERNS\n",
    "print(\"üéØ SOLUTION ACCELERATOR INTEGRATION PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üèóÔ∏è Your Architecture Integration:\")\n",
    "print(\"   Azure Databricks (Online Channel) ‚Üí Azure Data Lake ‚Üí Fabric Bronze ‚Üí Silver ‚Üí Gold\")\n",
    "\n",
    "print(f\"\\nüìã Implementation Roadmap:\")\n",
    "roadmap = '''\n",
    "Week 1-2: Databricks Setup\n",
    "‚îú‚îÄ‚îÄ ‚úÖ Create Unity Catalog structure (solution_accelerator.bronze/silver/gold)\n",
    "‚îú‚îÄ‚îÄ ‚úÖ Generate sample data in Delta format  \n",
    "‚îú‚îÄ‚îÄ üîÑ Set up external storage mounts to Azure Data Lake\n",
    "‚îî‚îÄ‚îÄ üîÑ Create basic ETL notebooks\n",
    "\n",
    "Week 3-4: Data Pipeline Development  \n",
    "‚îú‚îÄ‚îÄ üîÑ Build Bronze ‚Üí Silver transformation jobs\n",
    "‚îú‚îÄ‚îÄ üîÑ Implement data quality checks with Delta\n",
    "‚îú‚îÄ‚îÄ üîÑ Set up automated workflows\n",
    "‚îî‚îÄ‚îÄ üîÑ Create monitoring and alerting\n",
    "\n",
    "Week 5-6: Fabric Integration\n",
    "‚îú‚îÄ‚îÄ üîÑ Configure Fabric to read from same Azure Data Lake\n",
    "‚îú‚îÄ‚îÄ üîÑ Implement cross-channel data merging in Fabric Silver tier\n",
    "‚îú‚îÄ‚îÄ üîÑ Build Gold tier analytics in Fabric\n",
    "‚îî‚îÄ‚îÄ üîÑ Create Power BI dashboards\n",
    "\n",
    "Week 7-8: Production Readiness\n",
    "‚îú‚îÄ‚îÄ üîÑ Implement Delta Live Tables for streaming\n",
    "‚îú‚îÄ‚îÄ üîÑ Set up automated deployment pipelines  \n",
    "‚îú‚îÄ‚îÄ üîÑ Add comprehensive monitoring\n",
    "‚îî‚îÄ‚îÄ üîÑ Document integration patterns\n",
    "'''\n",
    "print(roadmap)\n",
    "\n",
    "# Show what you've accomplished in this crash course\n",
    "print(f\"\\nüéâ What You've Accomplished Today:\")\n",
    "print(\"   ‚úÖ Understanding Unity Catalog three-level namespace\")\n",
    "print(\"   ‚úÖ Creating Delta tables with sample data\")\n",
    "print(\"   ‚úÖ Practicing time travel and MERGE operations\")\n",
    "print(\"   ‚úÖ Learning about Jobs, Workflows, and DLT\")\n",
    "print(\"   ‚úÖ Understanding integration patterns for your project\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-59b70778\" language=\"python\">\n",
    "# üìä PRACTICAL NEXT STEPS FOR YOUR PROJECT\n",
    "print(\"\\nüìä PRACTICAL NEXT STEPS FOR YOUR PROJECT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üîß Immediate Actions (This Week):\")\n",
    "print(\"   1. ‚úÖ Complete this crash course notebook\")\n",
    "print(\"   2. Import your existing product generation notebook\")\n",
    "print(\"   3. Convert generated data to Delta format\")\n",
    "print(\"   4. Practice with Unity Catalog structure\")\n",
    "print(\"   5. Create customer and order generation notebooks\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Week Actions:\")\n",
    "print(\"   1. Set up Azure Data Lake mount point\")\n",
    "print(\"   2. Create automated jobs for data processing\")\n",
    "print(\"   3. Build basic Bronze ‚Üí Silver transformation\")\n",
    "print(\"   4. Test cross-channel data integration\")\n",
    "print(\"   5. Set up monitoring and quality checks\")\n",
    "\n",
    "print(f\"\\nüìà Integration Strategy:\")\n",
    "print(\"   1. Use Delta Lake as the reliable foundation\")\n",
    "print(\"   2. Leverage Unity Catalog for governance\")\n",
    "print(\"   3. External mounts for Fabric integration\")\n",
    "print(\"   4. Jobs/Workflows for automation\")\n",
    "print(\"   5. DLT for advanced production pipelines\")\n",
    "\n",
    "print(f\"\\nüí° Key Success Patterns for Your Solution Accelerator:\")\n",
    "patterns = '''\n",
    "‚úÖ Unity Catalog Governance:\n",
    "   ‚Ä¢ Use catalog.schema.table naming consistently\n",
    "   ‚Ä¢ Set up proper permissions and stewardship\n",
    "   ‚Ä¢ Document business glossary terms\n",
    "\n",
    "‚úÖ Delta Lake Foundation:\n",
    "   ‚Ä¢ All tables in Delta format for ACID transactions\n",
    "   ‚Ä¢ Use time travel for debugging and auditing\n",
    "   ‚Ä¢ Implement MERGE for efficient updates\n",
    "\n",
    "‚úÖ External Integration:\n",
    "   ‚Ä¢ Mount Azure Data Lake for Fabric connectivity\n",
    "   ‚Ä¢ Use consistent file formats across platforms\n",
    "   ‚Ä¢ Implement proper error handling and monitoring\n",
    "\n",
    "‚úÖ Automation Ready:\n",
    "   ‚Ä¢ Design notebooks for job conversion\n",
    "   ‚Ä¢ Use parameters for flexibility\n",
    "   ‚Ä¢ Implement proper logging and alerts\n",
    "\n",
    "‚úÖ Production Patterns:\n",
    "   ‚Ä¢ Use DLT for complex pipelines\n",
    "   ‚Ä¢ Implement comprehensive data quality checks\n",
    "   ‚Ä¢ Set up proper monitoring and governance\n",
    "'''\n",
    "print(patterns)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-aedc9ce1\" language=\"python\">\n",
    "# üéì KNOWLEDGE CHECK AND SUMMARY\n",
    "print(\"\\nüéì FINAL KNOWLEDGE CHECK - What You've Learned\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üìö Unity Catalog Mastery (30 min):\")\n",
    "print(\"   ‚úÖ Three-level namespace: catalog.schema.table\")\n",
    "print(\"   ‚úÖ Create catalogs and schemas for medallion architecture\")\n",
    "print(\"   ‚úÖ Understanding data governance and permissions\")\n",
    "print(\"   ‚úÖ Integration with solution accelerator structure\")\n",
    "\n",
    "print(f\"\\nüöÄ Delta Lake Expertise (45 min):\")\n",
    "print(\"   ‚úÖ Create Delta tables from sample data\")\n",
    "print(\"   ‚úÖ Time travel and versioning capabilities\")\n",
    "print(\"   ‚úÖ MERGE operations for upserts and data management\") \n",
    "print(\"   ‚úÖ ACID transactions and optimization features\")\n",
    "print(\"   ‚úÖ Schema evolution and data quality\")\n",
    "\n",
    "print(f\"\\nüîß Data Engineering Skills (30 min):\")\n",
    "print(\"   ‚úÖ Convert notebooks to automated jobs\")\n",
    "print(\"   ‚úÖ Set up workflows with dependencies\")\n",
    "print(\"   ‚úÖ External storage integration patterns\")\n",
    "print(\"   ‚úÖ Delta Live Tables for advanced ETL\")\n",
    "print(\"   ‚úÖ Production-ready pipeline design\")\n",
    "\n",
    "print(f\"\\nüéØ Solution Accelerator Ready:\")\n",
    "print(\"   ‚úÖ Technical foundation established\")\n",
    "print(\"   ‚úÖ Integration patterns understood\")\n",
    "print(\"   ‚úÖ Production-ready architecture knowledge\")\n",
    "print(\"   ‚úÖ Hands-on experience with enterprise features\")\n",
    "\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "print(\"   üéØ Target: 2 hours comprehensive learning\")\n",
    "print(\"   ‚úÖ Unity Catalog: 30 minutes\")\n",
    "print(\"   ‚úÖ Delta Lake: 45 minutes\") \n",
    "print(\"   ‚úÖ Data Engineering: 30 minutes\")\n",
    "print(\"   ‚úÖ Integration: 15 minutes\")\n",
    "print(\"   üéâ Total: 2 hours of intensive, practical learning!\")\n",
    "\n",
    "print(f\"\\nüöÄ You're Now Ready To:\")\n",
    "print(\"   ‚Ä¢ Build enterprise-grade data pipelines\")\n",
    "print(\"   ‚Ä¢ Implement your solution accelerator architecture\")\n",
    "print(\"   ‚Ä¢ Integrate Databricks with Microsoft Fabric\")\n",
    "print(\"   ‚Ä¢ Create production-ready automated workflows\")\n",
    "print(\"   ‚Ä¢ Apply best practices for data governance\")\n",
    "\n",
    "# Clean up demo data (optional)\n",
    "print(f\"\\nüóëÔ∏è Cleanup Options:\")\n",
    "print(\"   To remove demo table: DROP TABLE IF EXISTS customers_delta\")\n",
    "print(\"   To keep for practice: Table will remain for further exploration\")\n",
    "\n",
    "try:\n",
    "    table_exists = spark.sql(\"SHOW TABLES LIKE 'customers_delta'\").count() > 0\n",
    "    if table_exists:\n",
    "        print(\"   ‚úÖ Demo table 'customers_delta' is available for continued practice\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è Demo table was not created or is not visible\")\n",
    "except:\n",
    "    print(\"   ‚ÑπÔ∏è Unable to check table status\")\n",
    "\n",
    "print(f\"\\nüéä CONGRATULATIONS!\")\n",
    "print(\"You've completed the Azure Databricks 2-Hour Crash Course!\")\n",
    "print(\"Ready to build amazing enterprise data solutions! üöÄ\")\n",
    "</VSCode.Cell>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088137f3",
   "metadata": {},
   "source": [
    "# Azure Databricks 2-Hour Crash Course\n",
    "\n",
    "## üéØ Learning Objectives (2 Hours Total)\n",
    "- **30 min:** Unity Catalog fundamentals and three-level namespace\n",
    "- **45 min:** Delta Lake creation, time travel, and MERGE operations\n",
    "- **30 min:** Data Engineering basics (Jobs, Workflows, External Storage)\n",
    "- **15 min:** Integration patterns for your solution accelerator\n",
    "\n",
    "## üìö Prerequisites\n",
    "- Azure Databricks workspace access\n",
    "- Basic SQL and Python knowledge\n",
    "- Sample data available\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Unity Catalog Crash Course (30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèõÔ∏è PART 1A: UNITY CATALOG EXPLORATION (10 minutes)\n",
    "print(\"üèõÔ∏è UNITY CATALOG EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check current catalog context\n",
    "print(\"\\nüìç Current Catalog Context:\")\n",
    "print(f\"Current Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "print(f\"Current Schema: {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "\n",
    "# List all catalogs\n",
    "print(\"\\nüìö Available Catalogs:\")\n",
    "catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "for catalog in catalogs:\n",
    "    print(f\"  üìÅ {catalog.catalog}\")\n",
    "\n",
    "# List schemas in main catalog\n",
    "print(\"\\nüìã Schemas in 'main' catalog:\")\n",
    "try:\n",
    "    schemas = spark.sql(\"SHOW SCHEMAS IN CATALOG main\").collect()\n",
    "    for schema in schemas:\n",
    "        print(f\"  üìÇ main.{schema.databaseName}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Error: {e}\")\n",
    "    print(\"  üí° Unity Catalog may not be enabled or you may not have access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è PART 1B: CREATE YOUR OWN CATALOG STRUCTURE (10 minutes)\n",
    "print(\"\\nüèóÔ∏è CREATING CATALOG STRUCTURE FOR SOLUTION ACCELERATOR\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a catalog for your solution accelerator (if permissions allow)\n",
    "catalog_name = \"solution_accelerator\"\n",
    "schema_names = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "print(f\"\\nüìÅ Creating catalog: {catalog_name}\")\n",
    "try:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "    print(\"  ‚úÖ Catalog created/exists\")\n",
    "    \n",
    "    # Create schemas for medallion architecture\n",
    "    for schema in schema_names:\n",
    "        print(f\"\\nüìÇ Creating schema: {catalog_name}.{schema}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "        print(f\"  ‚úÖ Schema {catalog_name}.{schema} created/exists\")\n",
    "        \n",
    "    # Set working catalog\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    print(f\"\\n‚úÖ Now working in catalog: {catalog_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Cannot create catalog (permissions): {e}\")\n",
    "    print(\"  üí° Using 'main' catalog instead\")\n",
    "    catalog_name = \"main\"\n",
    "    \n",
    "    # Try to create schemas in main catalog\n",
    "    for schema in schema_names:\n",
    "        try:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "            print(f\"  ‚úÖ Schema {schema} created in main catalog\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not create schema {schema}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954574d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç PART 1C: THREE-LEVEL NAMESPACE DEMO (10 minutes)\n",
    "print(\"\\nüîç THREE-LEVEL NAMESPACE DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Demonstrate three-level namespace: catalog.schema.table\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "\n",
    "print(f\"üìç Three-Level Namespace Structure:\")\n",
    "print(f\"   Catalog: {current_catalog}\")\n",
    "print(f\"   Schema:  bronze, silver, gold\")\n",
    "print(f\"   Table:   customer, product, orders\")\n",
    "print(f\"   Full:    {current_catalog}.bronze.customer\")\n",
    "\n",
    "# Show current context\n",
    "print(f\"\\nüìã Current Working Context:\")\n",
    "print(f\"   Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "print(f\"   Schema:  {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "\n",
    "# List tables in current schema\n",
    "print(f\"\\nüìä Tables in current schema:\")\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    if tables:\n",
    "        for table in tables[:5]:  # Show first 5\n",
    "            print(f\"   üìã {table.tableName}\")\n",
    "    else:\n",
    "        print(\"   üì≠ No tables found (expected for new schemas)\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error listing tables: {e}\")\n",
    "\n",
    "print(\"\\nüí° Key Unity Catalog Concepts:\")\n",
    "print(\"   ‚Ä¢ Catalogs = Top-level containers (like databases)\")\n",
    "print(\"   ‚Ä¢ Schemas = Logical groupings within catalogs\")  \n",
    "print(\"   ‚Ä¢ Tables = Data assets within schemas\")\n",
    "print(\"   ‚Ä¢ Full path: catalog.schema.table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc3f32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Delta Lake Deep Dive (45 minutes)\n",
    "\n",
    "## What is Delta Lake?\n",
    "- **ACID transactions** on data lakes\n",
    "- **Time travel** - query historical versions\n",
    "- **Schema evolution** - safely modify table structure\n",
    "- **Optimizations** - Z-ordering, auto-compaction\n",
    "- **Merges/Upserts** - efficiently update data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ PART 2A: CREATE SAMPLE DATA AND DELTA TABLES (15 minutes)\n",
    "print(\"üöÄ CREATING SAMPLE DATA FOR DELTA LAKE DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample customer data\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample data\n",
    "customers_data = []\n",
    "for i in range(100):\n",
    "    customers_data.append({\n",
    "        'customer_id': i + 1,\n",
    "        'customer_name': f'Customer_{i+1:03d}',\n",
    "        'email': f'customer{i+1}@example.com',\n",
    "        'city': random.choice(['Seattle', 'Portland', 'San Francisco', 'Los Angeles', 'Denver']),\n",
    "        'registration_date': datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "        'status': random.choice(['Active', 'Inactive', 'Pending'])\n",
    "    })\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "customers_df = spark.createDataFrame(customers_data)\n",
    "\n",
    "print(\"üìä Sample Customer Data Created:\")\n",
    "customers_df.show(5)\n",
    "print(f\"   Total Records: {customers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a423b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è PART 2B: CREATE DELTA TABLE (10 minutes)\n",
    "print(\"\\nüèóÔ∏è CREATING DELTA TABLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set working schema\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "schema_name = \"bronze\" if current_catalog != \"main\" else \"default\"\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "    print(f\"‚úÖ Using schema: {current_catalog}.{schema_name}\")\n",
    "except:\n",
    "    print(f\"‚ö†Ô∏è Using default schema\")\n",
    "\n",
    "# Create Delta table\n",
    "table_name = \"customers_delta\"\n",
    "full_table_name = f\"{current_catalog}.{schema_name}.{table_name}\"\n",
    "\n",
    "print(f\"\\nüìã Creating Delta table: {full_table_name}\")\n",
    "\n",
    "# Write as Delta table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(\"‚úÖ Delta table created successfully!\")\n",
    "\n",
    "# Verify table creation\n",
    "print(f\"\\nüîç Table Information:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {table_name}\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚è∞ PART 2C: DELTA LAKE TIME TRAVEL (10 minutes)\n",
    "print(\"\\n‚è∞ DELTA LAKE TIME TRAVEL DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show initial version\n",
    "print(\"üìä Initial table state:\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").show()\n",
    "\n",
    "# Make some updates to create versions\n",
    "print(\"\\nüîÑ Creating new versions...\")\n",
    "\n",
    "# Version 1: Update some customers\n",
    "print(\"Version 1: Updating customer status...\")\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {table_name} \n",
    "    SET status = 'VIP' \n",
    "    WHERE customer_id <= 10\n",
    "\"\"\")\n",
    "\n",
    "# Version 2: Insert new customers  \n",
    "print(\"Version 2: Adding new customers...\")\n",
    "new_customers = spark.createDataFrame([\n",
    "    (101, 'Customer_101', 'customer101@example.com', 'Boston', datetime.now(), 'Active'),\n",
    "    (102, 'Customer_102', 'customer102@example.com', 'Chicago', datetime.now(), 'Active')\n",
    "], ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status'])\n",
    "\n",
    "new_customers.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "# Show table history\n",
    "print(\"\\nüìö Delta Table History:\")\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {table_name}\")\n",
    "history_df.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n",
    "\n",
    "# Time travel examples\n",
    "print(\"\\n‚è∞ Time Travel Examples:\")\n",
    "\n",
    "print(\"\\nüì∏ Version 0 (original):\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count, COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count FROM {table_name} VERSION AS OF 0\").show()\n",
    "\n",
    "print(\"\\nüì∏ Version 1 (after VIP update):\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count, COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count FROM {table_name} VERSION AS OF 1\").show()\n",
    "\n",
    "print(\"\\nüì∏ Current version:\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count, COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count FROM {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÄ PART 2D: DELTA MERGE OPERATIONS (10 minutes)\n",
    "print(\"\\nüîÄ DELTA MERGE OPERATIONS DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create updates dataset\n",
    "updates_data = [\n",
    "    (5, 'Customer_005_Updated', 'updated5@example.com', 'Updated_City', datetime.now(), 'Premium'),\n",
    "    (10, 'Customer_010_Updated', 'updated10@example.com', 'Updated_City', datetime.now(), 'Premium'),\n",
    "    (103, 'Customer_103_New', 'customer103@example.com', 'Miami', datetime.now(), 'Active')  # New customer\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates_data, \n",
    "    ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status'])\n",
    "\n",
    "print(\"üìä Updates to apply:\")\n",
    "updates_df.show()\n",
    "\n",
    "# Create temporary view for MERGE\n",
    "updates_df.createOrReplaceTempView(\"customer_updates\")\n",
    "\n",
    "# Perform MERGE operation\n",
    "print(\"\\nüîÄ Executing MERGE operation...\")\n",
    "merge_sql = f\"\"\"\n",
    "MERGE INTO {table_name} as target\n",
    "USING customer_updates as source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET \n",
    "        customer_name = source.customer_name,\n",
    "        email = source.email,\n",
    "        city = source.city,\n",
    "        status = source.status\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (customer_id, customer_name, email, city, registration_date, status)\n",
    "    VALUES (source.customer_id, source.customer_name, source.email, source.city, source.registration_date, source.status)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(merge_sql)\n",
    "print(\"‚úÖ MERGE operation completed!\")\n",
    "\n",
    "# Verify results\n",
    "print(\"\\nüìä Results after MERGE:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT customer_id, customer_name, city, status \n",
    "    FROM {table_name} \n",
    "    WHERE customer_id IN (5, 10, 103)\n",
    "    ORDER BY customer_id\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nüìà Total count after MERGE:\")\n",
    "spark.sql(f\"SELECT COUNT(*) as total_customers FROM {table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aff872",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Data Engineering Basics (30 minutes)\n",
    "\n",
    "## Key Concepts:\n",
    "- **Notebooks ‚Üí Jobs** (scheduled execution)\n",
    "- **Workflows** (multi-task orchestration)  \n",
    "- **External Storage** (Azure Data Lake integration)\n",
    "- **Delta Live Tables** (streaming ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba00996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß PART 3A: EXTERNAL STORAGE CONNECTION (10 minutes)\n",
    "print(\"üîß EXTERNAL STORAGE CONNECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check existing mounts\n",
    "print(\"üìé Current mounted storage:\")\n",
    "mounts = dbutils.fs.mounts()\n",
    "for mount in mounts:\n",
    "    print(f\"   üîó {mount.mountPoint} ‚Üí {mount.source}\")\n",
    "\n",
    "# Demo: How to mount Azure Data Lake (conceptual - requires credentials)\n",
    "print(f\"\\nüí° Mounting Azure Data Lake Storage (ADLS) - Conceptual Example:\")\n",
    "mount_example = '''\n",
    "# Mount ADLS Gen2 for your solution accelerator\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://container@storageaccount.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/solution-accelerator\",\n",
    "    extra_configs = {\n",
    "        \"fs.azure.account.auth.type.storageaccount.dfs.core.windows.net\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type.storageaccount.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id.storageaccount.dfs.core.windows.net\": \"application-id\",\n",
    "        \"fs.azure.account.oauth2.client.secret.storageaccount.dfs.core.windows.net\": \"service-credential-key\"\n",
    "    }\n",
    ")\n",
    "'''\n",
    "print(mount_example)\n",
    "\n",
    "# Show how to work with mounted storage\n",
    "print(\"\\nüìÅ Working with mounted storage:\")\n",
    "print(\"   ‚Ä¢ List files: dbutils.fs.ls('/mnt/solution-accelerator/')\")\n",
    "print(\"   ‚Ä¢ Read CSV: spark.read.csv('/mnt/solution-accelerator/data.csv')\")\n",
    "print(\"   ‚Ä¢ Write Delta: df.write.format('delta').save('/mnt/solution-accelerator/delta-table')\")\n",
    "\n",
    "# For your solution accelerator integration:\n",
    "print(f\"\\nüéØ For Your Solution Accelerator:\")\n",
    "print(\"   1. Mount Azure Data Lake where Fabric can access\")\n",
    "print(\"   2. Write Delta tables to mounted location\")  \n",
    "print(\"   3. Fabric reads from same Azure Data Lake location\")\n",
    "print(\"   4. Creates seamless Databricks ‚Üí Fabric pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bf10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä PART 3B: JOBS AND WORKFLOWS CONCEPTS (10 minutes)\n",
    "print(\"\\nüìä JOBS AND WORKFLOWS CONCEPTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üîÑ Converting Notebooks to Jobs:\")\n",
    "print(\"   1. Save your notebook (this one!)\")\n",
    "print(\"   2. Go to 'Workflows' in Databricks sidebar\")\n",
    "print(\"   3. Click 'Create Job'\")\n",
    "print(\"   4. Select this notebook as the task\")\n",
    "print(\"   5. Configure cluster and schedule\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Job Configuration Example for Your Solution Accelerator:\")\n",
    "job_config = '''\n",
    "Job Name: \"Solution Accelerator Data Pipeline\"\n",
    "Tasks:\n",
    "‚îú‚îÄ‚îÄ Task 1: \"Data Ingestion\" \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Notebook: /Users/your-email/Data_Ingestion.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Cluster: Job cluster (auto-terminating)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Libraries: Delta Lake, pandas\n",
    "‚îú‚îÄ‚îÄ Task 2: \"Data Transformation\" (depends on Task 1)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Notebook: /Users/your-email/Data_Transformation.ipynb  \n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Parameters: {\"source_table\": \"bronze.customers\"}\n",
    "‚îî‚îÄ‚îÄ Task 3: \"Data Quality Check\" (depends on Task 2)\n",
    "    ‚îú‚îÄ‚îÄ Notebook: /Users/your-email/Data_Quality.ipynb\n",
    "    ‚îî‚îÄ‚îÄ Alerts: Email on failure\n",
    "    \n",
    "Schedule: Daily at 2 AM\n",
    "Retry: 3 attempts with exponential backoff\n",
    "'''\n",
    "print(job_config)\n",
    "\n",
    "print(f\"\\nüìà Workflow Benefits:\")\n",
    "print(\"   ‚úÖ Automated execution\")\n",
    "print(\"   ‚úÖ Dependency management\") \n",
    "print(\"   ‚úÖ Error handling and retries\")\n",
    "print(\"   ‚úÖ Monitoring and alerts\")\n",
    "print(\"   ‚úÖ Parameter passing between tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåä PART 3C: DELTA LIVE TABLES OVERVIEW (10 minutes)\n",
    "print(\"\\nüåä DELTA LIVE TABLES (DLT) OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üí° What is Delta Live Tables?\")\n",
    "print(\"   ‚Ä¢ Declarative ETL framework\")\n",
    "print(\"   ‚Ä¢ Automatically manages dependencies\")\n",
    "print(\"   ‚Ä¢ Built-in data quality monitoring\")  \n",
    "print(\"   ‚Ä¢ Streaming and batch processing\")\n",
    "\n",
    "print(f\"\\nüìù DLT Example for Your Solution Accelerator:\")\n",
    "dlt_example = '''\n",
    "# Bronze Layer - Raw data ingestion\n",
    "@dlt.table(\n",
    "    comment=\"Raw customer data from source systems\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/customers\")\n",
    "        .load(\"/mnt/raw-data/customers/\")\n",
    "    )\n",
    "\n",
    "# Silver Layer - Cleaned and validated data  \n",
    "@dlt.table(\n",
    "    comment=\"Cleaned customer data with quality checks\"\n",
    ")\n",
    "@dlt.expect(\"valid_email\", \"email IS NOT NULL AND email RLIKE '^[^@]+@[^@]+\\\\\\\\.[^@]+$'\")\n",
    "@dlt.expect_or_drop(\"customer_id_not_null\", \"customer_id IS NOT NULL\")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        dlt.read(\"bronze_customers\")\n",
    "        .select(\"customer_id\", \"customer_name\", \"email\", \"city\", \"registration_date\")\n",
    "        .filter(\"customer_id IS NOT NULL\")\n",
    "    )\n",
    "\n",
    "# Gold Layer - Business-ready aggregated data\n",
    "@dlt.table(\n",
    "    comment=\"Customer metrics for business analytics\"\n",
    ")\n",
    "def gold_customer_metrics():\n",
    "    return (\n",
    "        dlt.read(\"silver_customers\")\n",
    "        .groupBy(\"city\")\n",
    "        .agg(\n",
    "            count(\"customer_id\").alias(\"customer_count\"),\n",
    "            max(\"registration_date\").alias(\"latest_registration\")\n",
    "        )\n",
    "    )\n",
    "'''\n",
    "print(dlt_example)\n",
    "\n",
    "print(f\"\\nüéØ DLT Benefits for Your Architecture:\")\n",
    "print(\"   ‚úÖ Automatic Bronze ‚Üí Silver ‚Üí Gold pipeline\")\n",
    "print(\"   ‚úÖ Built-in data quality monitoring\")\n",
    "print(\"   ‚úÖ Real-time streaming capability\")\n",
    "print(\"   ‚úÖ Automatic dependency resolution\")\n",
    "print(\"   ‚úÖ Lineage tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ee741",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Solution Accelerator Integration (15 minutes)\n",
    "\n",
    "## How This Applies to Your Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ PART 4: SOLUTION ACCELERATOR INTEGRATION PATTERNS\n",
    "print(\"üéØ SOLUTION ACCELERATOR INTEGRATION PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üèóÔ∏è Your Architecture Integration:\")\n",
    "print(\"   Databricks (Online Channel) ‚Üí Azure Data Lake ‚Üí Fabric Bronze ‚Üí Silver ‚Üí Gold\")\n",
    "\n",
    "print(f\"\\nüìã Implementation Roadmap:\")\n",
    "roadmap = '''\n",
    "Week 1-2: Databricks Setup\n",
    "‚îú‚îÄ‚îÄ Create Unity Catalog structure (solution_accelerator.bronze/silver/gold)\n",
    "‚îú‚îÄ‚îÄ Generate sample data in Delta format  \n",
    "‚îú‚îÄ‚îÄ Set up external storage mounts to Azure Data Lake\n",
    "‚îî‚îÄ‚îÄ Create basic ETL notebooks\n",
    "\n",
    "Week 3-4: Data Pipeline Development  \n",
    "‚îú‚îÄ‚îÄ Build Bronze ‚Üí Silver transformation jobs\n",
    "‚îú‚îÄ‚îÄ Implement data quality checks with Delta\n",
    "‚îú‚îÄ‚îÄ Set up automated workflows\n",
    "‚îî‚îÄ‚îÄ Create monitoring and alerting\n",
    "\n",
    "Week 5-6: Fabric Integration\n",
    "‚îú‚îÄ‚îÄ Configure Fabric to read from same Azure Data Lake\n",
    "‚îú‚îÄ‚îÄ Implement cross-channel data merging in Fabric Silver tier\n",
    "‚îú‚îÄ‚îÄ Build Gold tier analytics in Fabric\n",
    "‚îî‚îÄ‚îÄ Create Power BI dashboards\n",
    "\n",
    "Week 7-8: Production Readiness\n",
    "‚îú‚îÄ‚îÄ Implement Delta Live Tables for streaming\n",
    "‚îú‚îÄ‚îÄ Set up automated deployment pipelines  \n",
    "‚îú‚îÄ‚îÄ Add comprehensive monitoring\n",
    "‚îî‚îÄ‚îÄ Document integration patterns\n",
    "'''\n",
    "print(roadmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d18005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä PRACTICAL NEXT STEPS FOR YOUR PROJECT\n",
    "print(\"\\nüìä PRACTICAL NEXT STEPS FOR YOUR PROJECT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üîß Immediate Actions (Today):\")\n",
    "print(\"   1. ‚úÖ Run this crash course notebook\")\n",
    "print(\"   2. Create your sample data as Delta tables\")\n",
    "print(\"   3. Practice MERGE operations with your product/customer data\")\n",
    "print(\"   4. Set up basic Unity Catalog structure\")\n",
    "\n",
    "print(f\"\\nüöÄ This Week:\")\n",
    "print(\"   1. Import your existing product generation notebook\")\n",
    "print(\"   2. Convert generated data to Delta format\")  \n",
    "print(\"   3. Create customer and order generation notebooks\")\n",
    "print(\"   4. Build basic Bronze ‚Üí Silver transformation\")\n",
    "\n",
    "print(f\"\\nüìà Next Week:\")\n",
    "print(\"   1. Set up Azure Data Lake mount point\")\n",
    "print(\"   2. Create automated jobs for data processing\")\n",
    "print(\"   3. Test Fabric integration with Delta tables\")\n",
    "print(\"   4. Build monitoring and quality checks\")\n",
    "\n",
    "print(f\"\\nüí° Key Success Patterns:\")\n",
    "patterns = '''\n",
    "‚úÖ Unity Catalog for governance: catalog.schema.table naming\n",
    "‚úÖ Delta Lake for reliability: ACID transactions, time travel\n",
    "‚úÖ External mounts for integration: Databricks ‚Üî Fabric via ADLS\n",
    "‚úÖ Jobs/Workflows for automation: scheduled, monitored pipelines\n",
    "‚úÖ DLT for advanced ETL: streaming, quality, lineage\n",
    "'''\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéì KNOWLEDGE CHECK AND SUMMARY\n",
    "print(\"\\nüéì KNOWLEDGE CHECK - What You've Learned\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üìö Unity Catalog (30 min):\")\n",
    "print(\"   ‚úÖ Three-level namespace: catalog.schema.table\")\n",
    "print(\"   ‚úÖ Create catalogs and schemas for medallion architecture\")\n",
    "print(\"   ‚úÖ Understanding data governance and permissions\")\n",
    "\n",
    "print(f\"\\nüöÄ Delta Lake (45 min):\")\n",
    "print(\"   ‚úÖ Create Delta tables from sample data\")\n",
    "print(\"   ‚úÖ Time travel and versioning capabilities\")\n",
    "print(\"   ‚úÖ MERGE operations for upserts\") \n",
    "print(\"   ‚úÖ ACID transactions and optimization\")\n",
    "\n",
    "print(f\"\\nüîß Data Engineering (30 min):\")\n",
    "print(\"   ‚úÖ Convert notebooks to automated jobs\")\n",
    "print(\"   ‚úÖ Set up workflows with dependencies\")\n",
    "print(\"   ‚úÖ External storage integration patterns\")\n",
    "print(\"   ‚úÖ Delta Live Tables for advanced ETL\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for Your Solution Accelerator:\")\n",
    "print(\"   ‚úÖ Technical foundation established\")\n",
    "print(\"   ‚úÖ Integration patterns understood\")\n",
    "print(\"   ‚úÖ Production-ready architecture knowledge\")\n",
    "print(\"   ‚úÖ Hands-on experience with key features\")\n",
    "\n",
    "print(f\"\\nüöÄ Total Learning Time: 2 hours\")\n",
    "print(\"üéâ You're now ready to build your enterprise solution accelerator!\")\n",
    "\n",
    "# Clean up demo data (optional)\n",
    "try:\n",
    "    print(f\"\\nüóëÔ∏è Cleaning up demo table (optional):\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(\"   ‚úÖ Demo table removed\")\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è Could not remove demo table (may not have permissions)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
