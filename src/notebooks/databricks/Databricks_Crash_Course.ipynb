{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Databricks 2-Hour Crash Course\n",
    "\n",
    "## ğŸ¯ Learning Objectives (2 Hours Total)\n",
    "- **30 min:** Unity Catalog fundamentals and three-level namespace\n",
    "- **45 min:** Delta Lake creation, time travel, and MERGE operations\n",
    "- **30 min:** Data Engineering basics (Jobs, Workflows, External Storage)\n",
    "- **15 min:** Integration patterns for your solution accelerator\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "- Azure Databricks workspace access\n",
    "- Basic SQL and Python knowledge\n",
    "- Sample data available\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Unity Catalog Crash Course (30 minutes)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-354e3eea\" language=\"python\">\n",
    "# ğŸ›ï¸ PART 1A: UNITY CATALOG EXPLORATION (10 minutes)\n",
    "print(\"ğŸ›ï¸ UNITY CATALOG EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check current catalog context\n",
    "print(\"\\nğŸ“ Current Catalog Context:\")\n",
    "try:\n",
    "    print(f\"Current Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "    print(f\"Current Schema: {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error getting current context: {e}\")\n",
    "    print(\"ğŸ’¡ Unity Catalog may not be enabled\")\n",
    "\n",
    "# List all catalogs\n",
    "print(\"\\nğŸ“š Available Catalogs:\")\n",
    "try:\n",
    "    catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "    for catalog in catalogs:\n",
    "        print(f\"  ğŸ“ {catalog.catalog}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error listing catalogs: {e}\")\n",
    "\n",
    "# List schemas in main catalog\n",
    "print(\"\\nğŸ“‹ Schemas in 'main' catalog:\")\n",
    "try:\n",
    "    schemas = spark.sql(\"SHOW SCHEMAS IN CATALOG main\").collect()\n",
    "    for schema in schemas:\n",
    "        print(f\"  ğŸ“‚ main.{schema.databaseName}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"ğŸ’¡ Trying default schema listing...\")\n",
    "    try:\n",
    "        schemas = spark.sql(\"SHOW SCHEMAS\").collect()\n",
    "        for schema in schemas:\n",
    "            print(f\"  ğŸ“‚ {schema.databaseName}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Error with default schemas: {e2}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-8fa29ab2\" language=\"python\">\n",
    "# ğŸ—ï¸ PART 1B: CREATE YOUR OWN CATALOG STRUCTURE (10 minutes)\n",
    "print(\"\\nğŸ—ï¸ CREATING CATALOG STRUCTURE FOR SOLUTION ACCELERATOR\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a catalog for your solution accelerator (if permissions allow)\n",
    "catalog_name = \"solution_accelerator\"\n",
    "schema_names = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "print(f\"\\nğŸ“ Attempting to create catalog: {catalog_name}\")\n",
    "try:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "    print(\"  âœ… Catalog created/exists\")\n",
    "    current_catalog = catalog_name\n",
    "    \n",
    "    # Create schemas for medallion architecture\n",
    "    for schema in schema_names:\n",
    "        print(f\"\\nğŸ“‚ Creating schema: {catalog_name}.{schema}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "        print(f\"  âœ… Schema {catalog_name}.{schema} created/exists\")\n",
    "        \n",
    "    # Set working catalog\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    print(f\"\\nâœ… Now working in catalog: {catalog_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Cannot create catalog (permissions): {e}\")\n",
    "    print(\"  ğŸ’¡ Using 'main' catalog instead\")\n",
    "    current_catalog = \"main\"\n",
    "    \n",
    "    # Try to create schemas in main catalog or use default\n",
    "    for schema in schema_names:\n",
    "        try:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "            print(f\"  âœ… Schema {schema} created in main catalog\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Could not create schema {schema}: {e}\")\n",
    "            print(f\"     Will use 'default' schema\")\n",
    "\n",
    "print(f\"\\nğŸ“ Working Context:\")\n",
    "try:\n",
    "    print(f\"   Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "    print(f\"   Schema:  {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "except:\n",
    "    print(f\"   Using default context\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-c8de4372\" language=\"python\">\n",
    "# ğŸ” PART 1C: THREE-LEVEL NAMESPACE DEMO (10 minutes)\n",
    "print(\"\\nğŸ” THREE-LEVEL NAMESPACE DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Demonstrate three-level namespace: catalog.schema.table\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "except:\n",
    "    current_catalog = \"main\"\n",
    "    current_schema = \"default\"\n",
    "\n",
    "print(f\"ğŸ“ Three-Level Namespace Structure:\")\n",
    "print(f\"   Catalog: {current_catalog}\")\n",
    "print(f\"   Schema:  bronze, silver, gold (or {current_schema})\")\n",
    "print(f\"   Table:   customer, product, orders\")\n",
    "print(f\"   Full:    {current_catalog}.{current_schema}.customer\")\n",
    "\n",
    "# List tables in current schema\n",
    "print(f\"\\nğŸ“Š Tables in current schema:\")\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    if tables:\n",
    "        for table in tables[:5]:  # Show first 5\n",
    "            print(f\"   ğŸ“‹ {table.tableName}\")\n",
    "    else:\n",
    "        print(\"   ğŸ“­ No tables found (expected for new schemas)\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error listing tables: {e}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Unity Catalog Concepts:\")\n",
    "print(\"   â€¢ Catalogs = Top-level containers (like databases)\")\n",
    "print(\"   â€¢ Schemas = Logical groupings within catalogs\")  \n",
    "print(\"   â€¢ Tables = Data assets within schemas\")\n",
    "print(\"   â€¢ Full path: catalog.schema.table\")\n",
    "print(\"   â€¢ Unity Catalog provides governance, lineage, and access control\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-c3721e4c\" language=\"markdown\">\n",
    "---\n",
    "\n",
    "# Part 2: Delta Lake Deep Dive (45 minutes)\n",
    "\n",
    "## What is Delta Lake?\n",
    "- **ACID transactions** on data lakes\n",
    "- **Time travel** - query historical versions\n",
    "- **Schema evolution** - safely modify table structure\n",
    "- **Optimizations** - Z-ordering, auto-compaction\n",
    "- **Merges/Upserts** - efficiently update data\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-ce8a9ba3\" language=\"python\">\n",
    "# ğŸš€ PART 2A: CREATE SAMPLE DATA AND DELTA TABLES (15 minutes)\n",
    "print(\"ğŸš€ CREATING SAMPLE DATA FOR DELTA LAKE DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample customer data\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set seed for reproducible results\n",
    "random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "customers_data = []\n",
    "cities = ['Seattle', 'Portland', 'San Francisco', 'Los Angeles', 'Denver', 'Austin', 'Chicago', 'Boston']\n",
    "statuses = ['Active', 'Inactive', 'Pending', 'VIP']\n",
    "\n",
    "for i in range(100):\n",
    "    customers_data.append({\n",
    "        'customer_id': i + 1,\n",
    "        'customer_name': f'Customer_{i+1:03d}',\n",
    "        'email': f'customer{i+1}@example.com',\n",
    "        'city': random.choice(cities),\n",
    "        'registration_date': datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "        'status': random.choice(statuses),\n",
    "        'annual_spend': round(random.uniform(1000, 10000), 2)\n",
    "    })\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "try:\n",
    "    customers_df = spark.createDataFrame(customers_data)\n",
    "    print(\"ğŸ“Š Sample Customer Data Created:\")\n",
    "    customers_df.show(5)\n",
    "    print(f\"   Total Records: {customers_df.count()}\")\n",
    "    print(\"   âœ… Sample data generation successful\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating sample data: {e}\")\n",
    "    # Create a simple fallback dataset\n",
    "    print(\"Creating fallback dataset...\")\n",
    "    simple_data = [(1, \"Customer_001\", \"test@example.com\", \"Seattle\", datetime.now(), \"Active\", 5000.0)]\n",
    "    customers_df = spark.createDataFrame(simple_data, \n",
    "        ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status', 'annual_spend'])\n",
    "    print(\"âœ… Fallback data created\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-a3299514\" language=\"python\">\n",
    "# ğŸ—ï¸ PART 2B: CREATE DELTA TABLE (10 minutes)\n",
    "print(\"\\nğŸ—ï¸ CREATING DELTA TABLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set working schema\n",
    "try:\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "    \n",
    "    # Try to use bronze schema if it exists\n",
    "    try:\n",
    "        spark.sql(\"USE SCHEMA bronze\")\n",
    "        current_schema = \"bronze\"\n",
    "        print(f\"âœ… Using schema: {current_catalog}.bronze\")\n",
    "    except:\n",
    "        print(f\"âš ï¸ Using current schema: {current_catalog}.{current_schema}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Using default context: {e}\")\n",
    "    current_catalog = \"main\"\n",
    "    current_schema = \"default\"\n",
    "\n",
    "# Create Delta table\n",
    "table_name = \"customers_delta\"\n",
    "\n",
    "print(f\"\\nğŸ“‹ Creating Delta table: {table_name}\")\n",
    "print(f\"   Full path: {current_catalog}.{current_schema}.{table_name}\")\n",
    "\n",
    "try:\n",
    "    # Write as Delta table\n",
    "    customers_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "    print(\"âœ… Delta table created successfully!\")\n",
    "\n",
    "    # Verify table creation\n",
    "    print(f\"\\nğŸ” Table Information:\")\n",
    "    table_info = spark.sql(f\"DESCRIBE EXTENDED {table_name}\")\n",
    "    print(f\"   Rows: {customers_df.count()}\")\n",
    "    print(f\"   Columns: {len(customers_df.columns)}\")\n",
    "    print(f\"   Format: Delta Lake\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating Delta table: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to permissions or catalog configuration\")\n",
    "    \n",
    "    # Try alternative approach - create in temp location\n",
    "    try:\n",
    "        print(\"\\nğŸ”„ Trying alternative approach...\")\n",
    "        temp_path = \"/tmp/customers_delta\"\n",
    "        customers_df.write.format(\"delta\").mode(\"overwrite\").save(temp_path)\n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{temp_path}'\")\n",
    "        print(\"âœ… Delta table created in temp location\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Alternative approach failed: {e2}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-3d4541d4\" language=\"python\">\n",
    "# â° PART 2C: DELTA LAKE TIME TRAVEL (10 minutes)\n",
    "print(\"\\nâ° DELTA LAKE TIME TRAVEL DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Show initial version\n",
    "    print(\"ğŸ“Š Initial table state:\")\n",
    "    initial_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0][0]\n",
    "    print(f\"   Records: {initial_count}\")\n",
    "\n",
    "    # Make some updates to create versions\n",
    "    print(\"\\nğŸ”„ Creating new versions for time travel demo...\")\n",
    "\n",
    "    # Version 1: Update some customers to VIP status\n",
    "    print(\"Version 1: Updating customer status to VIP...\")\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {table_name} \n",
    "        SET status = 'VIP', annual_spend = annual_spend * 1.5\n",
    "        WHERE customer_id <= 10\n",
    "    \"\"\")\n",
    "    \n",
    "    vip_count_v1 = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name} WHERE status = 'VIP'\").collect()[0][0]\n",
    "    print(f\"   VIP customers after update: {vip_count_v1}\")\n",
    "\n",
    "    # Version 2: Insert new customers  \n",
    "    print(\"\\nVersion 2: Adding new customers...\")\n",
    "    new_customers = spark.createDataFrame([\n",
    "        (101, 'Customer_101', 'customer101@example.com', 'Boston', datetime.now(), 'Active', 7500.0),\n",
    "        (102, 'Customer_102', 'customer102@example.com', 'Chicago', datetime.now(), 'Active', 8200.0)\n",
    "    ], ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status', 'annual_spend'])\n",
    "\n",
    "    new_customers.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "    \n",
    "    final_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0][0]\n",
    "    print(f\"   Total customers after insert: {final_count}\")\n",
    "\n",
    "    # Show table history\n",
    "    print(\"\\nğŸ“š Delta Table History:\")\n",
    "    try:\n",
    "        history_df = spark.sql(f\"DESCRIBE HISTORY {table_name}\")\n",
    "        history_df.select(\"version\", \"timestamp\", \"operation\").show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not show history: {e}\")\n",
    "\n",
    "    # Time travel examples\n",
    "    print(\"\\nâ° Time Travel Examples:\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\nğŸ“¸ Version 0 (original):\")\n",
    "        v0_result = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as total_count, \n",
    "                   COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count \n",
    "            FROM {table_name} VERSION AS OF 0\n",
    "        \"\"\").collect()[0]\n",
    "        print(f\"   Total: {v0_result[0]}, VIP: {v0_result[1]}\")\n",
    "\n",
    "        print(\"\\nğŸ“¸ Version 1 (after VIP updates):\")\n",
    "        v1_result = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as total_count, \n",
    "                   COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count \n",
    "            FROM {table_name} VERSION AS OF 1\n",
    "        \"\"\").collect()[0]\n",
    "        print(f\"   Total: {v1_result[0]}, VIP: {v1_result[1]}\")\n",
    "\n",
    "        print(\"\\nğŸ“¸ Current version:\")\n",
    "        current_result = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as total_count, \n",
    "                   COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count \n",
    "            FROM {table_name}\n",
    "        \"\"\").collect()[0]\n",
    "        print(f\"   Total: {current_result[0]}, VIP: {current_result[1]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Time travel queries failed: {e}\")\n",
    "        print(\"ğŸ’¡ Time travel might not be available in this environment\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in time travel demo: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure the Delta table was created successfully first\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-d63f8880\" language=\"python\">\n",
    "# ğŸ”€ PART 2D: DELTA MERGE OPERATIONS (10 minutes)\n",
    "print(\"\\nğŸ”€ DELTA MERGE OPERATIONS DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Create updates dataset\n",
    "    updates_data = [\n",
    "        (5, 'Customer_005_Updated', 'updated5@example.com', 'Updated_City', datetime.now(), 'Premium', 12000.0),\n",
    "        (10, 'Customer_010_Updated', 'updated10@example.com', 'Updated_City', datetime.now(), 'Premium', 15000.0),\n",
    "        (103, 'Customer_103_New', 'customer103@example.com', 'Miami', datetime.now(), 'Active', 9500.0)  # New customer\n",
    "    ]\n",
    "\n",
    "    updates_df = spark.createDataFrame(updates_data, \n",
    "        ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status', 'annual_spend'])\n",
    "\n",
    "    print(\"ğŸ“Š Updates to apply:\")\n",
    "    updates_df.show()\n",
    "\n",
    "    # Create temporary view for MERGE\n",
    "    updates_df.createOrReplaceTempView(\"customer_updates\")\n",
    "\n",
    "    # Perform MERGE operation\n",
    "    print(\"\\nğŸ”€ Executing MERGE operation...\")\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {table_name} as target\n",
    "    USING customer_updates as source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET \n",
    "            customer_name = source.customer_name,\n",
    "            email = source.email,\n",
    "            city = source.city,\n",
    "            status = source.status,\n",
    "            annual_spend = source.annual_spend\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, customer_name, email, city, registration_date, status, annual_spend)\n",
    "        VALUES (source.customer_id, source.customer_name, source.email, source.city, source.registration_date, source.status, source.annual_spend)\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(merge_sql)\n",
    "    print(\"âœ… MERGE operation completed!\")\n",
    "\n",
    "    # Verify results\n",
    "    print(\"\\nğŸ“Š Results after MERGE:\")\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "        SELECT customer_id, customer_name, city, status, annual_spend\n",
    "        FROM {table_name} \n",
    "        WHERE customer_id IN (5, 10, 103)\n",
    "        ORDER BY customer_id\n",
    "    \"\"\")\n",
    "    result_df.show()\n",
    "\n",
    "    print(\"\\nğŸ“ˆ Total count after MERGE:\")\n",
    "    final_count = spark.sql(f\"SELECT COUNT(*) as total_customers FROM {table_name}\").collect()[0][0]\n",
    "    print(f\"   Total customers: {final_count}\")\n",
    "    \n",
    "    print(\"\\nğŸ’° Premium customers summary:\")\n",
    "    premium_summary = spark.sql(f\"\"\"\n",
    "        SELECT status, COUNT(*) as count, AVG(annual_spend) as avg_spend\n",
    "        FROM {table_name}\n",
    "        WHERE status IN ('Premium', 'VIP')\n",
    "        GROUP BY status\n",
    "        ORDER BY avg_spend DESC\n",
    "    \"\"\")\n",
    "    premium_summary.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in MERGE operation: {e}\")\n",
    "    print(\"ğŸ’¡ MERGE operations require Delta Lake table format\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-7fddcbf9\" language=\"markdown\">\n",
    "---\n",
    "\n",
    "# Part 3: Data Engineering Basics (30 minutes)\n",
    "\n",
    "## Key Concepts:\n",
    "- **Notebooks â†’ Jobs** (scheduled execution)\n",
    "- **Workflows** (multi-task orchestration)  \n",
    "- **External Storage** (Azure Data Lake integration)\n",
    "- **Delta Live Tables** (streaming ETL)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-10a1372e\" language=\"python\">\n",
    "# ğŸ”§ PART 3A: EXTERNAL STORAGE CONNECTION (10 minutes)\n",
    "print(\"ğŸ”§ EXTERNAL STORAGE CONNECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check existing mounts\n",
    "print(\"ğŸ“ Current mounted storage:\")\n",
    "try:\n",
    "    mounts = dbutils.fs.mounts()\n",
    "    if mounts:\n",
    "        for mount in mounts:\n",
    "            print(f\"   ğŸ”— {mount.mountPoint} â†’ {mount.source}\")\n",
    "    else:\n",
    "        print(\"   ğŸ“­ No external storage currently mounted\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error checking mounts: {e}\")\n",
    "\n",
    "# Demo: How to mount Azure Data Lake (conceptual - requires credentials)\n",
    "print(f\"\\nğŸ’¡ Mounting Azure Data Lake Storage (ADLS) - Conceptual Example:\")\n",
    "mount_example = '''\n",
    "# Mount ADLS Gen2 for your solution accelerator\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://container@storageaccount.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/solution-accelerator\",\n",
    "    extra_configs = {\n",
    "        \"fs.azure.account.auth.type.storageaccount.dfs.core.windows.net\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type.storageaccount.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id.storageaccount.dfs.core.windows.net\": \"application-id\",\n",
    "        \"fs.azure.account.oauth2.client.secret.storageaccount.dfs.core.windows.net\": \"service-credential-key\"\n",
    "    }\n",
    ")\n",
    "'''\n",
    "print(mount_example)\n",
    "\n",
    "# Show how to work with mounted storage\n",
    "print(\"\\nğŸ“ Working with mounted storage:\")\n",
    "print(\"   â€¢ List files: dbutils.fs.ls('/mnt/solution-accelerator/')\")\n",
    "print(\"   â€¢ Read CSV: spark.read.csv('/mnt/solution-accelerator/data.csv')\")\n",
    "print(\"   â€¢ Write Delta: df.write.format('delta').save('/mnt/solution-accelerator/delta-table')\")\n",
    "\n",
    "# For your solution accelerator integration:\n",
    "print(f\"\\nğŸ¯ For Your Solution Accelerator:\")\n",
    "print(\"   1. Mount Azure Data Lake where Fabric can access\")\n",
    "print(\"   2. Write Delta tables to mounted location\")  \n",
    "print(\"   3. Fabric reads from same Azure Data Lake location\")\n",
    "print(\"   4. Creates seamless Databricks â†’ Fabric pipeline\")\n",
    "\n",
    "# Demonstrate file operations that work without mounts\n",
    "print(f\"\\nğŸ› ï¸ Basic file operations (no mount required):\")\n",
    "try:\n",
    "    # Create a sample file in DBFS\n",
    "    sample_path = \"/tmp/sample_data.csv\"\n",
    "    sample_content = \"id,name,value\\n1,Sample,100\\n2,Test,200\"\n",
    "    dbutils.fs.put(sample_path, sample_content, overwrite=True)\n",
    "    print(f\"   âœ… Created sample file: {sample_path}\")\n",
    "    \n",
    "    # Read it back as DataFrame\n",
    "    df_sample = spark.read.csv(sample_path, header=True, inferSchema=True)\n",
    "    df_sample.show()\n",
    "    \n",
    "    # Clean up\n",
    "    dbutils.fs.rm(sample_path)\n",
    "    print(\"   âœ… Sample file removed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error in file operations: {e}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-26a8e5f7\" language=\"python\">\n",
    "# ğŸ“Š PART 3B: JOBS AND WORKFLOWS CONCEPTS (10 minutes)\n",
    "print(\"\\nğŸ“Š JOBS AND WORKFLOWS CONCEPTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ”„ Converting Notebooks to Jobs:\")\n",
    "print(\"   1. Save your notebook (this one!)\")\n",
    "print(\"   2. Go to 'Workflows' in Databricks sidebar\")\n",
    "print(\"   3. Click 'Create Job'\")\n",
    "print(\"   4. Select this notebook as the task\")\n",
    "print(\"   5. Configure cluster and schedule\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ Job Configuration Example for Your Solution Accelerator:\")\n",
    "job_config = '''\n",
    "Job Name: \"Solution Accelerator Data Pipeline\"\n",
    "Tasks:\n",
    "â”œâ”€â”€ Task 1: \"Data Ingestion\" \n",
    "â”‚   â”œâ”€â”€ Notebook: /Users/your-email/Data_Ingestion.ipynb\n",
    "â”‚   â”œâ”€â”€ Cluster: Job cluster (auto-terminating)\n",
    "â”‚   â”œâ”€â”€ Libraries: Delta Lake, pandas\n",
    "â”‚   â””â”€â”€ Schedule: Daily at 2 AM\n",
    "â”œâ”€â”€ Task 2: \"Data Transformation\" (depends on Task 1)\n",
    "â”‚   â”œâ”€â”€ Notebook: /Users/your-email/Data_Transformation.ipynb  \n",
    "â”‚   â”œâ”€â”€ Parameters: {\"source_table\": \"bronze.customers\"}\n",
    "â”‚   â””â”€â”€ Timeout: 30 minutes\n",
    "â””â”€â”€ Task 3: \"Data Quality Check\" (depends on Task 2)\n",
    "    â”œâ”€â”€ Notebook: /Users/your-email/Data_Quality.ipynb\n",
    "    â”œâ”€â”€ Alerts: Email on failure\n",
    "    â””â”€â”€ Retry: 3 attempts with exponential backoff\n",
    "'''\n",
    "print(job_config)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Workflow Benefits:\")\n",
    "print(\"   âœ… Automated execution (no manual intervention)\")\n",
    "print(\"   âœ… Dependency management (tasks run in order)\") \n",
    "print(\"   âœ… Error handling and retries (resilient)\")\n",
    "print(\"   âœ… Monitoring and alerts (proactive)\")\n",
    "print(\"   âœ… Parameter passing between tasks (flexible)\")\n",
    "print(\"   âœ… Cost optimization (clusters auto-terminate)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Real-world Use Cases:\")\n",
    "print(\"   â€¢ Daily data ingestion from source systems\")\n",
    "print(\"   â€¢ ETL pipelines with quality checks\")\n",
    "print(\"   â€¢ Model training and deployment\")\n",
    "print(\"   â€¢ Report generation and distribution\")\n",
    "print(\"   â€¢ Data validation and monitoring\")\n",
    "\n",
    "# Demonstrate job parameters concept\n",
    "print(f\"\\nğŸ“ Job Parameters Example:\")\n",
    "print(\"   In this notebook, you could access job parameters like:\")\n",
    "print(\"   source_date = dbutils.widgets.get('source_date')\")\n",
    "print(\"   table_name = dbutils.widgets.get('table_name')\")\n",
    "print(\"   environment = dbutils.widgets.get('environment')\")\n",
    "\n",
    "# Show current notebook info that would be useful for jobs\n",
    "try:\n",
    "    context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "    print(f\"\\nğŸ“ Current Notebook Context:\")\n",
    "    print(f\"   User: {context.tags().get('user').get()}\")\n",
    "    print(f\"   Notebook: {context.notebookPath().get()}\")\n",
    "except:\n",
    "    print(f\"\\nğŸ“ Notebook context not available in this environment\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-a0f12e84\" language=\"python\">\n",
    "# ğŸŒŠ PART 3C: DELTA LIVE TABLES OVERVIEW (10 minutes)\n",
    "print(\"\\nğŸŒŠ DELTA LIVE TABLES (DLT) OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ’¡ What is Delta Live Tables?\")\n",
    "print(\"   â€¢ Declarative ETL framework\")\n",
    "print(\"   â€¢ Automatically manages dependencies\")\n",
    "print(\"   â€¢ Built-in data quality monitoring\")  \n",
    "print(\"   â€¢ Streaming and batch processing\")\n",
    "print(\"   â€¢ Automatic schema evolution\")\n",
    "print(\"   â€¢ Cost optimization with auto-scaling\")\n",
    "\n",
    "print(f\"\\nğŸ“ DLT Example for Your Solution Accelerator:\")\n",
    "dlt_example = '''\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Bronze Layer - Raw data ingestion\n",
    "@dlt.table(\n",
    "    comment=\"Raw customer data from source systems\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/customers\")\n",
    "        .load(\"/mnt/raw-data/customers/\")\n",
    "    )\n",
    "\n",
    "# Silver Layer - Cleaned and validated data  \n",
    "@dlt.table(\n",
    "    comment=\"Cleaned customer data with quality checks\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "@dlt.expect(\"valid_email\", \"email IS NOT NULL AND email RLIKE '^[^@]+@[^@]+\\\\\\\\.[^@]+$'\")\n",
    "@dlt.expect_or_drop(\"customer_id_not_null\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_fail(\"reasonable_annual_spend\", \"annual_spend >= 0 AND annual_spend <= 100000\")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        dlt.read(\"bronze_customers\")\n",
    "        .select(\n",
    "            col(\"customer_id\").cast(\"int\"),\n",
    "            col(\"customer_name\"), \n",
    "            col(\"email\"), \n",
    "            col(\"city\"), \n",
    "            col(\"registration_date\").cast(\"date\"),\n",
    "            col(\"status\"),\n",
    "            col(\"annual_spend\").cast(\"double\")\n",
    "        )\n",
    "        .filter(\"customer_id IS NOT NULL\")\n",
    "        .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# Gold Layer - Business-ready aggregated data\n",
    "@dlt.table(\n",
    "    comment=\"Customer metrics for business analytics\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def gold_customer_metrics():\n",
    "    return (\n",
    "        dlt.read(\"silver_customers\")\n",
    "        .groupBy(\"city\", \"status\")\n",
    "        .agg(\n",
    "            count(\"customer_id\").alias(\"customer_count\"),\n",
    "            avg(\"annual_spend\").alias(\"avg_annual_spend\"),\n",
    "            max(\"registration_date\").alias(\"latest_registration\"),\n",
    "            sum(\"annual_spend\").alias(\"total_revenue\")\n",
    "        )\n",
    "        .withColumn(\"metric_date\", current_date())\n",
    "    )\n",
    "'''\n",
    "print(dlt_example)\n",
    "\n",
    "print(f\"\\nğŸ¯ DLT Benefits for Your Architecture:\")\n",
    "print(\"   âœ… Automatic Bronze â†’ Silver â†’ Gold pipeline\")\n",
    "print(\"   âœ… Built-in data quality monitoring and alerts\")\n",
    "print(\"   âœ… Real-time streaming capability\")\n",
    "print(\"   âœ… Automatic dependency resolution\")\n",
    "print(\"   âœ… Complete lineage tracking\")\n",
    "print(\"   âœ… Schema evolution without breaking pipelines\")\n",
    "print(\"   âœ… Automatic optimization and performance tuning\")\n",
    "\n",
    "print(f\"\\nğŸ—ï¸ DLT vs Traditional ETL:\")\n",
    "comparison = '''\n",
    "Traditional ETL:                    Delta Live Tables:\n",
    "â”œâ”€â”€ Manual dependency management    â”œâ”€â”€ Automatic dependency resolution\n",
    "â”œâ”€â”€ Custom error handling          â”œâ”€â”€ Built-in quality expectations  \n",
    "â”œâ”€â”€ Manual schema management       â”œâ”€â”€ Automatic schema evolution\n",
    "â”œâ”€â”€ Complex orchestration          â”œâ”€â”€ Declarative pipeline definition\n",
    "â”œâ”€â”€ Manual monitoring              â”œâ”€â”€ Built-in observability\n",
    "â””â”€â”€ Infrastructure management      â””â”€â”€ Serverless execution\n",
    "'''\n",
    "print(comparison)\n",
    "\n",
    "print(f\"\\nğŸš€ When to Use DLT:\")\n",
    "print(\"   âœ… Complex ETL pipelines with multiple stages\")\n",
    "print(\"   âœ… Streaming data processing requirements\")\n",
    "print(\"   âœ… Strict data quality requirements\")\n",
    "print(\"   âœ… Need for automatic dependency management\")\n",
    "print(\"   âœ… Production-grade data pipelines\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-b30275c0\" language=\"markdown\">\n",
    "---\n",
    "\n",
    "# Part 4: Solution Accelerator Integration (15 minutes)\n",
    "\n",
    "## How This Applies to Your Project\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-8636aedc\" language=\"python\">\n",
    "# ğŸ¯ PART 4: SOLUTION ACCELERATOR INTEGRATION PATTERNS\n",
    "print(\"ğŸ¯ SOLUTION ACCELERATOR INTEGRATION PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ—ï¸ Your Architecture Integration:\")\n",
    "print(\"   Azure Databricks (Online Channel) â†’ Azure Data Lake â†’ Fabric Bronze â†’ Silver â†’ Gold\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Implementation Roadmap:\")\n",
    "roadmap = '''\n",
    "Week 1-2: Databricks Setup\n",
    "â”œâ”€â”€ âœ… Create Unity Catalog structure (solution_accelerator.bronze/silver/gold)\n",
    "â”œâ”€â”€ âœ… Generate sample data in Delta format  \n",
    "â”œâ”€â”€ ğŸ”„ Set up external storage mounts to Azure Data Lake\n",
    "â””â”€â”€ ğŸ”„ Create basic ETL notebooks\n",
    "\n",
    "Week 3-4: Data Pipeline Development  \n",
    "â”œâ”€â”€ ğŸ”„ Build Bronze â†’ Silver transformation jobs\n",
    "â”œâ”€â”€ ğŸ”„ Implement data quality checks with Delta\n",
    "â”œâ”€â”€ ğŸ”„ Set up automated workflows\n",
    "â””â”€â”€ ğŸ”„ Create monitoring and alerting\n",
    "\n",
    "Week 5-6: Fabric Integration\n",
    "â”œâ”€â”€ ğŸ”„ Configure Fabric to read from same Azure Data Lake\n",
    "â”œâ”€â”€ ğŸ”„ Implement cross-channel data merging in Fabric Silver tier\n",
    "â”œâ”€â”€ ğŸ”„ Build Gold tier analytics in Fabric\n",
    "â””â”€â”€ ğŸ”„ Create Power BI dashboards\n",
    "\n",
    "Week 7-8: Production Readiness\n",
    "â”œâ”€â”€ ğŸ”„ Implement Delta Live Tables for streaming\n",
    "â”œâ”€â”€ ğŸ”„ Set up automated deployment pipelines  \n",
    "â”œâ”€â”€ ğŸ”„ Add comprehensive monitoring\n",
    "â””â”€â”€ ğŸ”„ Document integration patterns\n",
    "'''\n",
    "print(roadmap)\n",
    "\n",
    "# Show what you've accomplished in this crash course\n",
    "print(f\"\\nğŸ‰ What You've Accomplished Today:\")\n",
    "print(\"   âœ… Understanding Unity Catalog three-level namespace\")\n",
    "print(\"   âœ… Creating Delta tables with sample data\")\n",
    "print(\"   âœ… Practicing time travel and MERGE operations\")\n",
    "print(\"   âœ… Learning about Jobs, Workflows, and DLT\")\n",
    "print(\"   âœ… Understanding integration patterns for your project\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-59b70778\" language=\"python\">\n",
    "# ğŸ“Š PRACTICAL NEXT STEPS FOR YOUR PROJECT\n",
    "print(\"\\nğŸ“Š PRACTICAL NEXT STEPS FOR YOUR PROJECT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ”§ Immediate Actions (This Week):\")\n",
    "print(\"   1. âœ… Complete this crash course notebook\")\n",
    "print(\"   2. Import your existing product generation notebook\")\n",
    "print(\"   3. Convert generated data to Delta format\")\n",
    "print(\"   4. Practice with Unity Catalog structure\")\n",
    "print(\"   5. Create customer and order generation notebooks\")\n",
    "\n",
    "print(f\"\\nğŸš€ Next Week Actions:\")\n",
    "print(\"   1. Set up Azure Data Lake mount point\")\n",
    "print(\"   2. Create automated jobs for data processing\")\n",
    "print(\"   3. Build basic Bronze â†’ Silver transformation\")\n",
    "print(\"   4. Test cross-channel data integration\")\n",
    "print(\"   5. Set up monitoring and quality checks\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Integration Strategy:\")\n",
    "print(\"   1. Use Delta Lake as the reliable foundation\")\n",
    "print(\"   2. Leverage Unity Catalog for governance\")\n",
    "print(\"   3. External mounts for Fabric integration\")\n",
    "print(\"   4. Jobs/Workflows for automation\")\n",
    "print(\"   5. DLT for advanced production pipelines\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Success Patterns for Your Solution Accelerator:\")\n",
    "patterns = '''\n",
    "âœ… Unity Catalog Governance:\n",
    "   â€¢ Use catalog.schema.table naming consistently\n",
    "   â€¢ Set up proper permissions and stewardship\n",
    "   â€¢ Document business glossary terms\n",
    "\n",
    "âœ… Delta Lake Foundation:\n",
    "   â€¢ All tables in Delta format for ACID transactions\n",
    "   â€¢ Use time travel for debugging and auditing\n",
    "   â€¢ Implement MERGE for efficient updates\n",
    "\n",
    "âœ… External Integration:\n",
    "   â€¢ Mount Azure Data Lake for Fabric connectivity\n",
    "   â€¢ Use consistent file formats across platforms\n",
    "   â€¢ Implement proper error handling and monitoring\n",
    "\n",
    "âœ… Automation Ready:\n",
    "   â€¢ Design notebooks for job conversion\n",
    "   â€¢ Use parameters for flexibility\n",
    "   â€¢ Implement proper logging and alerts\n",
    "\n",
    "âœ… Production Patterns:\n",
    "   â€¢ Use DLT for complex pipelines\n",
    "   â€¢ Implement comprehensive data quality checks\n",
    "   â€¢ Set up proper monitoring and governance\n",
    "'''\n",
    "print(patterns)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell id=\"#VSC-aedc9ce1\" language=\"python\">\n",
    "# ğŸ“ KNOWLEDGE CHECK AND SUMMARY\n",
    "print(\"\\nğŸ“ FINAL KNOWLEDGE CHECK - What You've Learned\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ“š Unity Catalog Mastery (30 min):\")\n",
    "print(\"   âœ… Three-level namespace: catalog.schema.table\")\n",
    "print(\"   âœ… Create catalogs and schemas for medallion architecture\")\n",
    "print(\"   âœ… Understanding data governance and permissions\")\n",
    "print(\"   âœ… Integration with solution accelerator structure\")\n",
    "\n",
    "print(f\"\\nğŸš€ Delta Lake Expertise (45 min):\")\n",
    "print(\"   âœ… Create Delta tables from sample data\")\n",
    "print(\"   âœ… Time travel and versioning capabilities\")\n",
    "print(\"   âœ… MERGE operations for upserts and data management\") \n",
    "print(\"   âœ… ACID transactions and optimization features\")\n",
    "print(\"   âœ… Schema evolution and data quality\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Data Engineering Skills (30 min):\")\n",
    "print(\"   âœ… Convert notebooks to automated jobs\")\n",
    "print(\"   âœ… Set up workflows with dependencies\")\n",
    "print(\"   âœ… External storage integration patterns\")\n",
    "print(\"   âœ… Delta Live Tables for advanced ETL\")\n",
    "print(\"   âœ… Production-ready pipeline design\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Solution Accelerator Ready:\")\n",
    "print(\"   âœ… Technical foundation established\")\n",
    "print(\"   âœ… Integration patterns understood\")\n",
    "print(\"   âœ… Production-ready architecture knowledge\")\n",
    "print(\"   âœ… Hands-on experience with enterprise features\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Performance Summary:\")\n",
    "print(\"   ğŸ¯ Target: 2 hours comprehensive learning\")\n",
    "print(\"   âœ… Unity Catalog: 30 minutes\")\n",
    "print(\"   âœ… Delta Lake: 45 minutes\") \n",
    "print(\"   âœ… Data Engineering: 30 minutes\")\n",
    "print(\"   âœ… Integration: 15 minutes\")\n",
    "print(\"   ğŸ‰ Total: 2 hours of intensive, practical learning!\")\n",
    "\n",
    "print(f\"\\nğŸš€ You're Now Ready To:\")\n",
    "print(\"   â€¢ Build enterprise-grade data pipelines\")\n",
    "print(\"   â€¢ Implement your solution accelerator architecture\")\n",
    "print(\"   â€¢ Integrate Databricks with Microsoft Fabric\")\n",
    "print(\"   â€¢ Create production-ready automated workflows\")\n",
    "print(\"   â€¢ Apply best practices for data governance\")\n",
    "\n",
    "# Clean up demo data (optional)\n",
    "print(f\"\\nğŸ—‘ï¸ Cleanup Options:\")\n",
    "print(\"   To remove demo table: DROP TABLE IF EXISTS customers_delta\")\n",
    "print(\"   To keep for practice: Table will remain for further exploration\")\n",
    "\n",
    "try:\n",
    "    table_exists = spark.sql(\"SHOW TABLES LIKE 'customers_delta'\").count() > 0\n",
    "    if table_exists:\n",
    "        print(\"   âœ… Demo table 'customers_delta' is available for continued practice\")\n",
    "    else:\n",
    "        print(\"   â„¹ï¸ Demo table was not created or is not visible\")\n",
    "except:\n",
    "    print(\"   â„¹ï¸ Unable to check table status\")\n",
    "\n",
    "print(f\"\\nğŸŠ CONGRATULATIONS!\")\n",
    "print(\"You've completed the Azure Databricks 2-Hour Crash Course!\")\n",
    "print(\"Ready to build amazing enterprise data solutions! ğŸš€\")\n",
    "</VSCode.Cell>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088137f3",
   "metadata": {},
   "source": [
    "# Azure Databricks 2-Hour Crash Course\n",
    "\n",
    "## ğŸ¯ Learning Objectives (2 Hours Total)\n",
    "- **30 min:** Unity Catalog fundamentals and three-level namespace\n",
    "- **45 min:** Delta Lake creation, time travel, and MERGE operations\n",
    "- **30 min:** Data Engineering basics (Jobs, Workflows, External Storage)\n",
    "- **15 min:** Integration patterns for your solution accelerator\n",
    "\n",
    "## ğŸ“š Prerequisites\n",
    "- Azure Databricks workspace access\n",
    "- Basic SQL and Python knowledge\n",
    "- Sample data available\n",
    "\n",
    "---\n",
    "\n",
    "# Part 1: Unity Catalog Crash Course (30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ›ï¸ PART 1A: UNITY CATALOG EXPLORATION (10 minutes)\n",
    "print(\"ğŸ›ï¸ UNITY CATALOG EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check current catalog context\n",
    "print(\"\\nğŸ“ Current Catalog Context:\")\n",
    "print(f\"Current Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "print(f\"Current Schema: {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "\n",
    "# List all catalogs\n",
    "print(\"\\nğŸ“š Available Catalogs:\")\n",
    "catalogs = spark.sql(\"SHOW CATALOGS\").collect()\n",
    "for catalog in catalogs:\n",
    "    print(f\"  ğŸ“ {catalog.catalog}\")\n",
    "\n",
    "# List schemas in main catalog\n",
    "print(\"\\nğŸ“‹ Schemas in 'main' catalog:\")\n",
    "try:\n",
    "    schemas = spark.sql(\"SHOW SCHEMAS IN CATALOG main\").collect()\n",
    "    for schema in schemas:\n",
    "        print(f\"  ğŸ“‚ main.{schema.databaseName}\")\n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Error: {e}\")\n",
    "    print(\"  ğŸ’¡ Unity Catalog may not be enabled or you may not have access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ PART 1B: CREATE YOUR OWN CATALOG STRUCTURE (10 minutes)\n",
    "print(\"\\nğŸ—ï¸ CREATING CATALOG STRUCTURE FOR SOLUTION ACCELERATOR\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a catalog for your solution accelerator (if permissions allow)\n",
    "catalog_name = \"solution_accelerator\"\n",
    "schema_names = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "print(f\"\\nğŸ“ Creating catalog: {catalog_name}\")\n",
    "try:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "    print(\"  âœ… Catalog created/exists\")\n",
    "    \n",
    "    # Create schemas for medallion architecture\n",
    "    for schema in schema_names:\n",
    "        print(f\"\\nğŸ“‚ Creating schema: {catalog_name}.{schema}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema}\")\n",
    "        print(f\"  âœ… Schema {catalog_name}.{schema} created/exists\")\n",
    "        \n",
    "    # Set working catalog\n",
    "    spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "    print(f\"\\nâœ… Now working in catalog: {catalog_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  âŒ Cannot create catalog (permissions): {e}\")\n",
    "    print(\"  ğŸ’¡ Using 'main' catalog instead\")\n",
    "    catalog_name = \"main\"\n",
    "    \n",
    "    # Try to create schemas in main catalog\n",
    "    for schema in schema_names:\n",
    "        try:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "            print(f\"  âœ… Schema {schema} created in main catalog\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Could not create schema {schema}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954574d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” PART 1C: THREE-LEVEL NAMESPACE DEMO (10 minutes)\n",
    "print(\"\\nğŸ” THREE-LEVEL NAMESPACE DEMONSTRATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Demonstrate three-level namespace: catalog.schema.table\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "\n",
    "print(f\"ğŸ“ Three-Level Namespace Structure:\")\n",
    "print(f\"   Catalog: {current_catalog}\")\n",
    "print(f\"   Schema:  bronze, silver, gold\")\n",
    "print(f\"   Table:   customer, product, orders\")\n",
    "print(f\"   Full:    {current_catalog}.bronze.customer\")\n",
    "\n",
    "# Show current context\n",
    "print(f\"\\nğŸ“‹ Current Working Context:\")\n",
    "print(f\"   Catalog: {spark.sql('SELECT current_catalog()').collect()[0][0]}\")\n",
    "print(f\"   Schema:  {spark.sql('SELECT current_schema()').collect()[0][0]}\")\n",
    "\n",
    "# List tables in current schema\n",
    "print(f\"\\nğŸ“Š Tables in current schema:\")\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\").collect()\n",
    "    if tables:\n",
    "        for table in tables[:5]:  # Show first 5\n",
    "            print(f\"   ğŸ“‹ {table.tableName}\")\n",
    "    else:\n",
    "        print(\"   ğŸ“­ No tables found (expected for new schemas)\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error listing tables: {e}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Unity Catalog Concepts:\")\n",
    "print(\"   â€¢ Catalogs = Top-level containers (like databases)\")\n",
    "print(\"   â€¢ Schemas = Logical groupings within catalogs\")  \n",
    "print(\"   â€¢ Tables = Data assets within schemas\")\n",
    "print(\"   â€¢ Full path: catalog.schema.table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc3f32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Delta Lake Deep Dive (45 minutes)\n",
    "\n",
    "## What is Delta Lake?\n",
    "- **ACID transactions** on data lakes\n",
    "- **Time travel** - query historical versions\n",
    "- **Schema evolution** - safely modify table structure\n",
    "- **Optimizations** - Z-ordering, auto-compaction\n",
    "- **Merges/Upserts** - efficiently update data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ PART 2A: CREATE SAMPLE DATA AND DELTA TABLES (15 minutes)\n",
    "print(\"ğŸš€ CREATING SAMPLE DATA FOR DELTA LAKE DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create sample customer data\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate sample data\n",
    "customers_data = []\n",
    "for i in range(100):\n",
    "    customers_data.append({\n",
    "        'customer_id': i + 1,\n",
    "        'customer_name': f'Customer_{i+1:03d}',\n",
    "        'email': f'customer{i+1}@example.com',\n",
    "        'city': random.choice(['Seattle', 'Portland', 'San Francisco', 'Los Angeles', 'Denver']),\n",
    "        'registration_date': datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "        'status': random.choice(['Active', 'Inactive', 'Pending'])\n",
    "    })\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "customers_df = spark.createDataFrame(customers_data)\n",
    "\n",
    "print(\"ğŸ“Š Sample Customer Data Created:\")\n",
    "customers_df.show(5)\n",
    "print(f\"   Total Records: {customers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a423b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ PART 2B: CREATE DELTA TABLE (10 minutes)\n",
    "print(\"\\nğŸ—ï¸ CREATING DELTA TABLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set working schema\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "schema_name = \"bronze\" if current_catalog != \"main\" else \"default\"\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "    print(f\"âœ… Using schema: {current_catalog}.{schema_name}\")\n",
    "except:\n",
    "    print(f\"âš ï¸ Using default schema\")\n",
    "\n",
    "# Create Delta table\n",
    "table_name = \"customers_delta\"\n",
    "full_table_name = f\"{current_catalog}.{schema_name}.{table_name}\"\n",
    "\n",
    "print(f\"\\nğŸ“‹ Creating Delta table: {full_table_name}\")\n",
    "\n",
    "# Write as Delta table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(\"âœ… Delta table created successfully!\")\n",
    "\n",
    "# Verify table creation\n",
    "print(f\"\\nğŸ” Table Information:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {table_name}\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â° PART 2C: DELTA LAKE TIME TRAVEL (10 minutes)\n",
    "print(\"\\nâ° DELTA LAKE TIME TRAVEL DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show initial version\n",
    "print(\"ğŸ“Š Initial table state:\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").show()\n",
    "\n",
    "# Make some updates to create versions\n",
    "print(\"\\nğŸ”„ Creating new versions...\")\n",
    "\n",
    "# Version 1: Update some customers\n",
    "print(\"Version 1: Updating customer status...\")\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {table_name} \n",
    "    SET status = 'VIP' \n",
    "    WHERE customer_id <= 10\n",
    "\"\"\")\n",
    "\n",
    "# Version 2: Insert new customers  \n",
    "print(\"Version 2: Adding new customers...\")\n",
    "new_customers = spark.createDataFrame([\n",
    "    (101, 'Customer_101', 'customer101@example.com', 'Boston', datetime.now(), 'Active'),\n",
    "    (102, 'Customer_102', 'customer102@example.com', 'Chicago', datetime.now(), 'Active')\n",
    "], ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status'])\n",
    "\n",
    "new_customers.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "# Show table history\n",
    "print(\"\\nğŸ“š Delta Table History:\")\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {table_name}\")\n",
    "history_df.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n",
    "\n",
    "# Time travel examples\n",
    "print(\"\\nâ° Time Travel Examples:\")\n",
    "\n",
    "print(\"\\nğŸ“¸ Version 0 (original):\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count, COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count FROM {table_name} VERSION AS OF 0\").show()\n",
    "\n",
    "print(\"\\nğŸ“¸ Version 1 (after VIP update):\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count, COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count FROM {table_name} VERSION AS OF 1\").show()\n",
    "\n",
    "print(\"\\nğŸ“¸ Current version:\")\n",
    "spark.sql(f\"SELECT COUNT(*) as count, COUNT(CASE WHEN status='VIP' THEN 1 END) as vip_count FROM {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”€ PART 2D: DELTA MERGE OPERATIONS (10 minutes)\n",
    "print(\"\\nğŸ”€ DELTA MERGE OPERATIONS DEMO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create updates dataset\n",
    "updates_data = [\n",
    "    (5, 'Customer_005_Updated', 'updated5@example.com', 'Updated_City', datetime.now(), 'Premium'),\n",
    "    (10, 'Customer_010_Updated', 'updated10@example.com', 'Updated_City', datetime.now(), 'Premium'),\n",
    "    (103, 'Customer_103_New', 'customer103@example.com', 'Miami', datetime.now(), 'Active')  # New customer\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(updates_data, \n",
    "    ['customer_id', 'customer_name', 'email', 'city', 'registration_date', 'status'])\n",
    "\n",
    "print(\"ğŸ“Š Updates to apply:\")\n",
    "updates_df.show()\n",
    "\n",
    "# Create temporary view for MERGE\n",
    "updates_df.createOrReplaceTempView(\"customer_updates\")\n",
    "\n",
    "# Perform MERGE operation\n",
    "print(\"\\nğŸ”€ Executing MERGE operation...\")\n",
    "merge_sql = f\"\"\"\n",
    "MERGE INTO {table_name} as target\n",
    "USING customer_updates as source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET \n",
    "        customer_name = source.customer_name,\n",
    "        email = source.email,\n",
    "        city = source.city,\n",
    "        status = source.status\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (customer_id, customer_name, email, city, registration_date, status)\n",
    "    VALUES (source.customer_id, source.customer_name, source.email, source.city, source.registration_date, source.status)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(merge_sql)\n",
    "print(\"âœ… MERGE operation completed!\")\n",
    "\n",
    "# Verify results\n",
    "print(\"\\nğŸ“Š Results after MERGE:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT customer_id, customer_name, city, status \n",
    "    FROM {table_name} \n",
    "    WHERE customer_id IN (5, 10, 103)\n",
    "    ORDER BY customer_id\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nğŸ“ˆ Total count after MERGE:\")\n",
    "spark.sql(f\"SELECT COUNT(*) as total_customers FROM {table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aff872",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Data Engineering Basics (30 minutes)\n",
    "\n",
    "## Key Concepts:\n",
    "- **Notebooks â†’ Jobs** (scheduled execution)\n",
    "- **Workflows** (multi-task orchestration)  \n",
    "- **External Storage** (Azure Data Lake integration)\n",
    "- **Delta Live Tables** (streaming ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba00996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ PART 3A: EXTERNAL STORAGE CONNECTION (10 minutes)\n",
    "print(\"ğŸ”§ EXTERNAL STORAGE CONNECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check existing mounts\n",
    "print(\"ğŸ“ Current mounted storage:\")\n",
    "mounts = dbutils.fs.mounts()\n",
    "for mount in mounts:\n",
    "    print(f\"   ğŸ”— {mount.mountPoint} â†’ {mount.source}\")\n",
    "\n",
    "# Demo: How to mount Azure Data Lake (conceptual - requires credentials)\n",
    "print(f\"\\nğŸ’¡ Mounting Azure Data Lake Storage (ADLS) - Conceptual Example:\")\n",
    "mount_example = '''\n",
    "# Mount ADLS Gen2 for your solution accelerator\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://container@storageaccount.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/solution-accelerator\",\n",
    "    extra_configs = {\n",
    "        \"fs.azure.account.auth.type.storageaccount.dfs.core.windows.net\": \"OAuth\",\n",
    "        \"fs.azure.account.oauth.provider.type.storageaccount.dfs.core.windows.net\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "        \"fs.azure.account.oauth2.client.id.storageaccount.dfs.core.windows.net\": \"application-id\",\n",
    "        \"fs.azure.account.oauth2.client.secret.storageaccount.dfs.core.windows.net\": \"service-credential-key\"\n",
    "    }\n",
    ")\n",
    "'''\n",
    "print(mount_example)\n",
    "\n",
    "# Show how to work with mounted storage\n",
    "print(\"\\nğŸ“ Working with mounted storage:\")\n",
    "print(\"   â€¢ List files: dbutils.fs.ls('/mnt/solution-accelerator/')\")\n",
    "print(\"   â€¢ Read CSV: spark.read.csv('/mnt/solution-accelerator/data.csv')\")\n",
    "print(\"   â€¢ Write Delta: df.write.format('delta').save('/mnt/solution-accelerator/delta-table')\")\n",
    "\n",
    "# For your solution accelerator integration:\n",
    "print(f\"\\nğŸ¯ For Your Solution Accelerator:\")\n",
    "print(\"   1. Mount Azure Data Lake where Fabric can access\")\n",
    "print(\"   2. Write Delta tables to mounted location\")  \n",
    "print(\"   3. Fabric reads from same Azure Data Lake location\")\n",
    "print(\"   4. Creates seamless Databricks â†’ Fabric pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bf10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š PART 3B: JOBS AND WORKFLOWS CONCEPTS (10 minutes)\n",
    "print(\"\\nğŸ“Š JOBS AND WORKFLOWS CONCEPTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ”„ Converting Notebooks to Jobs:\")\n",
    "print(\"   1. Save your notebook (this one!)\")\n",
    "print(\"   2. Go to 'Workflows' in Databricks sidebar\")\n",
    "print(\"   3. Click 'Create Job'\")\n",
    "print(\"   4. Select this notebook as the task\")\n",
    "print(\"   5. Configure cluster and schedule\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ Job Configuration Example for Your Solution Accelerator:\")\n",
    "job_config = '''\n",
    "Job Name: \"Solution Accelerator Data Pipeline\"\n",
    "Tasks:\n",
    "â”œâ”€â”€ Task 1: \"Data Ingestion\" \n",
    "â”‚   â”œâ”€â”€ Notebook: /Users/your-email/Data_Ingestion.ipynb\n",
    "â”‚   â”œâ”€â”€ Cluster: Job cluster (auto-terminating)\n",
    "â”‚   â””â”€â”€ Libraries: Delta Lake, pandas\n",
    "â”œâ”€â”€ Task 2: \"Data Transformation\" (depends on Task 1)\n",
    "â”‚   â”œâ”€â”€ Notebook: /Users/your-email/Data_Transformation.ipynb  \n",
    "â”‚   â””â”€â”€ Parameters: {\"source_table\": \"bronze.customers\"}\n",
    "â””â”€â”€ Task 3: \"Data Quality Check\" (depends on Task 2)\n",
    "    â”œâ”€â”€ Notebook: /Users/your-email/Data_Quality.ipynb\n",
    "    â””â”€â”€ Alerts: Email on failure\n",
    "    \n",
    "Schedule: Daily at 2 AM\n",
    "Retry: 3 attempts with exponential backoff\n",
    "'''\n",
    "print(job_config)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Workflow Benefits:\")\n",
    "print(\"   âœ… Automated execution\")\n",
    "print(\"   âœ… Dependency management\") \n",
    "print(\"   âœ… Error handling and retries\")\n",
    "print(\"   âœ… Monitoring and alerts\")\n",
    "print(\"   âœ… Parameter passing between tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒŠ PART 3C: DELTA LIVE TABLES OVERVIEW (10 minutes)\n",
    "print(\"\\nğŸŒŠ DELTA LIVE TABLES (DLT) OVERVIEW\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ’¡ What is Delta Live Tables?\")\n",
    "print(\"   â€¢ Declarative ETL framework\")\n",
    "print(\"   â€¢ Automatically manages dependencies\")\n",
    "print(\"   â€¢ Built-in data quality monitoring\")  \n",
    "print(\"   â€¢ Streaming and batch processing\")\n",
    "\n",
    "print(f\"\\nğŸ“ DLT Example for Your Solution Accelerator:\")\n",
    "dlt_example = '''\n",
    "# Bronze Layer - Raw data ingestion\n",
    "@dlt.table(\n",
    "    comment=\"Raw customer data from source systems\"\n",
    ")\n",
    "def bronze_customers():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/mnt/schema/customers\")\n",
    "        .load(\"/mnt/raw-data/customers/\")\n",
    "    )\n",
    "\n",
    "# Silver Layer - Cleaned and validated data  \n",
    "@dlt.table(\n",
    "    comment=\"Cleaned customer data with quality checks\"\n",
    ")\n",
    "@dlt.expect(\"valid_email\", \"email IS NOT NULL AND email RLIKE '^[^@]+@[^@]+\\\\\\\\.[^@]+$'\")\n",
    "@dlt.expect_or_drop(\"customer_id_not_null\", \"customer_id IS NOT NULL\")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        dlt.read(\"bronze_customers\")\n",
    "        .select(\"customer_id\", \"customer_name\", \"email\", \"city\", \"registration_date\")\n",
    "        .filter(\"customer_id IS NOT NULL\")\n",
    "    )\n",
    "\n",
    "# Gold Layer - Business-ready aggregated data\n",
    "@dlt.table(\n",
    "    comment=\"Customer metrics for business analytics\"\n",
    ")\n",
    "def gold_customer_metrics():\n",
    "    return (\n",
    "        dlt.read(\"silver_customers\")\n",
    "        .groupBy(\"city\")\n",
    "        .agg(\n",
    "            count(\"customer_id\").alias(\"customer_count\"),\n",
    "            max(\"registration_date\").alias(\"latest_registration\")\n",
    "        )\n",
    "    )\n",
    "'''\n",
    "print(dlt_example)\n",
    "\n",
    "print(f\"\\nğŸ¯ DLT Benefits for Your Architecture:\")\n",
    "print(\"   âœ… Automatic Bronze â†’ Silver â†’ Gold pipeline\")\n",
    "print(\"   âœ… Built-in data quality monitoring\")\n",
    "print(\"   âœ… Real-time streaming capability\")\n",
    "print(\"   âœ… Automatic dependency resolution\")\n",
    "print(\"   âœ… Lineage tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ee741",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Solution Accelerator Integration (15 minutes)\n",
    "\n",
    "## How This Applies to Your Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ PART 4: SOLUTION ACCELERATOR INTEGRATION PATTERNS\n",
    "print(\"ğŸ¯ SOLUTION ACCELERATOR INTEGRATION PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ—ï¸ Your Architecture Integration:\")\n",
    "print(\"   Databricks (Online Channel) â†’ Azure Data Lake â†’ Fabric Bronze â†’ Silver â†’ Gold\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Implementation Roadmap:\")\n",
    "roadmap = '''\n",
    "Week 1-2: Databricks Setup\n",
    "â”œâ”€â”€ Create Unity Catalog structure (solution_accelerator.bronze/silver/gold)\n",
    "â”œâ”€â”€ Generate sample data in Delta format  \n",
    "â”œâ”€â”€ Set up external storage mounts to Azure Data Lake\n",
    "â””â”€â”€ Create basic ETL notebooks\n",
    "\n",
    "Week 3-4: Data Pipeline Development  \n",
    "â”œâ”€â”€ Build Bronze â†’ Silver transformation jobs\n",
    "â”œâ”€â”€ Implement data quality checks with Delta\n",
    "â”œâ”€â”€ Set up automated workflows\n",
    "â””â”€â”€ Create monitoring and alerting\n",
    "\n",
    "Week 5-6: Fabric Integration\n",
    "â”œâ”€â”€ Configure Fabric to read from same Azure Data Lake\n",
    "â”œâ”€â”€ Implement cross-channel data merging in Fabric Silver tier\n",
    "â”œâ”€â”€ Build Gold tier analytics in Fabric\n",
    "â””â”€â”€ Create Power BI dashboards\n",
    "\n",
    "Week 7-8: Production Readiness\n",
    "â”œâ”€â”€ Implement Delta Live Tables for streaming\n",
    "â”œâ”€â”€ Set up automated deployment pipelines  \n",
    "â”œâ”€â”€ Add comprehensive monitoring\n",
    "â””â”€â”€ Document integration patterns\n",
    "'''\n",
    "print(roadmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d18005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š PRACTICAL NEXT STEPS FOR YOUR PROJECT\n",
    "print(\"\\nğŸ“Š PRACTICAL NEXT STEPS FOR YOUR PROJECT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ”§ Immediate Actions (Today):\")\n",
    "print(\"   1. âœ… Run this crash course notebook\")\n",
    "print(\"   2. Create your sample data as Delta tables\")\n",
    "print(\"   3. Practice MERGE operations with your product/customer data\")\n",
    "print(\"   4. Set up basic Unity Catalog structure\")\n",
    "\n",
    "print(f\"\\nğŸš€ This Week:\")\n",
    "print(\"   1. Import your existing product generation notebook\")\n",
    "print(\"   2. Convert generated data to Delta format\")  \n",
    "print(\"   3. Create customer and order generation notebooks\")\n",
    "print(\"   4. Build basic Bronze â†’ Silver transformation\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Next Week:\")\n",
    "print(\"   1. Set up Azure Data Lake mount point\")\n",
    "print(\"   2. Create automated jobs for data processing\")\n",
    "print(\"   3. Test Fabric integration with Delta tables\")\n",
    "print(\"   4. Build monitoring and quality checks\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Success Patterns:\")\n",
    "patterns = '''\n",
    "âœ… Unity Catalog for governance: catalog.schema.table naming\n",
    "âœ… Delta Lake for reliability: ACID transactions, time travel\n",
    "âœ… External mounts for integration: Databricks â†” Fabric via ADLS\n",
    "âœ… Jobs/Workflows for automation: scheduled, monitored pipelines\n",
    "âœ… DLT for advanced ETL: streaming, quality, lineage\n",
    "'''\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ KNOWLEDGE CHECK AND SUMMARY\n",
    "print(\"\\nğŸ“ KNOWLEDGE CHECK - What You've Learned\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ğŸ“š Unity Catalog (30 min):\")\n",
    "print(\"   âœ… Three-level namespace: catalog.schema.table\")\n",
    "print(\"   âœ… Create catalogs and schemas for medallion architecture\")\n",
    "print(\"   âœ… Understanding data governance and permissions\")\n",
    "\n",
    "print(f\"\\nğŸš€ Delta Lake (45 min):\")\n",
    "print(\"   âœ… Create Delta tables from sample data\")\n",
    "print(\"   âœ… Time travel and versioning capabilities\")\n",
    "print(\"   âœ… MERGE operations for upserts\") \n",
    "print(\"   âœ… ACID transactions and optimization\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Data Engineering (30 min):\")\n",
    "print(\"   âœ… Convert notebooks to automated jobs\")\n",
    "print(\"   âœ… Set up workflows with dependencies\")\n",
    "print(\"   âœ… External storage integration patterns\")\n",
    "print(\"   âœ… Delta Live Tables for advanced ETL\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Ready for Your Solution Accelerator:\")\n",
    "print(\"   âœ… Technical foundation established\")\n",
    "print(\"   âœ… Integration patterns understood\")\n",
    "print(\"   âœ… Production-ready architecture knowledge\")\n",
    "print(\"   âœ… Hands-on experience with key features\")\n",
    "\n",
    "print(f\"\\nğŸš€ Total Learning Time: 2 hours\")\n",
    "print(\"ğŸ‰ You're now ready to build your enterprise solution accelerator!\")\n",
    "\n",
    "# Clean up demo data (optional)\n",
    "try:\n",
    "    print(f\"\\nğŸ—‘ï¸ Cleaning up demo table (optional):\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    print(\"   âœ… Demo table removed\")\n",
    "except:\n",
    "    print(\"   âš ï¸ Could not remove demo table (may not have permissions)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
