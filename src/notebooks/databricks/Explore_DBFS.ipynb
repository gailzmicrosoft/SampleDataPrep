{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e400f9a7",
   "metadata": {},
   "source": [
    "# Explore Databricks File System (DBFS)\n",
    "\n",
    "## Overview\n",
    "This notebook explores the DBFS structure and capabilities in your Azure Databricks workspace.\n",
    "\n",
    "## Learning Steps\n",
    "- **Step 1:** Explore DBFS root structure\n",
    "- **Step 2:** Discover sample datasets (including TPC-H search)\n",
    "- **Step 3:** Examine FileStore for uploads\n",
    "- **Step 4:** Check mounted external storage\n",
    "- **Step 5:** Test DBFS utility functions\n",
    "- **Step 6:** Learn useful DBFS commands\n",
    "- **Step 7:** Run DBFS health check\n",
    "- **Step 8:** Search for TPC-H and benchmark datasets\n",
    "\n",
    "## DBFS Key Locations\n",
    "- `/` - Root directory\n",
    "- `/databricks-datasets/` - Sample datasets provided by Databricks\n",
    "- `/FileStore/` - Files uploaded via web UI\n",
    "- `/mnt/` - Mounted external storage (Azure Data Lake, Blob Storage)\n",
    "- `/tmp/` - Temporary files\n",
    "- `/user/` - User-specific directories\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Explore DBFS Root Structure\n",
    "print(\"🔍 STEP 1: EXPLORING DBFS ROOT STRUCTURE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# List root directory contents\n",
    "print(\"\\n📁 Root Directory Contents:\")\n",
    "root_contents = dbutils.fs.ls(\"/\")\n",
    "for item in root_contents:\n",
    "    item_type = \"📁\" if item.isDir else \"📄\"\n",
    "    size = f\"({item.size} bytes)\" if not item.isDir else \"\"\n",
    "    print(f\"  {item_type} {item.name} {size}\")\n",
    "\n",
    "print(\"\\n✅ Step 1 Complete: Root structure explored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d010e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Explore Databricks Sample Datasets\n",
    "print(\"\\n🎯 STEP 2: DATABRICKS SAMPLE DATASETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    datasets = dbutils.fs.ls(\"/databricks-datasets/\")\n",
    "    print(f\"\\n📊 Found {len(datasets)} sample dataset categories:\")\n",
    "    \n",
    "    # Show first 10 categories\n",
    "    for i, item in enumerate(datasets[:10]):\n",
    "        print(f\"  📁 {item.name}\")\n",
    "    \n",
    "    if len(datasets) > 10:\n",
    "        print(f\"  ... and {len(datasets) - 10} more categories\")\n",
    "    \n",
    "    # ENHANCED: Search through ALL 56 datasets for useful ones\n",
    "    print(f\"\\n🔍 COMPREHENSIVE DATASET SEARCH:\")\n",
    "    print(f\"Searching through all {len(datasets)} datasets for business-relevant data...\")\n",
    "    \n",
    "    # Categorize datasets by usefulness for your solution accelerator\n",
    "    business_datasets = []\n",
    "    benchmark_datasets = []\n",
    "    tpch_datasets = []\n",
    "    retail_datasets = []\n",
    "    \n",
    "    for item in datasets:\n",
    "        dataset_name = item.name.lower()\n",
    "        \n",
    "        # Check for TPC-H\n",
    "        if 'tpch' in dataset_name or 'tpc-h' in dataset_name or 'tpc_h' in dataset_name:\n",
    "            tpch_datasets.append(item.name)\n",
    "        \n",
    "        # Check for retail/sales data\n",
    "        elif any(keyword in dataset_name for keyword in ['retail', 'sales', 'ecommerce', 'store', 'customer', 'product']):\n",
    "            retail_datasets.append(item.name)\n",
    "        \n",
    "        # Check for benchmark data\n",
    "        elif any(keyword in dataset_name for keyword in ['benchmark', 'test', 'demo', 'sample']):\n",
    "            benchmark_datasets.append(item.name)\n",
    "        \n",
    "        # Check for other business datasets\n",
    "        elif any(keyword in dataset_name for keyword in ['airline', 'amazon', 'financial', 'bank', 'order']):\n",
    "            business_datasets.append(item.name)\n",
    "    \n",
    "    # Report findings\n",
    "    if tpch_datasets:\n",
    "        print(f\"\\n🎯 TPC-H DATASETS FOUND ({len(tpch_datasets)}):\")\n",
    "        for dataset in tpch_datasets:\n",
    "            print(f\"  📊 {dataset}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ No TPC-H datasets found by name\")\n",
    "    \n",
    "    if retail_datasets:\n",
    "        print(f\"\\n🛒 RETAIL/SALES DATASETS ({len(retail_datasets)}):\")\n",
    "        for dataset in retail_datasets[:5]:  # Show first 5\n",
    "            print(f\"  📊 {dataset}\")\n",
    "        if len(retail_datasets) > 5:\n",
    "            print(f\"  ... and {len(retail_datasets) - 5} more retail datasets\")\n",
    "    \n",
    "    if business_datasets:\n",
    "        print(f\"\\n💼 BUSINESS DATASETS ({len(business_datasets)}):\")\n",
    "        for dataset in business_datasets[:5]:  # Show first 5\n",
    "            print(f\"  📊 {dataset}\")\n",
    "        if len(business_datasets) > 5:\n",
    "            print(f\"  ... and {len(business_datasets) - 5} more business datasets\")\n",
    "    \n",
    "    # Explore specific useful datasets\n",
    "    useful_datasets = {\n",
    "        'airlines/': 'Airline data (good for order/booking patterns)',\n",
    "        'amazon/': 'E-commerce data (product/customer patterns)',\n",
    "        'retail-org/': 'Retail organization data',\n",
    "        'online_retail/': 'Online retail transactions',\n",
    "        'ecommerce/': 'E-commerce transaction data'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n🎯 EXPLORING PRIORITY DATASETS FOR YOUR PROJECT:\")\n",
    "    for dataset_path, description in useful_datasets.items():\n",
    "        try:\n",
    "            contents = dbutils.fs.ls(f\"/databricks-datasets/{dataset_path}\")\n",
    "            print(f\"\\n📁 {dataset_path} - {description}\")\n",
    "            print(f\"   Contents ({len(contents)} items):\")\n",
    "            for item in contents[:3]:  # Show first 3 items\n",
    "                print(f\"     📄 {item.name}\")\n",
    "        except:\n",
    "            print(f\"\\n📁 {dataset_path} - Not found\")\n",
    "    \n",
    "    # Explore a specific dataset (like NYC taxi or retail)\n",
    "    print(\"\\n🚕 Exploring NYC Taxi Data (if available):\")\n",
    "    try:\n",
    "        nyc_taxi = dbutils.fs.ls(\"/databricks-datasets/nyctaxi/\")\n",
    "        for item in nyc_taxi[:5]:  # Show first 5 items\n",
    "            print(f\"  📄 {item.name}\")\n",
    "    except:\n",
    "        print(\"  NYC Taxi dataset not found\")\n",
    "        \n",
    "    print(\"\\n🛒 Exploring Retail Data (if available):\")\n",
    "    try:\n",
    "        retail = dbutils.fs.ls(\"/databricks-datasets/retail-org/\")\n",
    "        for item in retail[:5]:  # Show first 5 items\n",
    "            print(f\"  📄 {item.name}\")\n",
    "    except:\n",
    "        print(\"  Retail dataset not found\")\n",
    "    \n",
    "    # Show ALL dataset names for reference\n",
    "    print(f\"\\n📋 COMPLETE DATASET LIST (all {len(datasets)} categories):\")\n",
    "    for i, item in enumerate(datasets, 1):\n",
    "        print(f\"  {i:2d}. {item.name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error accessing datasets: {e}\")\n",
    "\n",
    "print(\"\\n✅ Step 2 Complete: Sample datasets explored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2152389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Explore FileStore (for uploaded files)\n",
    "print(\"\\n📂 STEP 3: FILESTORE EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    filestore = dbutils.fs.ls(\"/FileStore/\")\n",
    "    print(f\"\\n📁 FileStore Contents ({len(filestore)} items):\")\n",
    "    \n",
    "    for item in filestore:\n",
    "        item_type = \"📁\" if item.isDir else \"📄\"\n",
    "        size = f\"({item.size} bytes)\" if not item.isDir else \"\"\n",
    "        print(f\"  {item_type} {item.name} {size}\")\n",
    "        \n",
    "        # Explore each directory found\n",
    "        if item.isDir:\n",
    "            try:\n",
    "                dir_contents = dbutils.fs.ls(f\"/FileStore/{item.name}\")\n",
    "                if dir_contents:\n",
    "                    print(f\"    📊 Contains {len(dir_contents)} items:\")\n",
    "                    for sub_item in dir_contents[:5]:  # Show first 5 items\n",
    "                        sub_type = \"📁\" if sub_item.isDir else \"📄\"\n",
    "                        sub_size = f\" ({sub_item.size} bytes)\" if not sub_item.isDir else \"\"\n",
    "                        print(f\"      {sub_type} {sub_item.name}{sub_size}\")\n",
    "                    if len(dir_contents) > 5:\n",
    "                        print(f\"      ... and {len(dir_contents) - 5} more items\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Error exploring {item.name}: {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ FileStore not found: {e}\")\n",
    "    print(\"💡 This means no files have been uploaded yet\")\n",
    "\n",
    "print(\"\\n✅ Step 3 Complete: FileStore examined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76599769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Check for mounted storage\n",
    "print(\"\\n🔗 STEP 4: MOUNTED STORAGE EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    mounts = dbutils.fs.mounts()\n",
    "    print(f\"\\n📎 Found {len(mounts)} mounted storage locations:\")\n",
    "    \n",
    "    for mount in mounts:\n",
    "        print(f\"  🔗 Mount: {mount.mountPoint}\")\n",
    "        print(f\"      Source: {mount.source}\")\n",
    "        print(f\"      Extras: {mount.extraConfigs}\")\n",
    "        print()\n",
    "        \n",
    "    # If mounts exist, explore the first one\n",
    "    if mounts:\n",
    "        first_mount = mounts[0].mountPoint\n",
    "        print(f\"🔍 Exploring first mount: {first_mount}\")\n",
    "        try:\n",
    "            mount_contents = dbutils.fs.ls(first_mount)\n",
    "            for item in mount_contents[:5]:  # Show first 5 items\n",
    "                item_type = \"📁\" if item.isDir else \"📄\"\n",
    "                print(f\"  {item_type} {item.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error exploring mount: {e}\")\n",
    "    else:\n",
    "        print(\"💡 No external storage mounted yet - this is normal for new workspaces\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error checking mounts: {e}\")\n",
    "\n",
    "print(\"\\n✅ Step 4 Complete: Mounted storage checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: DBFS Utility Functions Demo\n",
    "print(\"\\n🛠️ STEP 5: DBFS UTILITY FUNCTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a temporary directory and file for testing\n",
    "test_dir = \"/tmp/dbfs_exploration\"\n",
    "test_file = f\"{test_dir}/test_file.txt\"\n",
    "\n",
    "print(f\"\\n📝 Creating test directory: {test_dir}\")\n",
    "try:\n",
    "    dbutils.fs.mkdirs(test_dir)\n",
    "    print(\"  ✅ Directory created successfully\")\n",
    "    \n",
    "    # Create a test file\n",
    "    print(f\"\\n📄 Creating test file: {test_file}\")\n",
    "    dbutils.fs.put(test_file, \"Hello from DBFS exploration!\\nThis is a test file.\", overwrite=True)\n",
    "    print(\"  ✅ File created successfully\")\n",
    "    \n",
    "    # Read the file back\n",
    "    print(f\"\\n📖 Reading test file:\")\n",
    "    file_content = dbutils.fs.head(test_file)\n",
    "    print(f\"  Content: {file_content}\")\n",
    "    \n",
    "    # Check file info\n",
    "    print(f\"\\n📊 File information:\")\n",
    "    file_info = dbutils.fs.ls(test_dir)\n",
    "    for item in file_info:\n",
    "        print(f\"  📄 {item.name} - {item.size} bytes\")\n",
    "    \n",
    "    # Clean up\n",
    "    print(f\"\\n🗑️ Cleaning up test files:\")\n",
    "    dbutils.fs.rm(test_dir, recurse=True)\n",
    "    print(\"  ✅ Test files removed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Error in file operations: {e}\")\n",
    "\n",
    "print(\"\\n✅ Step 5 Complete: DBFS utility functions tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc0db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Display useful DBFS commands reference\n",
    "print(\"\\n📚 STEP 6: USEFUL DBFS COMMANDS REFERENCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "commands = \"\"\"\n",
    "🔍 EXPLORATION COMMANDS:\n",
    "  dbutils.fs.ls(\"/path/\")              # List directory contents\n",
    "  dbutils.fs.head(\"/path/file.txt\")    # Read first part of file\n",
    "  dbutils.fs.mounts()                  # Show mounted storage\n",
    "\n",
    "📁 DIRECTORY OPERATIONS:\n",
    "  dbutils.fs.mkdirs(\"/path/new_dir\")   # Create directory\n",
    "  dbutils.fs.rm(\"/path/\", True)        # Remove directory recursively\n",
    "  dbutils.fs.cp(\"/src\", \"/dest\")       # Copy files/directories\n",
    "\n",
    "📄 FILE OPERATIONS:\n",
    "  dbutils.fs.put(\"/path/file.txt\", \"content\")  # Create/write file\n",
    "  dbutils.fs.mv(\"/old_path\", \"/new_path\")      # Move/rename file\n",
    "  \n",
    "🔗 MOUNT OPERATIONS:\n",
    "  dbutils.fs.mount(source, mount_point, extra_configs)  # Mount storage\n",
    "  dbutils.fs.unmount(mount_point)                       # Unmount storage\n",
    "\n",
    "📊 INFORMATION COMMANDS:\n",
    "  dbutils.fs.ls(\"/path/\")              # Detailed file/directory info\n",
    "  %fs ls /path/                        # Magic command alternative\n",
    "  \n",
    "💡 TIPS:\n",
    "  - Use %fs magic commands for quick operations: %fs ls /\n",
    "  - DBFS paths start with /dbfs/ when accessed from driver node\n",
    "  - Use display(dbutils.fs.ls(\"/path/\")) for formatted output\n",
    "\"\"\"\n",
    "\n",
    "print(commands)\n",
    "print(\"\\n✅ Step 6 Complete: DBFS commands reference provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Quick DBFS Health Check\n",
    "print(\"\\n🏥 STEP 7: DBFS HEALTH CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "health_checks = [\n",
    "    (\"Root access\", lambda: len(dbutils.fs.ls(\"/\")) > 0),\n",
    "    (\"Sample datasets\", lambda: len(dbutils.fs.ls(\"/databricks-datasets/\")) > 0),\n",
    "    (\"FileStore access\", lambda: dbutils.fs.ls(\"/FileStore/\") is not None),\n",
    "    (\"Temp directory writable\", lambda: dbutils.fs.mkdirs(\"/tmp/health_check\") and dbutils.fs.rm(\"/tmp/health_check\", True))\n",
    "]\n",
    "\n",
    "for check_name, check_func in health_checks:\n",
    "    try:\n",
    "        result = check_func()\n",
    "        status = \"✅ PASS\" if result else \"⚠️ FAIL\"\n",
    "        print(f\"  {status} {check_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ ERROR {check_name}: {e}\")\n",
    "\n",
    "print(\"\\n✅ Step 7 Complete: DBFS health check finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: TPC-H Dataset Search (Based on Your Results)\n",
    "print(\"\\n🔍 STEP 8: SEARCHING FOR TPC-H DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Since we found databricks-datasets/, let's explore it thoroughly\n",
    "try:\n",
    "    datasets = dbutils.fs.ls(\"/databricks-datasets/\")\n",
    "    print(f\"\\n📊 Found {len(datasets)} dataset categories:\")\n",
    "    \n",
    "    # Look specifically for TPC-H related datasets\n",
    "    tpch_found = False\n",
    "    for item in datasets:\n",
    "        dataset_name = item.name.lower()\n",
    "        if 'tpch' in dataset_name or 'tpc-h' in dataset_name or 'tpc_h' in dataset_name:\n",
    "            print(f\"  🎯 FOUND TPC-H: {item.name}\")\n",
    "            tpch_found = True\n",
    "            \n",
    "            # Explore TPC-H contents\n",
    "            try:\n",
    "                tpch_contents = dbutils.fs.ls(f\"/databricks-datasets/{item.name}\")\n",
    "                print(f\"     📋 TPC-H Contents ({len(tpch_contents)} items):\")\n",
    "                for tpch_item in tpch_contents[:10]:  # Show first 10\n",
    "                    print(f\"       📄 {tpch_item.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ❌ Error exploring TPC-H: {e}\")\n",
    "    \n",
    "    if not tpch_found:\n",
    "        print(\"\\n🔍 TPC-H not found by name, checking all datasets...\")\n",
    "        for item in datasets[:20]:  # Check first 20 datasets\n",
    "            print(f\"  📁 {item.name}\")\n",
    "        \n",
    "        # Let's check if there are any benchmark or sample datasets\n",
    "        benchmark_keywords = ['benchmark', 'sample', 'demo', 'test', 'retail', 'sales']\n",
    "        print(f\"\\n🎯 Looking for benchmark/sample datasets:\")\n",
    "        for item in datasets:\n",
    "            for keyword in benchmark_keywords:\n",
    "                if keyword in item.name.lower():\n",
    "                    print(f\"  📊 Potential dataset: {item.name}\")\n",
    "                    break\n",
    "    \n",
    "    # Also check for any SQL or relational datasets\n",
    "    print(f\"\\n🔍 Looking for SQL/Relational datasets:\")\n",
    "    sql_keywords = ['sql', 'db', 'table', 'relational']\n",
    "    for item in datasets:\n",
    "        for keyword in sql_keywords:\n",
    "            if keyword in item.name.lower():\n",
    "                print(f\"  🗄️ SQL-related: {item.name}\")\n",
    "                break\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error exploring datasets: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Step 8 Complete: TPC-H and benchmark dataset search finished\")\n",
    "\n",
    "print(f\"\\n🎉 DBFS EXPLORATION COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"📝 Summary of completed steps:\")\n",
    "print(\"  ✅ Step 1: Root structure explored\")\n",
    "print(\"  ✅ Step 2: Sample datasets discovered\")\n",
    "print(\"  ✅ Step 3: FileStore examined\")\n",
    "print(\"  ✅ Step 4: Mounted storage checked\")\n",
    "print(\"  ✅ Step 5: Utility functions tested\")\n",
    "print(\"  ✅ Step 6: Commands reference provided\")\n",
    "print(\"  ✅ Step 7: Health check completed\")\n",
    "print(\"  ✅ Step 8: TPC-H dataset search finished\")\n",
    "\n",
    "print(f\"\\n📝 Next Steps:\")\n",
    "print(\"  1. ✅ DBFS exploration successful\")\n",
    "print(\"  2. 🔍 Dataset discovery complete\")\n",
    "print(\"  3. 🚀 Ready for Unity Catalog and Delta Lake learning\")\n",
    "print(\"  4. 📊 Time to import the 2-hour crash course notebook!\")\n",
    "print(\"\\nYou can now navigate and work with the Databricks File System confidently! 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7047a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Explanation: Understanding DBFS Root Folders\n",
    "print(\"\\n📚 STEP 1 EXPLANATION: UNDERSTANDING YOUR DBFS FOLDERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "folder_explanations = {\n",
    "    \"Volume/\": {\n",
    "        \"purpose\": \"Unity Catalog Volumes (lowercase 'v')\",\n",
    "        \"description\": \"Legacy or alternative volume access path\",\n",
    "        \"usage\": \"Less commonly used, prefer /Volumes/\",\n",
    "        \"importance\": \"🟡 Medium - Legacy access\"\n",
    "    },\n",
    "    \"Volumes/\": {\n",
    "        \"purpose\": \"Unity Catalog Volumes (primary)\",\n",
    "        \"description\": \"Modern Databricks managed storage for Unity Catalog\",\n",
    "        \"usage\": \"Primary way to access Unity Catalog volumes\",\n",
    "        \"importance\": \"🟢 High - Modern Unity Catalog feature\"\n",
    "    },\n",
    "    \"databricks-datasets/\": {\n",
    "        \"purpose\": \"Sample Datasets\",\n",
    "        \"description\": \"Pre-loaded sample data provided by Databricks\",\n",
    "        \"usage\": \"Learning, testing, demos (includes potential TPC-H data)\",\n",
    "        \"importance\": \"🟢 High - Essential for learning and your solution accelerator\"\n",
    "    },\n",
    "    \"databricks-results/\": {\n",
    "        \"purpose\": \"Query Results Storage\",\n",
    "        \"description\": \"Temporary storage for SQL query results and downloads\",\n",
    "        \"usage\": \"System-managed, stores query outputs\",\n",
    "        \"importance\": \"🟡 Medium - System managed\"\n",
    "    },\n",
    "    \"volume/\": {\n",
    "        \"purpose\": \"Volume Access (lowercase)\",\n",
    "        \"description\": \"Alternative lowercase access to volumes\",\n",
    "        \"usage\": \"Alternative path, prefer /Volumes/\",\n",
    "        \"importance\": \"🟡 Medium - Alternative access\"\n",
    "    },\n",
    "    \"volumes/\": {\n",
    "        \"purpose\": \"Volume Access (lowercase plural)\",\n",
    "        \"description\": \"Another alternative access to volumes\",\n",
    "        \"usage\": \"Alternative path, prefer /Volumes/\",\n",
    "        \"importance\": \"🟡 Medium - Alternative access\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n📁 FOLDER BREAKDOWN:\")\n",
    "for folder, info in folder_explanations.items():\n",
    "    print(f\"\\n📂 /{folder}\")\n",
    "    print(f\"   🎯 Purpose: {info['purpose']}\")\n",
    "    print(f\"   📝 Description: {info['description']}\")\n",
    "    print(f\"   🔧 Usage: {info['usage']}\")\n",
    "    print(f\"   {info['importance']}\")\n",
    "\n",
    "print(f\"\\n🚀 KEY INSIGHTS FOR YOUR SOLUTION ACCELERATOR:\")\n",
    "print(\"   ✅ databricks-datasets/ - Your source for sample data and potential TPC-H\")\n",
    "print(\"   ✅ Volumes/ - Modern Unity Catalog storage (enterprise feature)\")\n",
    "print(\"   ✅ Multiple volume paths - Databricks provides flexibility\")\n",
    "print(\"   ⚠️ Notice: No /FileStore/ or /mnt/ in root - this workspace config\")\n",
    "\n",
    "print(f\"\\n💡 WHAT THIS TELLS US:\")\n",
    "print(\"   🏢 Modern Databricks workspace - Has Unity Catalog features\")\n",
    "print(\"   📊 Sample data available - Perfect for learning and testing\")\n",
    "print(\"   🔧 Enterprise ready - Volume support indicates advanced features\")\n",
    "print(\"   🎯 Clean setup - Minimal clutter, ready for development\")\n",
    "\n",
    "print(f\"\\n📝 MISSING FOLDERS (normal for new workspaces):\")\n",
    "missing_folders = [\n",
    "    (\"/FileStore/\", \"File uploads via UI - will appear when you upload files\"),\n",
    "    (\"/mnt/\", \"Mounted external storage - appears when you mount Azure Data Lake\"),\n",
    "    (\"/tmp/\", \"Temporary files - created as needed\"),\n",
    "    (\"/user/\", \"User directories - created when users access workspace\")\n",
    "]\n",
    "\n",
    "for folder, explanation in missing_folders:\n",
    "    print(f\"   📁 {folder} - {explanation}\")\n",
    "\n",
    "print(\"\\n✅ Step 1 Extended: DBFS folder structure explained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da055e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 Follow-up: Deep Dive into Business Datasets (Real Stories!)\n",
    "print(\"\\n🎉 STEP 2 FOLLOW-UP: BUSINESS DATASETS DEEP DIVE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"🎯 FOCUSING ON REAL BUSINESS STORIES:\")\n",
    "print(\"   ✅ retail-org/ - Real retail business structure\")\n",
    "print(\"   ✅ online_retail/ - Transaction patterns for online channel\")\n",
    "print(\"   ✅ amazon/ - E-commerce customer/product relationships\")\n",
    "print(\"   ❌ Skipping TPC-H - Synthetic data doesn't tell stories\")\n",
    "\n",
    "# Explore retail-org/ in detail\n",
    "try:\n",
    "    print(\"\\n🛒 EXPLORING RETAIL-ORG DATASET:\")\n",
    "    retail_contents = dbutils.fs.ls(\"/databricks-datasets/retail-org/\")\n",
    "    print(f\"   Found {len(retail_contents)} items in retail-org:\")\n",
    "    \n",
    "    for item in retail_contents:\n",
    "        if item.isDir:\n",
    "            print(f\"     📁 {item.name}\")\n",
    "            # Detailed peek at business-relevant folders\n",
    "            try:\n",
    "                folder_contents = dbutils.fs.ls(f\"/databricks-datasets/retail-org/{item.name}\")\n",
    "                print(f\"        📊 {len(folder_contents)} files - \", end=\"\")\n",
    "                \n",
    "                # Show file types for business context\n",
    "                if len(folder_contents) > 0:\n",
    "                    sample_file = folder_contents[0].name\n",
    "                    if 'csv' in sample_file.lower():\n",
    "                        print(\"CSV data files\")\n",
    "                    elif 'parquet' in sample_file.lower():\n",
    "                        print(\"Parquet data files\")\n",
    "                    else:\n",
    "                        print(\"Data files\")\n",
    "                else:\n",
    "                    print(\"Empty folder\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"        ❌ Error: {e}\")\n",
    "        else:\n",
    "            print(f\"     📄 {item.name}\")\n",
    "\n",
    "    # Focus on key business folders\n",
    "    business_folders = ['customers', 'active_promotions', 'loyalty_segments', 'company_employees']\n",
    "    print(f\"\\n🎯 KEY BUSINESS FOLDERS:\")\n",
    "    for folder in business_folders:\n",
    "        try:\n",
    "            folder_path = f\"/databricks-datasets/retail-org/{folder}/\"\n",
    "            folder_contents = dbutils.fs.ls(folder_path)\n",
    "            print(f\"     📊 {folder}/ - {len(folder_contents)} files\")\n",
    "            \n",
    "            # Show sample file for business context\n",
    "            if len(folder_contents) > 0:\n",
    "                sample_file = folder_contents[0].name\n",
    "                print(f\"        Sample: {sample_file}\")\n",
    "        except:\n",
    "            print(f\"     ⚠️ {folder}/ - Not found or empty\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error exploring retail-org: {e}\")\n",
    "\n",
    "# Explore online_retail/ dataset\n",
    "try:\n",
    "    print(\"\\n🛍️ EXPLORING ONLINE_RETAIL DATASET:\")\n",
    "    online_retail_contents = dbutils.fs.ls(\"/databricks-datasets/online_retail/\")\n",
    "    print(f\"   Found {len(online_retail_contents)} items in online_retail:\")\n",
    "    \n",
    "    for item in online_retail_contents:\n",
    "        item_type = \"📁\" if item.isDir else \"📄\"\n",
    "        size_info = f\" ({item.size} bytes)\" if not item.isDir else \"\"\n",
    "        print(f\"     {item_type} {item.name}{size_info}\")\n",
    "        \n",
    "        # If it's a directory, explore it\n",
    "        if item.isDir:\n",
    "            try:\n",
    "                sub_contents = dbutils.fs.ls(f\"/databricks-datasets/online_retail/{item.name}\")\n",
    "                print(f\"        📊 Contains {len(sub_contents)} files\")\n",
    "                if len(sub_contents) > 0:\n",
    "                    print(f\"        Sample: {sub_contents[0].name}\")\n",
    "            except:\n",
    "                print(f\"        ⚠️ Couldn't explore contents\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error exploring online_retail: {e}\")\n",
    "\n",
    "# Explore amazon/ dataset\n",
    "try:\n",
    "    print(\"\\n📦 EXPLORING AMAZON E-COMMERCE DATASET:\")\n",
    "    amazon_contents = dbutils.fs.ls(\"/databricks-datasets/amazon/\")\n",
    "    print(f\"   Found {len(amazon_contents)} items in amazon:\")\n",
    "    \n",
    "    for item in amazon_contents:\n",
    "        item_type = \"📁\" if item.isDir else \"📄\"\n",
    "        size_info = f\" ({item.size} bytes)\" if not item.isDir else \"\"\n",
    "        print(f\"     {item_type} {item.name}{size_info}\")\n",
    "        \n",
    "        # If it's a directory, explore it for e-commerce patterns\n",
    "        if item.isDir:\n",
    "            try:\n",
    "                sub_contents = dbutils.fs.ls(f\"/databricks-datasets/amazon/{item.name}\")\n",
    "                print(f\"        📊 Contains {len(sub_contents)} files\")\n",
    "                \n",
    "                # Look for data patterns\n",
    "                data_files = [f for f in sub_contents if not f.isDir]\n",
    "                if data_files:\n",
    "                    print(f\"        Data files: {len(data_files)}\")\n",
    "                    print(f\"        Sample: {data_files[0].name}\")\n",
    "            except:\n",
    "                print(f\"        ⚠️ Couldn't explore contents\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error exploring amazon: {e}\")\n",
    "\n",
    "print(f\"\\n🎯 PERFECT BUSINESS STORY DATASETS:\")\n",
    "print(\"   ✅ retail-org/ - Complete retail business (customers, loyalty, promotions)\")\n",
    "print(\"   ✅ online_retail/ - Transaction patterns and customer behavior\")\n",
    "print(\"   ✅ amazon/ - E-commerce product/customer relationships\")\n",
    "\n",
    "print(f\"\\n📊 YOUR SOLUTION ACCELERATOR STORY:\")\n",
    "story = \"\"\"\n",
    "🏪 RETAIL BUSINESS SCENARIO:\n",
    "├── 🛒 Physical Stores (retail-org data)\n",
    "│   ├── Customer demographics and loyalty programs\n",
    "│   ├── Employee management and store operations\n",
    "│   ├── Promotional campaigns and effectiveness\n",
    "│   └── Multi-location retail organization\n",
    "├── 🛍️ Online Channel (online_retail data)\n",
    "│   ├── E-commerce transaction patterns\n",
    "│   ├── Digital customer behavior\n",
    "│   ├── Product performance online\n",
    "│   └── Cross-channel customer journey\n",
    "└── 🎯 Market Intelligence (amazon patterns)\n",
    "    ├── Competitive product analysis\n",
    "    ├── Customer review patterns\n",
    "    ├── E-commerce best practices\n",
    "    └── Market trend insights\n",
    "\"\"\"\n",
    "print(story)\n",
    "\n",
    "print(f\"\\n🚀 WHY THESE DATASETS TELL GREAT STORIES:\")\n",
    "print(\"   📈 Real business challenges - Multi-channel retail operations\")\n",
    "print(\"   👥 Relatable scenarios - Customer loyalty, promotions, online shopping\")\n",
    "print(\"   💰 Clear ROI - Revenue optimization, customer retention, channel performance\")\n",
    "print(\"   🎯 Executive appeal - Practical business insights, not synthetic benchmarks\")\n",
    "\n",
    "print(f\"\\n✅ Step 2 Follow-up Complete: Business datasets analyzed\")\n",
    "print(f\"🎊 READY TO BUILD A COMPELLING SOLUTION ACCELERATOR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Follow-up: Create FileStore Structure for Your Project\n",
    "print(\"\\n🏗️ STEP 3 FOLLOW-UP: CREATING FILESTORE STRUCTURE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"🎯 CREATING ORGANIZED FILESTORE FOR SOLUTION ACCELERATOR:\")\n",
    "\n",
    "# Define the folder structure for your project\n",
    "project_folders = [\n",
    "    \"/FileStore/\",\n",
    "    \"/FileStore/solution_accelerator/\",\n",
    "    \"/FileStore/solution_accelerator/sample_data/\",\n",
    "    \"/FileStore/solution_accelerator/sample_data/products/\",\n",
    "    \"/FileStore/solution_accelerator/sample_data/customers/\",\n",
    "    \"/FileStore/solution_accelerator/sample_data/orders/\",\n",
    "    \"/FileStore/solution_accelerator/retail_data/\",\n",
    "    \"/FileStore/tables/\",\n",
    "    \"/FileStore/shared_uploads/\"\n",
    "]\n",
    "\n",
    "# Create the folder structure\n",
    "print(\"\\n📁 Creating project folder structure:\")\n",
    "for folder in project_folders:\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(folder)\n",
    "        print(f\"   ✅ Created: {folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ {folder}: {str(e)}\")\n",
    "\n",
    "# Create a README file in the main project folder\n",
    "readme_content = \"\"\"\n",
    "# Solution Accelerator Data Upload Area\n",
    "\n",
    "## Folder Structure:\n",
    "- sample_data/products/   - Upload Product_samples.csv here\n",
    "- sample_data/customers/  - Upload customer data when generated\n",
    "- sample_data/orders/     - Upload order data when generated\n",
    "- retail_data/           - Upload any real retail dataset files\n",
    "\n",
    "## Upload Instructions:\n",
    "1. Via Databricks UI: Data > Create > Upload File\n",
    "2. Via drag-and-drop: Drag files into any notebook\n",
    "3. Files uploaded via UI go to /FileStore/tables/\n",
    "4. Files uploaded via drag-drop go to /FileStore/shared_uploads/\n",
    "\n",
    "Created: {date}\n",
    "\"\"\".format(date=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "readme_path = \"/FileStore/solution_accelerator/README.txt\"\n",
    "try:\n",
    "    dbutils.fs.put(readme_path, readme_content, overwrite=True)\n",
    "    print(f\"\\n📄 Created project README: {readme_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Could not create README: {e}\")\n",
    "\n",
    "# Verify the structure was created\n",
    "print(f\"\\n🔍 Verifying FileStore structure:\")\n",
    "try:\n",
    "    filestore_contents = dbutils.fs.ls(\"/FileStore/\")\n",
    "    for item in filestore_contents:\n",
    "        item_type = \"📁\" if item.isDir else \"📄\"\n",
    "        print(f\"   {item_type} {item.name}\")\n",
    "        \n",
    "        # Show contents of solution_accelerator folder\n",
    "        if item.name == \"solution_accelerator/\" and item.isDir:\n",
    "            try:\n",
    "                sa_contents = dbutils.fs.ls(\"/FileStore/solution_accelerator/\")\n",
    "                for sa_item in sa_contents:\n",
    "                    sa_type = \"📁\" if sa_item.isDir else \"📄\"\n",
    "                    print(f\"     {sa_type} {sa_item.name}\")\n",
    "                    \n",
    "                    # Show sample_data contents\n",
    "                    if sa_item.name == \"sample_data/\" and sa_item.isDir:\n",
    "                        sample_contents = dbutils.fs.ls(\"/FileStore/solution_accelerator/sample_data/\")\n",
    "                        for sample_item in sample_contents:\n",
    "                            sample_type = \"📁\" if sample_item.isDir else \"📄\"\n",
    "                            print(f\"       {sample_type} {sample_item.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ❌ Error exploring solution_accelerator: {e}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error verifying structure: {e}\")\n",
    "\n",
    "print(f\"\\n🎯 HOW TO UPLOAD YOUR PRODUCT_SAMPLES.CSV:\")\n",
    "upload_instructions = \"\"\"\n",
    "OPTION 1 - Databricks UI Upload:\n",
    "1. Click 'Data' in left sidebar\n",
    "2. Click 'Create' button\n",
    "3. Select 'Upload File'\n",
    "4. Browse to C:\\\\temp\\\\samples\\\\Product_samples.csv\n",
    "5. Upload (will go to /FileStore/tables/)\n",
    "6. Move to organized folder if needed\n",
    "\n",
    "OPTION 2 - Drag and Drop:\n",
    "1. Open this notebook in Databricks\n",
    "2. Open Windows Explorer to C:\\\\temp\\\\samples\\\\\n",
    "3. Drag Product_samples.csv into this notebook\n",
    "4. File will appear in /FileStore/shared_uploads/\n",
    "5. Move to /FileStore/solution_accelerator/sample_data/products/\n",
    "\n",
    "OPTION 3 - Programmatic Upload (Advanced):\n",
    "Use dbutils.fs.cp() to copy from local path (if accessible)\n",
    "\"\"\"\n",
    "\n",
    "print(upload_instructions)\n",
    "\n",
    "print(f\"\\n🎯 UPLOAD TARGETS FOR YOUR FILES:\")\n",
    "print(\"   📄 Product_samples.csv → /FileStore/solution_accelerator/sample_data/products/\")\n",
    "print(\"   📊 Future customer data → /FileStore/solution_accelerator/sample_data/customers/\")\n",
    "print(\"   🛒 Future order data → /FileStore/solution_accelerator/sample_data/orders/\")\n",
    "print(\"   📈 Retail datasets → /FileStore/solution_accelerator/retail_data/\")\n",
    "\n",
    "print(f\"\\n✅ FileStore structure created and ready for uploads!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Update: Check for Your Uploaded File\n",
    "print(\"\\n📤 STEP 3 UPDATE: CHECKING FOR YOUR UPLOADED FILE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"🎯 LOOKING FOR YOUR UPLOADED FILES:\")\n",
    "\n",
    "# Check if FileStore now exists after your upload\n",
    "try:\n",
    "    filestore_contents = dbutils.fs.ls(\"/FileStore/\")\n",
    "    print(f\"\\n📁 FileStore Contents ({len(filestore_contents)} items):\")\n",
    "    \n",
    "    for item in filestore_contents:\n",
    "        item_type = \"📁\" if item.isDir else \"📄\"\n",
    "        size = f\"({item.size} bytes)\" if not item.isDir else \"\"\n",
    "        print(f\"   {item_type} {item.name} {size}\")\n",
    "        \n",
    "        # Look specifically for your source_data folder\n",
    "        if \"source_data\" in item.name:\n",
    "            print(f\"   🎯 Found your source_data folder!\")\n",
    "            \n",
    "            # Explore the source_data folder\n",
    "            try:\n",
    "                source_data_contents = dbutils.fs.ls(f\"/FileStore/{item.name}\")\n",
    "                print(f\"      📊 Contents of {item.name} ({len(source_data_contents)} items):\")\n",
    "                \n",
    "                for source_item in source_data_contents:\n",
    "                    source_type = \"📁\" if source_item.isDir else \"📄\"\n",
    "                    source_size = f\" ({source_item.size} bytes)\" if not source_item.isDir else \"\"\n",
    "                    print(f\"        {source_type} {source_item.name}{source_size}\")\n",
    "                    \n",
    "                    # Check if this is your product_tents.csv file\n",
    "                    if \"product_tents.csv\" in source_item.name.lower() or \"product\" in source_item.name.lower():\n",
    "                        print(f\"        🎯 Found your product CSV file!\")\n",
    "                        \n",
    "                        # Try to read a sample of the file\n",
    "                        file_path = f\"/FileStore/{item.name}{source_item.name}\"\n",
    "                        print(f\"        📋 File path: {file_path}\")\n",
    "                        \n",
    "                        try:\n",
    "                            # Read first few lines to verify structure\n",
    "                            file_content = dbutils.fs.head(file_path, max_bytes=500)\n",
    "                            print(f\"        📖 First few lines:\")\n",
    "                            lines = file_content.split('\\n')[:5]  # Show first 5 lines\n",
    "                            for i, line in enumerate(lines):\n",
    "                                print(f\"           {i+1}: {line}\")\n",
    "                        except Exception as read_error:\n",
    "                            print(f\"        ❌ Could not read file: {read_error}\")\n",
    "                            \n",
    "            except Exception as folder_error:\n",
    "                print(f\"      ❌ Could not explore source_data folder: {folder_error}\")\n",
    "\n",
    "    # Also check common upload locations\n",
    "    common_locations = [\"/FileStore/tables/\", \"/FileStore/shared_uploads/\"]\n",
    "    for location in common_locations:\n",
    "        try:\n",
    "            location_contents = dbutils.fs.ls(location)\n",
    "            if location_contents:\n",
    "                print(f\"\\n📁 {location} ({len(location_contents)} items):\")\n",
    "                for item in location_contents[:3]:  # Show first 3\n",
    "                    item_type = \"📁\" if item.isDir else \"📄\"\n",
    "                    size = f\" ({item.size} bytes)\" if not item.isDir else \"\"\n",
    "                    print(f\"     {item_type} {item.name}{size}\")\n",
    "                    \n",
    "                    if \"product\" in item.name.lower():\n",
    "                        print(f\"     🎯 Found product-related file: {item.name}\")\n",
    "        except:\n",
    "            print(f\"\\n📁 {location}: Empty or doesn't exist\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking FileStore: {e}\")\n",
    "\n",
    "print(f\"\\n🎯 WORKING WITH YOUR UPLOADED FILE:\")\n",
    "working_instructions = \"\"\"\n",
    "Now that you've uploaded your file, you can:\n",
    "\n",
    "1. 📊 Read the CSV file into a DataFrame:\n",
    "   df = spark.read.csv(\"/FileStore/source_data/product_tents.csv\", header=True, inferSchema=True)\n",
    "   df.show()\n",
    "\n",
    "2. 🔍 Explore the data structure:\n",
    "   df.printSchema()\n",
    "   df.count()\n",
    "   df.columns\n",
    "\n",
    "3. 📈 Create Delta tables from your data:\n",
    "   df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products_sample\")\n",
    "\n",
    "4. 🛒 Use this as basis for retail business scenarios:\n",
    "   # Filter by brand, analyze date patterns, etc.\n",
    "   df.filter(df.BrandName == \"Fabrikam\").show()\n",
    "\"\"\"\n",
    "\n",
    "print(working_instructions)\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS WITH YOUR DATA:\")\n",
    "print(\"   1. ✅ File uploaded successfully to FileStore\")\n",
    "print(\"   2. 📊 Ready to create Delta tables for Unity Catalog learning\")\n",
    "print(\"   3. 🎯 Perfect foundation for your retail business scenarios\")\n",
    "print(\"   4. 📈 Use this data in the 2-hour crash course notebook\")\n",
    "\n",
    "print(f\"\\n✅ Upload verification complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
