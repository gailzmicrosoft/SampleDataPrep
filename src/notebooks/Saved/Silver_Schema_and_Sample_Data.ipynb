{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54c02b1",
   "metadata": {},
   "source": [
    "# Bronze to Silver Schema Analysis\n",
    "\n",
    "**Objective**: Analyze schema of RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_LH_silver, and come up with sample data generation strategy and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b511150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell 1: Environment Setup and Configuration\n",
    "\n",
    "# Environment Setup and Configuration\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Configuration for Silver Retail Data Model Analysis\n",
    "print(\"🛍️ FABRIC RETAIL DATA MODEL - SAMPLE DATA GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Target silver lakehouse (your deployed retail model)\n",
    "SILVER_LAKEHOUSE = \"RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_LH_silver\"\n",
    "\n",
    "# Key retail entities we expect to find\n",
    "SILVER_MAIN_ENTITIES = ['customer', 'order', 'product', 'brand', 'store', 'inventory', 'sales']\n",
    "\n",
    "# Sample data generation parameters\n",
    "SAMPLE_DATA_CONFIG = {\n",
    "    \"customers\": 1000,      # Number of sample customers\n",
    "    \"products\": 500,        # Number of sample products\n",
    "    \"orders\": 2000,         # Number of sample orders\n",
    "    \"stores\": 50,           # Number of sample stores\n",
    "    \"brands\": 100,          # Number of sample brands\n",
    "    \"date_range_days\": 365  # Historical data range (1 year)\n",
    "}\n",
    "\n",
    "print(f\"✅ Configuration loaded\")\n",
    "print(f\"🎯 Target: {SILVER_LAKEHOUSE}\")\n",
    "print(f\"📊 Sample data scale: {SAMPLE_DATA_CONFIG}\")\n",
    "print(f\"📅 Analysis timestamp: {datetime.now().isoformat()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff33061",
   "metadata": {},
   "source": [
    "## Step 1: Discover Silver Layer Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell 2: Discover Silver Layer Structure\n",
    "\n",
    "# STEP 1: Discover Silver Layer Structure - Simplified & Complete\n",
    "print(\"🎯 ANALYZING SILVER LAYER STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize variables for capturing analysis\n",
    "analysis_output_lines = []\n",
    "silver_schema_analysis = []\n",
    "\n",
    "def capture_print(text):\n",
    "    \"\"\"Capture print output for saving to file\"\"\"\n",
    "    print(text)\n",
    "    analysis_output_lines.append(text)\n",
    "\n",
    "try:\n",
    "    # Get ALL tables from the silver lakehouse  \n",
    "    capture_print(\"🔍 Discovering all tables in silver lakehouse...\")\n",
    "    \n",
    "    # Try multiple methods to get all tables\n",
    "    try:\n",
    "        # Method 1: SHOW TABLES (most reliable)\n",
    "        silver_tables_df = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "        \n",
    "        # Handle different column names\n",
    "        table_col = None\n",
    "        for col in ['tableName', 'table_name', 'name']:\n",
    "            if col in silver_tables_df.columns:\n",
    "                table_col = col\n",
    "                break\n",
    "        \n",
    "        if table_col is None and len(silver_tables_df.columns) > 0:\n",
    "            table_col = silver_tables_df.columns[0]\n",
    "            \n",
    "        silver_tables = silver_tables_df[table_col].tolist() if table_col else []\n",
    "        \n",
    "    except Exception as e:\n",
    "        capture_print(f\"⚠️ SHOW TABLES failed: {str(e)}\")\n",
    "        # Method 2: Use catalog API\n",
    "        try:\n",
    "            silver_tables = [table.name for table in spark.catalog.listTables()]\n",
    "        except Exception as e2:\n",
    "            capture_print(f\"⚠️ Catalog API failed: {str(e2)}\")\n",
    "            silver_tables = []\n",
    "    \n",
    "    capture_print(f\"✅ Found {len(silver_tables)} tables total\")\n",
    "    \n",
    "    if len(silver_tables) == 0:\n",
    "        capture_print(\"📋 No tables found - silver lakehouse appears to be empty\")\n",
    "        capture_print(\"💡 This is expected if this is the first run\")\n",
    "        silver_summary = {\"total_tables\": 0}\n",
    "        phase1_key_tables = {}\n",
    "    else:\n",
    "        # PHASE 1 KEY TABLES IDENTIFICATION\n",
    "        capture_print(f\"\\n🎯 PHASE 1 KEY TABLES IDENTIFICATION\")\n",
    "        capture_print(\"=\" * 45)\n",
    "        \n",
    "        # Define the 8 key tables for Phase 1 sample data generation\n",
    "        PHASE1_TARGET_TABLES = ['Party', 'Location', 'Customer', 'Brand', 'Order', 'OrderLine', 'Invoice', 'InvoiceLine']\n",
    "        \n",
    "        # Find matching tables (case-insensitive)\n",
    "        phase1_key_tables = {}\n",
    "        phase1_found = []\n",
    "        \n",
    "        for target in PHASE1_TARGET_TABLES:\n",
    "            # Look for exact match first, then case-insensitive\n",
    "            found_table = None\n",
    "            for table_name in silver_tables:\n",
    "                if table_name == target:\n",
    "                    found_table = table_name\n",
    "                    break\n",
    "                elif table_name.lower() == target.lower():\n",
    "                    found_table = table_name\n",
    "                    break\n",
    "            \n",
    "            if found_table:\n",
    "                phase1_found.append(found_table)\n",
    "                capture_print(f\"✅ Found: {target} -> {found_table}\")\n",
    "            else:\n",
    "                capture_print(f\"❌ Missing: {target}\")\n",
    "        \n",
    "        capture_print(f\"\\n📊 Phase 1 Status: {len(phase1_found)}/{len(PHASE1_TARGET_TABLES)} key tables found\")\n",
    "        \n",
    "        # SIMPLIFIED ANALYSIS: Just table name and column count\n",
    "        capture_print(f\"\\n📊 ALL TABLES SUMMARY (Name & Column Count)\")\n",
    "        capture_print(\"=\" * 50)\n",
    "        \n",
    "        table_info = []\n",
    "        \n",
    "        for i, table_name in enumerate(sorted(silver_tables), 1):\n",
    "            try:\n",
    "                # Get table structure efficiently\n",
    "                df = spark.table(table_name)\n",
    "                column_count = len(df.columns)\n",
    "                row_count = df.count()\n",
    "                columns = df.columns\n",
    "                \n",
    "                # Mark if this is a Phase 1 key table\n",
    "                is_phase1_key = table_name in phase1_found\n",
    "                marker = \"🎯\" if is_phase1_key else \"  \"\n",
    "                \n",
    "                # Simple output format\n",
    "                capture_print(f\"{marker} {i:2d}. {table_name:<30} | {column_count:2d} columns | {row_count:,} rows\")\n",
    "                \n",
    "                # Store for CSV export\n",
    "                table_info_entry = {\n",
    "                    \"table_number\": i,\n",
    "                    \"table_name\": table_name,\n",
    "                    \"column_count\": column_count,\n",
    "                    \"row_count\": row_count,\n",
    "                    \"columns\": columns,\n",
    "                    \"is_phase1_key\": is_phase1_key\n",
    "                }\n",
    "                table_info.append(table_info_entry)\n",
    "                \n",
    "                # Store Phase 1 key table details separately\n",
    "                if is_phase1_key:\n",
    "                    phase1_key_tables[table_name] = {\n",
    "                        \"columns\": columns,\n",
    "                        \"column_count\": column_count,\n",
    "                        \"row_count\": row_count,\n",
    "                        \"schema_details\": table_info_entry\n",
    "                    }\n",
    "                \n",
    "            except Exception as e:\n",
    "                capture_print(f\"   {i:2d}. {table_name:<30} | ERROR: {str(e)}\")\n",
    "                table_info.append({\n",
    "                    \"table_number\": i,\n",
    "                    \"table_name\": table_name,\n",
    "                    \"column_count\": 0,\n",
    "                    \"row_count\": 0,\n",
    "                    \"columns\": [],\n",
    "                    \"error\": str(e),\n",
    "                    \"is_phase1_key\": table_name in phase1_found\n",
    "                })\n",
    "\n",
    "        # 🔍 DETAILED STRUCTURE PRINTING FOR KEY PHASE 1 TABLES\n",
    "        capture_print(f\"\\n🔍 DETAILED STRUCTURE FOR PHASE 1 KEY TABLES\")\n",
    "        capture_print(\"=\" * 55)\n",
    "        capture_print(\"📋 This detailed output will be shared to help with data generation in cells 3-5\")\n",
    "        capture_print()\n",
    "        \n",
    "        # Get detailed schema information for each key table\n",
    "        for table_name in PHASE1_TARGET_TABLES:\n",
    "            if table_name in phase1_key_tables:\n",
    "                capture_print(f\"📊 TABLE: {table_name}\")\n",
    "                capture_print(\"-\" * (10 + len(table_name)))\n",
    "                \n",
    "                try:\n",
    "                    # Get DataFrame to analyze schema\n",
    "                    df = spark.table(table_name)\n",
    "                    \n",
    "                    # Print column details with data types\n",
    "                    capture_print(f\"Columns ({len(df.columns)}):\")\n",
    "                    for field in df.schema.fields:\n",
    "                        nullable = \"NULL\" if field.nullable else \"NOT NULL\"\n",
    "                        capture_print(f\"  • {field.name:<25} | {str(field.dataType):<20} | {nullable}\")\n",
    "                    \n",
    "                    # Print current row count\n",
    "                    row_count = df.count()\n",
    "                    capture_print(f\"Current rows: {row_count:,}\")\n",
    "                    \n",
    "                    # If table has data, show sample\n",
    "                    if row_count > 0:\n",
    "                        capture_print(\"Sample data (first 3 rows):\")\n",
    "                        sample_df = df.limit(3).toPandas()\n",
    "                        for idx, row in sample_df.iterrows():\n",
    "                            capture_print(f\"  Row {idx+1}: {dict(row)}\")\n",
    "                    else:\n",
    "                        capture_print(\"Status: Empty table (ready for data generation)\")\n",
    "                    \n",
    "                    capture_print()  # Empty line between tables\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    capture_print(f\"❌ Error analyzing {table_name}: {str(e)}\")\n",
    "                    capture_print()\n",
    "            else:\n",
    "                capture_print(f\"❌ TABLE: {table_name} - NOT FOUND\")\n",
    "                capture_print(\"-\" * (15 + len(table_name)))\n",
    "                capture_print(\"Status: Table does not exist in silver lakehouse\")\n",
    "                capture_print()\n",
    "        \n",
    "        # Summary\n",
    "        capture_print(f\"\\n📋 DISCOVERY COMPLETE\")\n",
    "        capture_print(\"=\" * 30)\n",
    "        capture_print(f\"✅ Total tables discovered: {len(silver_tables)}\")\n",
    "        capture_print(f\"🎯 Phase 1 key tables found: {len(phase1_key_tables)}/{len(PHASE1_TARGET_TABLES)}\")\n",
    "        capture_print(f\"✅ Successfully analyzed: {len([t for t in table_info if 'error' not in t])}\")\n",
    "        if any('error' in t for t in table_info):\n",
    "            error_count = len([t for t in table_info if 'error' in t])\n",
    "            capture_print(f\"⚠️  Tables with errors: {error_count}\")\n",
    "        \n",
    "        # Store results\n",
    "        silver_schema_analysis = table_info\n",
    "        silver_summary = {\n",
    "            \"total_tables\": len(silver_tables),\n",
    "            \"analyzed_successfully\": len([t for t in table_info if 'error' not in t]),\n",
    "            \"tables_with_errors\": len([t for t in table_info if 'error' in t]),\n",
    "            \"table_list\": [t[\"table_name\"] for t in table_info],\n",
    "            \"phase1_key_tables\": list(phase1_key_tables.keys()),\n",
    "            \"phase1_found_count\": len(phase1_key_tables),\n",
    "            \"phase1_target_count\": len(PHASE1_TARGET_TABLES)\n",
    "        }\n",
    "\n",
    "except Exception as e:\n",
    "    capture_print(f\"❌ Critical error accessing silver lakehouse: {str(e)}\")\n",
    "    capture_print(\"💡 Check if you're connected to the correct lakehouse\")\n",
    "    silver_summary = {\"error\": str(e)}\n",
    "    silver_schema_analysis = []\n",
    "    phase1_key_tables = {}\n",
    "\n",
    "# Final summary\n",
    "analysis_timestamp = datetime.now().isoformat()\n",
    "capture_print(f\"\\n📋 Analysis completed at: {analysis_timestamp}\")\n",
    "\n",
    "# Make key variables available for subsequent cells\n",
    "print(f\"\\n🔧 VARIABLES READY FOR NEXT CELLS\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"✅ silver_schema_analysis: All {len(silver_schema_analysis)} tables\")\n",
    "print(f\"✅ phase1_key_tables: {len(phase1_key_tables)} focused tables\")\n",
    "print(f\"✅ silver_summary: Complete analysis summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f8845",
   "metadata": {},
   "source": [
    "## Step 2: Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79630556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell 3: Foundation Data Generation Functions\n",
    "# UPDATED SAMPLE DATA GENERATION FOR ENTERPRISE SCHEMA\n",
    "print(\"🏢 ENTERPRISE RETAIL DATA MODEL - SAMPLE DATA GENERATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Updated configuration based on discovered schema\n",
    "ENTERPRISE_CONFIG = {\n",
    "    \"parties\": 1200,           # Base parties (customers, retailers, vendors)\n",
    "    \"locations\": 500,          # Geographic locations \n",
    "    \"customers\": 1000,         # Individual customers\n",
    "    \"customer_accounts\": 800,  # Customer accounts (subset of customers)\n",
    "    \"brands\": 50,              # Product brands\n",
    "    \"orders\": 2000,            # Sales orders\n",
    "    \"order_lines\": 8000,       # Order line items (avg 4 per order)\n",
    "    \"invoices\": 1800,          # Invoices (90% of orders)\n",
    "    \"invoice_lines\": 7200,     # Invoice line items\n",
    "    \"date_range_days\": 365     # Historical data range\n",
    "}\n",
    "\n",
    "print(f\"📊 Enterprise scale configuration:\")\n",
    "for key, value in ENTERPRISE_CONFIG.items():\n",
    "    print(f\"  • {key}: {value:,}\")\n",
    "print()\n",
    "\n",
    "def generate_party_data(num_parties=1200):\n",
    "    \"\"\"Generate Party records using company-approved customer data format\"\"\"\n",
    "    print(f\"👥 Generating {num_parties} Party records using company-approved format...\")\n",
    "    \n",
    "    # Company-approved customer names (from customer_data.csv template)\n",
    "    company_approved_customers = [\n",
    "        'Amanda', 'Anna', 'Ashley', 'Brandy', 'Brittany', 'Caroline', 'Catherine', 'Christina', 'Crystal',\n",
    "        'Deborah', 'Donna', 'Elizabeth', 'Frances', 'Jennifer', 'Jessica', 'Kimberly', 'Linda', 'Lisa',\n",
    "        'Mary', 'Melissa', 'Michelle', 'Patricia', 'Rachel', 'Rebecca', 'Sandra', 'Sarah', 'Sharon',\n",
    "        'Stephanie', 'Susan', 'Tracy', 'Angela', 'Brian', 'Christopher', 'Daniel', 'David', 'Gary',\n",
    "        'James', 'Jason', 'Jeffrey', 'John', 'Joseph', 'Kenneth', 'Kevin', 'Mark', 'Michael'\n",
    "    ]\n",
    "    \n",
    "    # Party types for retail model\n",
    "    party_types = ['INDIVIDUAL', 'ORGANIZATION', 'RETAILER', 'VENDOR', 'CARRIER']\n",
    "    \n",
    "    party_data = []\n",
    "    \n",
    "    for i in range(num_parties):\n",
    "        # Use company-approved names with cycling\n",
    "        base_name = company_approved_customers[i % len(company_approved_customers)]\n",
    "        \n",
    "        # Add uniqueness for larger datasets\n",
    "        if i >= len(company_approved_customers):\n",
    "            cycle_num = i // len(company_approved_customers) + 1\n",
    "            party_name = f\"{base_name} {cycle_num}\"\n",
    "        else:\n",
    "            party_name = base_name\n",
    "        \n",
    "        party_data.append({\n",
    "            'PartyId': i + 1,\n",
    "            'PartyTypeId': random.choice(party_types),\n",
    "            'PartyName': party_name,\n",
    "            'CreatedDate': datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "            'IsActive': True\n",
    "        })\n",
    "    \n",
    "    return party_data\n",
    "\n",
    "def generate_location_data(num_locations=500):\n",
    "    \"\"\"Generate Location records focused on Buffalo NY area (company compliance)\"\"\"\n",
    "    print(f\"📍 Generating {num_locations} Location records (Buffalo NY focus)...\")\n",
    "    \n",
    "    # Buffalo NY area zip codes and neighborhoods (company compliant)\n",
    "    buffalo_areas = [\n",
    "        {'zip': '14201', 'area': 'Downtown Buffalo'},\n",
    "        {'zip': '14202', 'area': 'Elmwood Village'},\n",
    "        {'zip': '14203', 'area': 'South Buffalo'},\n",
    "        {'zip': '14204', 'area': 'West Side'},\n",
    "        {'zip': '14205', 'area': 'Riverside'},\n",
    "        {'zip': '14206', 'area': 'East Buffalo'},\n",
    "        {'zip': '14207', 'area': 'Seneca-Babcock'},\n",
    "        {'zip': '14208', 'area': 'University Heights'},\n",
    "        {'zip': '14209', 'area': 'Black Rock'},\n",
    "        {'zip': '14210', 'area': 'South Park'},\n",
    "        {'zip': '14211', 'area': 'Riverside'},\n",
    "        {'zip': '14212', 'area': 'East Side'},\n",
    "        {'zip': '14213', 'area': 'East Buffalo'},\n",
    "        {'zip': '14214', 'area': 'North Buffalo'},\n",
    "        {'zip': '14215', 'area': 'North Buffalo'},\n",
    "        {'zip': '14216', 'area': 'North Buffalo'},\n",
    "        {'zip': '14217', 'area': 'Kenmore'},\n",
    "        {'zip': '14218', 'area': 'Kaisertown'},\n",
    "        {'zip': '14219', 'area': 'South Buffalo'},\n",
    "        {'zip': '14220', 'area': 'South Buffalo'}\n",
    "    ]\n",
    "    \n",
    "    location_data = []\n",
    "    \n",
    "    for i in range(num_locations):\n",
    "        area_info = random.choice(buffalo_areas)\n",
    "        \n",
    "        location_data.append({\n",
    "            'LocationId': i + 1,\n",
    "            'LocationName': f\"{area_info['area']} - {i + 1}\",\n",
    "            'Address': f\"{random.randint(1, 9999)} Main St\",\n",
    "            'City': 'Buffalo',\n",
    "            'StateProvince': 'NY',\n",
    "            'PostalCode': area_info['zip'],\n",
    "            'Country': 'USA',\n",
    "            'LocationType': random.choice(['STORE', 'WAREHOUSE', 'OFFICE', 'DISTRIBUTION_CENTER']),\n",
    "            'IsActive': True\n",
    "        })\n",
    "    \n",
    "    return location_data\n",
    "\n",
    "def generate_customer_data(num_customers=1000, party_data=None):\n",
    "    \"\"\"Generate Customer records linked to Party records\"\"\"\n",
    "    print(f\"👤 Generating {num_customers} Customer records...\")\n",
    "    \n",
    "    if not party_data:\n",
    "        print(\"⚠️ No party data provided - generating customers without party linkage\")\n",
    "        party_ids = list(range(1, num_customers + 1))\n",
    "    else:\n",
    "        # Use existing party IDs\n",
    "        party_ids = [p['PartyId'] for p in party_data if p['PartyTypeId'] == 'INDIVIDUAL']\n",
    "        if len(party_ids) < num_customers:\n",
    "            # Extend with additional IDs if needed\n",
    "            party_ids.extend(range(max(party_ids) + 1, max(party_ids) + 1 + (num_customers - len(party_ids))))\n",
    "    \n",
    "    customer_data = []\n",
    "    \n",
    "    for i in range(num_customers):\n",
    "        customer_data.append({\n",
    "            'CustomerId': i + 1,\n",
    "            'PartyId': party_ids[i % len(party_ids)],\n",
    "            'CustomerNumber': f\"CUST{(i + 1):06d}\",\n",
    "            'CustomerType': random.choice(['INDIVIDUAL', 'BUSINESS']),\n",
    "            'Email': f\"customer{i + 1}@example.com\",  # Company compliant domain\n",
    "            'Phone': f\"+1-716-{random.randint(200, 999)}-{random.randint(1000, 9999)}\",  # Buffalo area code\n",
    "            'CreatedDate': datetime.now() - timedelta(days=random.randint(1, 365)),\n",
    "            'IsActive': True\n",
    "        })\n",
    "    \n",
    "    return customer_data\n",
    "\n",
    "def generate_brand_data(num_brands=50):\n",
    "    \"\"\"Generate Brand records for retail products\"\"\"\n",
    "    print(f\"🏷️ Generating {num_brands} Brand records...\")\n",
    "    \n",
    "    # Generic brand names for sample data\n",
    "    brand_names = [\n",
    "        'Premium', 'Classic', 'Elite', 'Select', 'Choice', 'Prime', 'Quality', 'Standard',\n",
    "        'Superior', 'Deluxe', 'Essential', 'Basic', 'Advanced', 'Professional', 'Commercial',\n",
    "        'Industrial', 'Retail', 'Consumer', 'Business', 'Enterprise'\n",
    "    ]\n",
    "    \n",
    "    brand_categories = [\n",
    "        'Electronics', 'Clothing', 'Home & Garden', 'Sports', 'Automotive', 'Health & Beauty',\n",
    "        'Books & Media', 'Toys & Games', 'Food & Beverage', 'Office Supplies'\n",
    "    ]\n",
    "    \n",
    "    brand_data = []\n",
    "    \n",
    "    for i in range(num_brands):\n",
    "        brand_name = brand_names[i % len(brand_names)]\n",
    "        category = brand_categories[i % len(brand_categories)]\n",
    "        \n",
    "        # Add uniqueness for larger datasets\n",
    "        if i >= len(brand_names):\n",
    "            cycle_num = i // len(brand_names) + 1\n",
    "            full_brand_name = f\"{brand_name} {category} {cycle_num}\"\n",
    "        else:\n",
    "            full_brand_name = f\"{brand_name} {category}\"\n",
    "        \n",
    "        brand_data.append({\n",
    "            'BrandId': i + 1,\n",
    "            'BrandName': full_brand_name,\n",
    "            'BrandCode': f\"BR{(i + 1):03d}\",\n",
    "            'Category': category,\n",
    "            'Description': f\"Quality {category.lower()} products from {brand_name}\",\n",
    "            'IsActive': True,\n",
    "            'CreatedDate': datetime.now() - timedelta(days=random.randint(1, 180))\n",
    "        })\n",
    "    \n",
    "    return brand_data\n",
    "\n",
    "# Generate foundation data\n",
    "print(\"🏗️ GENERATING FOUNDATION DATA\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Generate in dependency order\n",
    "parties = generate_party_data(ENTERPRISE_CONFIG['parties'])\n",
    "locations = generate_location_data(ENTERPRISE_CONFIG['locations'])\n",
    "customers = generate_customer_data(ENTERPRISE_CONFIG['customers'], parties)\n",
    "brands = generate_brand_data(ENTERPRISE_CONFIG['brands'])\n",
    "\n",
    "print(f\"\\n✅ Foundation data generated:\")\n",
    "print(f\"  • Parties: {len(parties):,}\")\n",
    "print(f\"  • Locations: {len(locations):,}\")\n",
    "print(f\"  • Customers: {len(customers):,}\")\n",
    "print(f\"  • Brands: {len(brands):,}\")\n",
    "print(f\"\\n🔧 Data ready for order generation in next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell 4: Order System Generation\n",
    "print(\"📦 GENERATING ORDER SYSTEM DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def generate_order_data(num_orders=2000, customers=None, locations=None):\n",
    "    \"\"\"Generate Order records with proper customer and location linkage\"\"\"\n",
    "    print(f\"📋 Generating {num_orders} Order records...\")\n",
    "    \n",
    "    if not customers:\n",
    "        print(\"⚠️ No customer data provided - using default customer IDs\")\n",
    "        customer_ids = list(range(1, 1001))  # Default 1000 customers\n",
    "    else:\n",
    "        customer_ids = [c['CustomerId'] for c in customers]\n",
    "    \n",
    "    if not locations:\n",
    "        print(\"⚠️ No location data provided - using default location IDs\")\n",
    "        location_ids = list(range(1, 501))  # Default 500 locations\n",
    "    else:\n",
    "        location_ids = [l['LocationId'] for l in locations]\n",
    "    \n",
    "    order_statuses = ['PENDING', 'CONFIRMED', 'SHIPPED', 'DELIVERED', 'CANCELLED']\n",
    "    order_types = ['ONLINE', 'IN_STORE', 'PHONE', 'CATALOG']\n",
    "    \n",
    "    order_data = []\n",
    "    \n",
    "    for i in range(num_orders):\n",
    "        # Generate realistic order date (within last year)\n",
    "        order_date = datetime.now() - timedelta(days=random.randint(1, 365))\n",
    "        \n",
    "        # Calculate delivery date (if shipped/delivered)\n",
    "        status = random.choice(order_statuses)\n",
    "        delivery_date = None\n",
    "        if status in ['SHIPPED', 'DELIVERED']:\n",
    "            delivery_date = order_date + timedelta(days=random.randint(1, 14))\n",
    "        \n",
    "        order_data.append({\n",
    "            'OrderId': i + 1,\n",
    "            'OrderNumber': f\"ORD{(i + 1):07d}\",\n",
    "            'CustomerId': random.choice(customer_ids),\n",
    "            'LocationId': random.choice(location_ids),\n",
    "            'OrderDate': order_date,\n",
    "            'OrderType': random.choice(order_types),\n",
    "            'OrderStatus': status,\n",
    "            'TotalAmount': round(random.uniform(25.00, 2500.00), 2),\n",
    "            'TaxAmount': 0,  # Will calculate based on total\n",
    "            'ShippingAmount': round(random.uniform(5.99, 49.99), 2) if status != 'IN_STORE' else 0,\n",
    "            'DeliveryDate': delivery_date,\n",
    "            'CreatedDate': order_date,\n",
    "            'ModifiedDate': order_date + timedelta(hours=random.randint(1, 48))\n",
    "        })\n",
    "    \n",
    "    # Calculate tax amounts (8.25% NY sales tax)\n",
    "    for order in order_data:\n",
    "        tax_rate = 0.0825  # Buffalo NY sales tax rate\n",
    "        order['TaxAmount'] = round(order['TotalAmount'] * tax_rate, 2)\n",
    "        order['TotalAmount'] = round(order['TotalAmount'] + order['TaxAmount'] + order['ShippingAmount'], 2)\n",
    "    \n",
    "    return order_data\n",
    "\n",
    "def generate_order_line_data(orders=None, brands=None, avg_lines_per_order=4):\n",
    "    \"\"\"Generate OrderLine records for each order\"\"\"\n",
    "    if not orders:\n",
    "        print(\"⚠️ No order data provided - cannot generate order lines\")\n",
    "        return []\n",
    "    \n",
    "    total_lines = len(orders) * avg_lines_per_order\n",
    "    print(f\"📝 Generating ~{total_lines:,} OrderLine records ({avg_lines_per_order} avg per order)...\")\n",
    "    \n",
    "    if not brands:\n",
    "        print(\"⚠️ No brand data provided - using default brand IDs\")\n",
    "        brand_ids = list(range(1, 51))  # Default 50 brands\n",
    "    else:\n",
    "        brand_ids = [b['BrandId'] for b in brands]\n",
    "    \n",
    "    order_line_data = []\n",
    "    line_id_counter = 1\n",
    "    \n",
    "    for order in orders:\n",
    "        # Determine number of lines for this order (1-8 lines, weighted toward 3-5)\n",
    "        num_lines = random.choices(\n",
    "            [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "            weights=[5, 10, 20, 25, 25, 10, 3, 2]\n",
    "        )[0]\n",
    "        \n",
    "        order_total = 0\n",
    "        \n",
    "        for line_num in range(1, num_lines + 1):\n",
    "            # Generate line item details\n",
    "            quantity = random.randint(1, 5)\n",
    "            unit_price = round(random.uniform(9.99, 299.99), 2)\n",
    "            line_total = round(quantity * unit_price, 2)\n",
    "            order_total += line_total\n",
    "            \n",
    "            # Generate product description\n",
    "            brand_id = random.choice(brand_ids)\n",
    "            product_names = ['Widget', 'Gadget', 'Tool', 'Device', 'Item', 'Product', 'Component']\n",
    "            product_name = f\"{random.choice(product_names)} {random.randint(100, 999)}\"\n",
    "            \n",
    "            order_line_data.append({\n",
    "                'OrderLineId': line_id_counter,\n",
    "                'OrderId': order['OrderId'],\n",
    "                'LineNumber': line_num,\n",
    "                'ProductSKU': f\"SKU{brand_id:03d}{random.randint(1000, 9999)}\",\n",
    "                'ProductName': product_name,\n",
    "                'BrandId': brand_id,\n",
    "                'Quantity': quantity,\n",
    "                'UnitPrice': unit_price,\n",
    "                'LineTotal': line_total,\n",
    "                'DiscountAmount': round(random.uniform(0, line_total * 0.2), 2) if random.random() < 0.3 else 0,\n",
    "                'CreatedDate': order['OrderDate']\n",
    "            })\n",
    "            \n",
    "            line_id_counter += 1\n",
    "    \n",
    "    return order_line_data\n",
    "\n",
    "def generate_invoice_data(orders=None):\n",
    "    \"\"\"Generate Invoice records (90% of orders get invoiced)\"\"\"\n",
    "    if not orders:\n",
    "        print(\"⚠️ No order data provided - cannot generate invoices\")\n",
    "        return []\n",
    "    \n",
    "    # Only invoice orders that are confirmed, shipped, or delivered\n",
    "    invoiceable_orders = [o for o in orders if o['OrderStatus'] in ['CONFIRMED', 'SHIPPED', 'DELIVERED']]\n",
    "    num_invoices = math.floor(len(invoiceable_orders) * 0.9)  # 90% get invoiced\n",
    "    \n",
    "    print(f\"🧾 Generating {num_invoices:,} Invoice records from {len(invoiceable_orders):,} eligible orders...\")\n",
    "    \n",
    "    selected_orders = random.sample(invoiceable_orders, num_invoices)\n",
    "    invoice_data = []\n",
    "    \n",
    "    for i, order in enumerate(selected_orders):\n",
    "        # Invoice date is typically same day or 1-2 days after order\n",
    "        invoice_date = order['OrderDate'] + timedelta(days=random.randint(0, 2))\n",
    "        \n",
    "        # Due date is typically 30 days from invoice\n",
    "        due_date = invoice_date + timedelta(days=30)\n",
    "        \n",
    "        invoice_data.append({\n",
    "            'InvoiceId': i + 1,\n",
    "            'InvoiceNumber': f\"INV{(i + 1):07d}\",\n",
    "            'OrderId': order['OrderId'],\n",
    "            'CustomerId': order['CustomerId'],\n",
    "            'InvoiceDate': invoice_date,\n",
    "            'DueDate': due_date,\n",
    "            'SubtotalAmount': order['TotalAmount'] - order['TaxAmount'] - order['ShippingAmount'],\n",
    "            'TaxAmount': order['TaxAmount'],\n",
    "            'ShippingAmount': order['ShippingAmount'],\n",
    "            'TotalAmount': order['TotalAmount'],\n",
    "            'PaymentStatus': random.choice(['PENDING', 'PAID', 'OVERDUE', 'PARTIAL']),\n",
    "            'PaymentDate': invoice_date + timedelta(days=random.randint(1, 45)) if random.random() < 0.8 else None,\n",
    "            'CreatedDate': invoice_date\n",
    "        })\n",
    "    \n",
    "    return invoice_data\n",
    "\n",
    "def generate_invoice_line_data(invoices=None, order_lines=None):\n",
    "    \"\"\"Generate InvoiceLine records based on OrderLine data\"\"\"\n",
    "    if not invoices or not order_lines:\n",
    "        print(\"⚠️ Missing invoice or order line data - cannot generate invoice lines\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"📋 Generating InvoiceLine records for {len(invoices):,} invoices...\")\n",
    "    \n",
    "    # Create mapping of OrderId to OrderLines\n",
    "    order_lines_map = {}\n",
    "    for line in order_lines:\n",
    "        order_id = line['OrderId']\n",
    "        if order_id not in order_lines_map:\n",
    "            order_lines_map[order_id] = []\n",
    "        order_lines_map[order_id].append(line)\n",
    "    \n",
    "    invoice_line_data = []\n",
    "    line_id_counter = 1\n",
    "    \n",
    "    for invoice in invoices:\n",
    "        order_id = invoice['OrderId']\n",
    "        \n",
    "        if order_id in order_lines_map:\n",
    "            for order_line in order_lines_map[order_id]:\n",
    "                invoice_line_data.append({\n",
    "                    'InvoiceLineId': line_id_counter,\n",
    "                    'InvoiceId': invoice['InvoiceId'],\n",
    "                    'OrderLineId': order_line['OrderLineId'],\n",
    "                    'LineNumber': order_line['LineNumber'],\n",
    "                    'ProductSKU': order_line['ProductSKU'],\n",
    "                    'ProductName': order_line['ProductName'],\n",
    "                    'Quantity': order_line['Quantity'],\n",
    "                    'UnitPrice': order_line['UnitPrice'],\n",
    "                    'LineTotal': order_line['LineTotal'],\n",
    "                    'DiscountAmount': order_line['DiscountAmount'],\n",
    "                    'CreatedDate': invoice['InvoiceDate']\n",
    "                })\n",
    "                line_id_counter += 1\n",
    "    \n",
    "    return invoice_line_data\n",
    "\n",
    "# Generate order system data\n",
    "print(\"🔄 Generating order system data in dependency order...\")\n",
    "\n",
    "orders = generate_order_data(ENTERPRISE_CONFIG['orders'], customers, locations)\n",
    "order_lines = generate_order_line_data(orders, brands, 4)\n",
    "invoices = generate_invoice_data(orders)\n",
    "invoice_lines = generate_invoice_line_data(invoices, order_lines)\n",
    "\n",
    "print(f\"\\n✅ Order system data generated:\")\n",
    "print(f\"  • Orders: {len(orders):,}\")\n",
    "print(f\"  • Order Lines: {len(order_lines):,}\")\n",
    "print(f\"  • Invoices: {len(invoices):,}\")\n",
    "print(f\"  • Invoice Lines: {len(invoice_lines):,}\")\n",
    "print(f\"\\n🎯 All sample data ready for loading!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell 5: Schema-Aware Data Loading\n",
    "print(\"🎯 SCHEMA-AWARE DATA LOADING\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def load_data_to_table(data_list, table_name, schema_info=None):\n",
    "    \"\"\"Load generated data to silver table with schema awareness\"\"\"\n",
    "    if not data_list:\n",
    "        print(f\"⚠️ No data provided for {table_name}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(f\"📊 Loading {len(data_list):,} records to {table_name}...\")\n",
    "        \n",
    "        # Create DataFrame from generated data\n",
    "        df = spark.createDataFrame(data_list)\n",
    "        \n",
    "        # If we have schema info from discovery, validate columns\n",
    "        if schema_info and 'columns' in schema_info:\n",
    "            expected_columns = schema_info['columns']\n",
    "            actual_columns = df.columns\n",
    "            \n",
    "            print(f\"  🔍 Schema validation:\")\n",
    "            print(f\"    Expected columns: {expected_columns}\")\n",
    "            print(f\"    Generated columns: {actual_columns}\")\n",
    "            \n",
    "            # Check for missing columns\n",
    "            missing_cols = set(expected_columns) - set(actual_columns)\n",
    "            extra_cols = set(actual_columns) - set(expected_columns)\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"    ⚠️ Missing columns: {missing_cols}\")\n",
    "            if extra_cols:\n",
    "                print(f\"    ⚠️ Extra columns: {extra_cols}\")\n",
    "            \n",
    "            if not missing_cols and not extra_cols:\n",
    "                print(f\"    ✅ Schema matches perfectly!\")\n",
    "        \n",
    "        # Check if table exists and has data\n",
    "        try:\n",
    "            existing_df = spark.table(table_name)\n",
    "            existing_count = existing_df.count()\n",
    "            \n",
    "            if existing_count > 0:\n",
    "                print(f\"  ⚠️ Table {table_name} already contains {existing_count:,} records\")\n",
    "                print(f\"  💡 Appending {len(data_list):,} new records...\")\n",
    "                # Append mode\n",
    "                df.write.mode('append').saveAsTable(table_name)\n",
    "            else:\n",
    "                print(f\"  📝 Table {table_name} is empty - inserting {len(data_list):,} records...\")\n",
    "                # Overwrite mode for empty table\n",
    "                df.write.mode('overwrite').saveAsTable(table_name)\n",
    "                \n",
    "        except Exception as table_error:\n",
    "            print(f\"  ❌ Error accessing table {table_name}: {str(table_error)}\")\n",
    "            print(f\"  💡 This might be expected if the table doesn't exist yet\")\n",
    "            return\n",
    "        \n",
    "        # Verify the load\n",
    "        final_df = spark.table(table_name)\n",
    "        final_count = final_df.count()\n",
    "        print(f\"  ✅ Successfully loaded! Table {table_name} now has {final_count:,} records\")\n",
    "        \n",
    "        # Show sample of loaded data\n",
    "        print(f\"  📋 Sample records:\")\n",
    "        sample_data = final_df.limit(3).collect()\n",
    "        for i, row in enumerate(sample_data, 1):\n",
    "            print(f\"    Row {i}: {row.asDict()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error loading data to {table_name}: {str(e)}\")\n",
    "        print(f\"  💡 Check table permissions and schema compatibility\")\n",
    "\n",
    "# Load data using discovered schema information\n",
    "print(\"🚀 Loading generated data to silver tables...\")\n",
    "print()\n",
    "\n",
    "# Data loading order (respecting dependencies)\n",
    "loading_plan = [\n",
    "    {'data': parties, 'table': 'Party', 'description': 'Foundation party records'},\n",
    "    {'data': locations, 'table': 'Location', 'description': 'Geographic locations'},\n",
    "    {'data': customers, 'table': 'Customer', 'description': 'Customer records (linked to parties)'},\n",
    "    {'data': brands, 'table': 'Brand', 'description': 'Product brand records'},\n",
    "    {'data': orders, 'table': 'Order', 'description': 'Sales order headers'},\n",
    "    {'data': order_lines, 'table': 'OrderLine', 'description': 'Order line items'},\n",
    "    {'data': invoices, 'table': 'Invoice', 'description': 'Invoice headers'},\n",
    "    {'data': invoice_lines, 'table': 'InvoiceLine', 'description': 'Invoice line items'}\n",
    "]\n",
    "\n",
    "loading_results = []\n",
    "\n",
    "for step in loading_plan:\n",
    "    table_name = step['table']\n",
    "    data = step['data']\n",
    "    description = step['description']\n",
    "    \n",
    "    print(f\"📦 Loading {table_name}: {description}\")\n",
    "    \n",
    "    # Get schema info if available from discovery\n",
    "    schema_info = phase1_key_tables.get(table_name, None)\n",
    "    \n",
    "    # Load the data\n",
    "    load_data_to_table(data, table_name, schema_info)\n",
    "    \n",
    "    loading_results.append({\n",
    "        'table': table_name,\n",
    "        'records_generated': len(data),\n",
    "        'description': description\n",
    "    })\n",
    "    \n",
    "    print()  # Empty line between tables\n",
    "\n",
    "# Final summary\n",
    "print(\"📋 LOADING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 35)\n",
    "for result in loading_results:\n",
    "    print(f\"✅ {result['table']:<12} | {result['records_generated']:>6,} records | {result['description']}\")\n",
    "\n",
    "total_records = sum(r['records_generated'] for r in loading_results)\n",
    "print(f\"\\n🎯 Total records generated: {total_records:,}\")\n",
    "print(f\"📅 Load completed: {datetime.now().isoformat()}\")\n",
    "print(f\"\\n🎉 Sample data generation complete!\")\n",
    "print(f\"💡 Your Fabric Retail Data Model is now populated with enterprise-scale sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07253e01",
   "metadata": {},
   "source": [
    "## ✅ **NOTEBOOK STRUCTURE SUMMARY - CORRECTED**\n",
    "\n",
    "### **📋 Current Cell Organization:**\n",
    "\n",
    "**Cell 1: Environment Setup** 🔧  \n",
    "Sets up imports, ensures `math` module is available for Spark compatibility.\n",
    "\n",
    "**Cell 2: Schema Discovery & Analysis** 🔍  \n",
    "Discovers all 57 tables in the retail data model and creates `phase1_key_tables` variable containing the 8 Phase 1 tables with their actual column schemas.\n",
    "\n",
    "**Cell 3: Foundation Data Generation Functions** 🏗️  \n",
    "Defines functions to generate parties, locations, customers, and brands with company compliance (Buffalo NY, @example.com emails).\n",
    "\n",
    "**Cell 4: Order System Generation** 📦  \n",
    "Generates orders and order lines using Spark-compatible calculations (math.floor instead of round).\n",
    "\n",
    "**Cell 5: Schema-Aware Data Loading (Combined & Simplified)** 🎯  \n",
    "Uses the discovered schemas from Cell 2 to generate and load data that matches the actual table structures.\n",
    "\n",
    "---\n",
    "\n",
    "### **🚀 Execution Order:**\n",
    "1. **Cell 1** → Setup environment\n",
    "2. **Cell 2** → Discover schemas (creates `phase1_key_tables`)\n",
    "3. **Cell 3** → Define data generation functions  \n",
    "4. **Cell 4** → Generate order system data\n",
    "5. **Cell 5** → Execute schema-aware loading\n",
    "\n",
    "**✅ All major issues resolved:** PySparkTypeError fixed, NameError resolved, workflow streamlined to 5 cells."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
