{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: List New Tables Added to Lakehouse\n",
    "# This notebook identifies tables added beyond the original 49 tables\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configuration - Schema name for CREATE TABLE statements output\n",
    "TARGET_SCHEMA_NAME = \"retail\"  # Change this to your desired schema name\n",
    "\n",
    "# Original 49 tables list\n",
    "original_tables = [\n",
    "    \"CustomerAccount\",\n",
    "    \"CustomerAccountEmail\", \n",
    "    \"CustomerAccountLocation\",\n",
    "    \"CustomerAccountTelephoneNumber\",\n",
    "    \"CustomerGroup\",\n",
    "    \"CustomerLocation\",\n",
    "    \"CustomerName\",\n",
    "    \"CustomerRelationshipType\",\n",
    "    \"CustomerStatusType\",\n",
    "    \"CustomerTelephoneNumber\",\n",
    "    \"CustomerTradeName\",\n",
    "    \"CustomerType\",\n",
    "    \"HouseholdLocation\",\n",
    "    \"IndividualCustomer\",\n",
    "    \"Invoice\",\n",
    "    \"InvoiceLine\",\n",
    "    \"Location\",\n",
    "    \"Order\",\n",
    "    \"OrderActivityType\",\n",
    "    \"OrderAdjustment\",\n",
    "    \"OrderCharge\",\n",
    "    \"OrderChargeType\",\n",
    "    \"OrderClassification\",\n",
    "    \"OrderCondition\",\n",
    "    \"OrderDeliveryTerm\",\n",
    "    \"OrderFinanceTerm\",\n",
    "    \"OrderHold\",\n",
    "    \"OrderLanguageUsage\",\n",
    "    \"OrderLine\",\n",
    "    \"OrderLineAdjustment\",\n",
    "    \"OrderLineAdjustmentReason\",\n",
    "    \"OrderLineCharge\",\n",
    "    \"OrderLineHold\",\n",
    "    \"OrderLineStatus\",\n",
    "    \"OrderPartyRelationshipType\",\n",
    "    \"OrderPayment\",\n",
    "    \"OrderProcessingStatus\",\n",
    "    \"OrderRelatedParty\",\n",
    "    \"OrderSalesTerm\",\n",
    "    \"OrderStatus\",\n",
    "    \"OrderStatusType\",\n",
    "    \"OrderType\",\n",
    "    \"Party\",\n",
    "    \"PartyLocation\",\n",
    "    \"PartyTelephoneNumber\",\n",
    "    \"Retailer\",\n",
    "    \"SalesOrderCondition\",\n",
    "    \"UsaLocation\",\n",
    "    \"UsLocation\"\n",
    "]\n",
    "\n",
    "print(f\"üîç Analyzing tables in lakehouse\")\n",
    "print(f\"üìä Original table count: {len(original_tables)}\")\n",
    "print(f\"üéØ Target schema for new tables: {TARGET_SCHEMA_NAME}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ff84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all current tables in the lakehouse\n",
    "try:\n",
    "    # Use SHOW TABLES without specifying schema for lakehouse\n",
    "    current_tables_df = spark.sql(\"SHOW TABLES\")\n",
    "    current_tables = [row['tableName'] for row in current_tables_df.collect()]\n",
    "    \n",
    "    print(f\"üìã Current total tables in lakehouse: {len(current_tables)}\")\n",
    "    \n",
    "    # Find new tables (case-insensitive comparison)\n",
    "    original_tables_lower = [table.lower() for table in original_tables]\n",
    "    new_tables = [table for table in current_tables \n",
    "                  if table.lower() not in original_tables_lower]\n",
    "    \n",
    "    if new_tables:\n",
    "        print(f\"\\nüÜï NEW TABLES ADDED ({len(new_tables)}):\")\n",
    "        print(\"=\" * 40)\n",
    "        for i, table in enumerate(new_tables, 1):\n",
    "            print(f\"{i:2d}. {table}\")\n",
    "        \n",
    "        print(f\"\\nüìà Summary:\")\n",
    "        print(f\"   ‚Ä¢ Original tables: {len(original_tables)}\")\n",
    "        print(f\"   ‚Ä¢ Current tables:  {len(current_tables)}\")\n",
    "        print(f\"   ‚Ä¢ New tables:      {len(new_tables)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No new tables found beyond the original {len(original_tables)} tables\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing lakehouse tables: {str(e)}\")\n",
    "    print(\"üí° Make sure you have access permissions to the lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb47c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CREATE TABLE statements for new tables only\n",
    "if new_tables:\n",
    "    print(f\"\\nüîß GENERATING CREATE TABLE STATEMENTS FOR NEW TABLES:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First, generate schema creation statement\n",
    "    print(f\"\\n# Create {TARGET_SCHEMA_NAME} schema\")\n",
    "    print(f'SCHEMA_NAME = \"{TARGET_SCHEMA_NAME}\"')\n",
    "    print(f'spark.sql(f\"CREATE DATABASE IF NOT EXISTS {{SCHEMA_NAME}}\")')\n",
    "    print(f'print(f\"‚úÖ {{SCHEMA_NAME}} schema ready!\")')\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, table_name in enumerate(new_tables, 1):\n",
    "        try:\n",
    "            # Get table schema information - no schema prefix needed\n",
    "            describe_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "            columns = describe_df.collect()\n",
    "            \n",
    "            # Filter out non-column rows\n",
    "            valid_columns = [col for col in columns \n",
    "                           if not col['col_name'].startswith('#') and col['col_name'].strip() != '']\n",
    "            \n",
    "            print(f\"\\n# {i}. Create {table_name} table\")\n",
    "            print(f'create_table_sql = f\"\"\"')\n",
    "            print(f'CREATE TABLE IF NOT EXISTS {{SCHEMA_NAME}}.{table_name} (')\n",
    "            \n",
    "            # Generate column definitions\n",
    "            for j, col in enumerate(valid_columns):\n",
    "                col_name = col['col_name']\n",
    "                data_type = col['data_type']\n",
    "                \n",
    "                # Convert Spark types to more standard SQL types if needed\n",
    "                if 'bigint' in data_type.lower():\n",
    "                    data_type = 'BIGINT'\n",
    "                elif 'int' in data_type.lower():\n",
    "                    data_type = 'INT'\n",
    "                elif 'string' in data_type.lower():\n",
    "                    data_type = 'STRING'\n",
    "                elif 'double' in data_type.lower():\n",
    "                    data_type = 'DOUBLE'\n",
    "                elif 'decimal' in data_type.lower():\n",
    "                    data_type = data_type.upper()\n",
    "                elif 'boolean' in data_type.lower():\n",
    "                    data_type = 'BOOLEAN'\n",
    "                elif 'timestamp' in data_type.lower():\n",
    "                    data_type = 'TIMESTAMP'\n",
    "                elif 'date' in data_type.lower():\n",
    "                    data_type = 'DATE'\n",
    "                \n",
    "                # Add comma for all but last column\n",
    "                comma = \",\" if j < len(valid_columns) - 1 else \"\"\n",
    "                print(f'    {col_name} {data_type}{comma}')\n",
    "            \n",
    "            print(')')\n",
    "            print('USING DELTA')\n",
    "            print('\"\"\"')\n",
    "            print(f'spark.sql(create_table_sql)')\n",
    "            print(f'print(f\"‚úÖ {{SCHEMA_NAME}}.{table_name} table created!\")')\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error describing table {table_name}: {str(e)}\")\n",
    "            \n",
    "else:\n",
    "    print(\"\\nüí° No new tables to generate CREATE statements for\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
