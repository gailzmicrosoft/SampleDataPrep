{"cells":[{"cell_type":"markdown","source":["# SalesLT to Retail Data Bronze Layer\n","\n","**Objective**: Copy all SalesLT tables to bronze layer in Retail Data lakehouse using simplified Fabric PySpark approach\n","\n","**Prerequisites**:\n","- Microsoft Fabric environment with PySpark runtime\n","- Access to SalesLT tables (via shortcuts or direct tables)\n","- Write permissions to target bronze lakehouse\n","- Source lakehouse shortcut configured in bronze lakehouse\n","\n","**Setup Strategy**:\n","1. **Current Context**: Running in bronze lakehouse (`RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze`)\n","2. **Source Access**: Access SalesLT tables from `Gaiye_Test_Lakehouse` via shortcuts\n","3. **Data Flow**: Shortcut tables â†’ Bronze Files + Tables with metadata enrichment\n","\n","**Expected Tables**: address, customer, customeraddress, product, productcategory, productdescription, productmodel, productmodelproductdescription, salesorderdetail, salesorderheader\n","\n","**Workflow Options**:\n","- **First Run**: Execute all steps (1-5) for initial setup and validation\n","- **Subsequent Runs**: Execute steps 1, 4, 5 only (Setup â†’ Process â†’ Validate) for regular data refreshes\n","- **Quick Refresh**: Steps 2-3 can be skipped once environment is validated and working\n","- **Optimized Refresh**: Execute steps 1, 4 only (Setup â†’ Process) for fastest data updates once pipeline is proven reliable\n"],"metadata":{},"id":"4864afcf"},{"cell_type":"markdown","source":["## Step 1: Environment Setup"],"metadata":{},"id":"4fdef79f"},{"cell_type":"code","source":["# Import required libraries\n","import pandas as pd\n","from datetime import datetime\n","from pyspark.sql.functions import lit, current_timestamp\n","from pyspark.sql.types import StringType\n","\n","# Configuration\n","BRONZE_TARGET_PATH = \"Files/SalesLT/\"\n","SOURCE_SYSTEM = \"SalesLT\"\n","SOURCE_DATABASE = \"Gaiye_Test_Lakehouse\"\n","LOAD_TIMESTAMP = datetime.now().isoformat()\n","LOAD_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n","\n","# Expected SalesLT tables\n","EXPECTED_TABLES = [\n","    'address', 'customer', 'customeraddress', 'product', \n","    'productcategory', 'productdescription', 'productmodel',\n","    'productmodelproductdescription', 'salesorderdetail', 'salesorderheader'\n","]\n","\n","print(\"ğŸš€ SalesLT to Retail Data Bronze Pipeline\")\n","print(\"=\" * 50)\n","print(f\"âœ… Libraries imported\")\n","print(f\"ğŸ“… Load timestamp: {LOAD_TIMESTAMP}\")\n","print(f\"ğŸ¯ Target path: {BRONZE_TARGET_PATH}\")\n","print(f\"ğŸ“¥ Source database: {SOURCE_DATABASE}\")\n","print(f\"ğŸ“Š Expected tables: {len(EXPECTED_TABLES)}\")\n","print(f\"âœ… Microsoft Fabric PySpark environment ready\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:42:43.6604205Z","session_start_time":"2025-07-18T20:42:43.661509Z","execution_start_time":"2025-07-18T20:42:55.4597309Z","execution_finish_time":"2025-07-18T20:43:00.1122798Z","parent_msg_id":"92ae684d-4144-47e3-ad1b-f32c8ce55753"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ğŸš€ SalesLT to Retail Data Bronze Pipeline\n==================================================\nâœ… Libraries imported\nğŸ“… Load timestamp: 2025-07-18T20:42:55.981296\nğŸ¯ Target path: Files/SalesLT/\nğŸ“¥ Source database: Gaiye_Test_Lakehouse\nğŸ“Š Expected tables: 10\nâœ… Microsoft Fabric PySpark environment ready\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df0ae949"},{"cell_type":"markdown","source":["## Step 2: Discover Available Tables"],"metadata":{},"id":"600b215c"},{"cell_type":"code","source":["# Discover source tables from Gaiye_Test_Lakehouse\n","print(\"ğŸ” DISCOVERING SOURCE TABLES\")\n","print(\"=\" * 50)\n","\n","# Check source lakehouse context\n","print(f\"ğŸ  Current context: Bronze lakehouse (target)\")\n","print(f\"ğŸ“¥ Source database: {SOURCE_DATABASE}\")\n","print(f\"ğŸ“¤ Target path: {BRONZE_TARGET_PATH}\")\n","print(f\"ğŸ“‹ Expected table names: {', '.join(EXPECTED_TABLES)}\")\n","print()\n","\n","try:\n","    # Get tables from source database specifically\n","    print(f\"ğŸ” Querying tables from {SOURCE_DATABASE}...\")\n","    source_tables_df = spark.sql(f\"SHOW TABLES IN {SOURCE_DATABASE}\").toPandas()\n","    \n","    print(f\"âœ… Total tables found in source: {len(source_tables_df)}\")\n","    print(f\"âœ… Spark SQL connection confirmed\")\n","    \n","    if len(source_tables_df) > 0:\n","        # Handle flexible column naming\n","        table_column = None\n","        for possible_col in ['tableName', 'table_name', 'name']:\n","            if possible_col in source_tables_df.columns:\n","                table_column = possible_col\n","                break\n","        \n","        if table_column is None:\n","            table_column = source_tables_df.columns[0]\n","            print(f\"ğŸ” Using column '{table_column}' as table name\")\n","        \n","        print(f\"\\nğŸ“‹ All available tables in {SOURCE_DATABASE}:\")\n","        for _, row in source_tables_df.iterrows():\n","            table_name = row[table_column]\n","            # Check if this matches our expected tables\n","            is_expected = any(expected.lower() == table_name.lower() for expected in EXPECTED_TABLES)\n","            marker = \"ğŸ¯\" if is_expected else \"ğŸ“‹\"\n","            print(f\"   {marker} {table_name}\")\n","    else:\n","        print(f\"\\nâš ï¸ No tables found in {SOURCE_DATABASE}!\")\n","        print(\"ğŸ’¡ Verify the source lakehouse contains data\")\n","    \n","    # Find matching tables from our expected list\n","    available_tables = []\n","    missing_tables = []\n","    table_mapping = {}  # Map expected names to actual names\n","    \n","    if len(source_tables_df) > 0:\n","        # Get actual table names (case-sensitive)\n","        actual_table_names = source_tables_df[table_column].tolist()\n","        actual_table_names_lower = [name.lower() for name in actual_table_names]\n","        \n","        print(f\"\\nğŸ” Matching expected tables with available tables:\")\n","        for expected_table in EXPECTED_TABLES:\n","            # Find case-insensitive match\n","            matching_indices = [i for i, name in enumerate(actual_table_names_lower) \n","                              if name == expected_table.lower()]\n","            \n","            if matching_indices:\n","                # Use the actual table name (with correct case)\n","                actual_table_name = actual_table_names[matching_indices[0]]\n","                available_tables.append(actual_table_name)\n","                table_mapping[expected_table] = actual_table_name\n","                print(f\"   âœ… Found: {expected_table} â†’ {actual_table_name}\")\n","            else:\n","                missing_tables.append(expected_table)\n","                print(f\"   âŒ Missing: {expected_table}\")\n","    \n","    print(f\"\\nğŸ“Š DISCOVERY SUMMARY\")\n","    print(f\"âœ… Available tables found: {len(available_tables)}\")\n","    print(f\"âŒ Missing tables: {len(missing_tables)}\")\n","    \n","    if len(available_tables) == 0:\n","        print(f\"\\nâš ï¸ No expected tables found in {SOURCE_DATABASE}!\")\n","        print(\"ğŸ’¡ Check that the source lakehouse contains the SalesLT tables\")\n","        print(\"ğŸ’¡ Verify table names match expected format\")\n","        print()\n","        print(\"ğŸ”§ TROUBLESHOOTING:\")\n","        print(\"1. Ensure you have a shortcut to the source lakehouse\")\n","        print(\"2. Refresh the lakehouse view in Fabric\")\n","        print(\"3. Check the source lakehouse contains data\")\n","        \n","        # Show what was actually found for debugging\n","        if len(source_tables_df) > 0:\n","            print(f\"\\nğŸ” Debug - Available table names in source:\")\n","            for _, row in source_tables_df.iterrows():\n","                print(f\"   ğŸ“‹ '{row[table_column]}'\")\n","    else:\n","        print(f\"\\nğŸ‰ Ready to process {len(available_tables)} tables!\")\n","        \n","        # Store for next steps (use actual table names with correct casing)\n","        TABLES_TO_PROCESS = available_tables\n","        TABLE_MAPPING = table_mapping\n","        print(f\"ğŸ“ Tables to process: {', '.join(TABLES_TO_PROCESS)}\")\n","        print(f\"ğŸš€ Source: {SOURCE_DATABASE} â†’ Target: {BRONZE_TARGET_PATH}\")\n","        \n","        if missing_tables:\n","            print(f\"\\nâš ï¸ Missing tables (will be skipped): {', '.join(missing_tables)}\")\n","        \n","except Exception as e:\n","    print(f\"âŒ Failed to discover tables: {str(e)}\")\n","    print()\n","    print(\"ğŸ”§ TROUBLESHOOTING:\")\n","    print(f\"1. Ensure {SOURCE_DATABASE} is accessible from this lakehouse\")\n","    print(\"2. Check lakehouse attachment/shortcut configuration\")\n","    print(\"3. Refresh the lakehouse view in Fabric\")\n","    print(\"4. Verify source lakehouse permissions\")\n","    TABLES_TO_PROCESS = []\n","    TABLE_MAPPING = {}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:44:18.6337801Z","session_start_time":null,"execution_start_time":"2025-07-18T20:44:18.6352705Z","execution_finish_time":"2025-07-18T20:44:39.1158145Z","parent_msg_id":"4f1ebba6-7e05-447f-92b0-c2ac2e880efa"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ğŸ” DISCOVERING SOURCE TABLES\n==================================================\nğŸ  Current context: Bronze lakehouse (target)\nğŸ“¥ Source database: Gaiye_Test_Lakehouse\nğŸ“¤ Target path: Files/SalesLT/\nğŸ“‹ Expected table names: address, customer, customeraddress, product, productcategory, productdescription, productmodel, productmodelproductdescription, salesorderdetail, salesorderheader\n\nğŸ” Querying tables from Gaiye_Test_Lakehouse...\nâœ… Total tables found in source: 10\nâœ… Spark SQL connection confirmed\n\nğŸ“‹ All available tables in Gaiye_Test_Lakehouse:\n   ğŸ¯ ProductModelProductDescription\n   ğŸ¯ SalesOrderHeader\n   ğŸ¯ SalesOrderDetail\n   ğŸ¯ Customer\n   ğŸ¯ Product\n   ğŸ¯ ProductModel\n   ğŸ¯ CustomerAddress\n   ğŸ¯ Address\n   ğŸ¯ ProductDescription\n   ğŸ¯ ProductCategory\n\nğŸ” Matching expected tables with available tables:\n   âœ… Found: address â†’ Address\n   âœ… Found: customer â†’ Customer\n   âœ… Found: customeraddress â†’ CustomerAddress\n   âœ… Found: product â†’ Product\n   âœ… Found: productcategory â†’ ProductCategory\n   âœ… Found: productdescription â†’ ProductDescription\n   âœ… Found: productmodel â†’ ProductModel\n   âœ… Found: productmodelproductdescription â†’ ProductModelProductDescription\n   âœ… Found: salesorderdetail â†’ SalesOrderDetail\n   âœ… Found: salesorderheader â†’ SalesOrderHeader\n\nğŸ“Š DISCOVERY SUMMARY\nâœ… Available tables found: 10\nâŒ Missing tables: 0\n\nğŸ‰ Ready to process 10 tables!\nğŸ“ Tables to process: Address, Customer, CustomerAddress, Product, ProductCategory, ProductDescription, ProductModel, ProductModelProductDescription, SalesOrderDetail, SalesOrderHeader\nğŸš€ Source: Gaiye_Test_Lakehouse â†’ Target: Files/SalesLT/\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1aa8ea7c"},{"cell_type":"markdown","source":["## Step 3: Test Bronze Layer Access"],"metadata":{},"id":"9f6a49d7"},{"cell_type":"code","source":["# Test write access to bronze layer\n","print(\"ğŸ§ª TESTING BRONZE LAYER ACCESS\")\n","print(\"=\" * 50)\n","\n","test_path = f\"{BRONZE_TARGET_PATH}_test_access\"\n","\n","try:\n","    # Create test data\n","    test_data = [(\"access_test\", LOAD_TIMESTAMP, \"success\")]\n","    test_df = spark.createDataFrame(test_data, [\"test_type\", \"timestamp\", \"status\"])\n","    \n","    # Test write to bronze location\n","    print(f\"ğŸ“ Testing write to: {test_path}\")\n","    test_df.write.mode(\"overwrite\").parquet(test_path)\n","    \n","    # Verify read access\n","    verify_df = spark.read.parquet(test_path)\n","    test_count = verify_df.count()\n","    \n","    print(f\"âœ… Write access confirmed\")\n","    print(f\"âœ… Read access confirmed ({test_count} test records)\")\n","    print(f\"ğŸ¯ Target path ready: {BRONZE_TARGET_PATH}\")\n","    \n","    # Display test data to confirm\n","    print(\"\\nğŸ“‹ Test data sample:\")\n","    verify_df.show()\n","    \n","except Exception as e:\n","    print(f\"âŒ Bronze layer access test failed: {str(e)}\")\n","    print(\"ğŸ’¡ Ensure you have write permissions to the current lakehouse\")\n","    print(\"ğŸ’¡ Check Files directory structure and permissions\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:44:51.594009Z","session_start_time":null,"execution_start_time":"2025-07-18T20:44:51.5951742Z","execution_finish_time":"2025-07-18T20:45:22.8479328Z","parent_msg_id":"6beeeaf2-9f19-43fc-abc1-fe332a787e38"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ğŸ§ª TESTING BRONZE LAYER ACCESS\n==================================================\nğŸ“ Testing write to: Files/SalesLT/_test_access\nâœ… Write access confirmed\nâœ… Read access confirmed (1 test records)\nğŸ¯ Target path ready: Files/SalesLT/\n\nğŸ“‹ Test data sample:\n+-----------+--------------------+-------+\n|  test_type|           timestamp| status|\n+-----------+--------------------+-------+\n|access_test|2025-07-18T20:42:...|success|\n+-----------+--------------------+-------+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ae119fb"},{"cell_type":"markdown","source":["## Step 4: Process SalesLT Tables to Bronze"],"metadata":{},"id":"09179308"},{"cell_type":"code","source":["# Copy SalesLT tables to bronze layer with metadata enrichment\n","print(\"ğŸš€ PROCESSING SALESLT TABLES TO BRONZE\")\n","print(\"=\" * 60)\n","\n","if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n","    print(\"âŒ No tables to process. Run previous steps first.\")\n","else:\n","    print(f\"ğŸ“‹ Processing {len(TABLES_TO_PROCESS)} tables\")\n","    print(f\"ğŸ“¥ Source: {SOURCE_DATABASE}\")\n","    print(f\"ğŸ“¤ Target: {BRONZE_TARGET_PATH}\")\n","    print(f\"ğŸ“… Load date: {LOAD_DATE}\")\n","    print()\n","    \n","    # Processing results tracking\n","    results = []\n","    total_rows_processed = 0\n","    \n","    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n","        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Processing {table_name}...\")\n","        \n","        try:\n","            # Read source table using qualified name\n","            print(f\"   ğŸ“– Reading from {SOURCE_DATABASE}.{table_name}...\")\n","            source_df = spark.sql(f\"SELECT * FROM {SOURCE_DATABASE}.{table_name}\")\n","            row_count = source_df.count()\n","            \n","            print(f\"   âœ… Source data loaded: {row_count:,} rows\")\n","            \n","            # Add bronze layer metadata columns\n","            print(f\"   ğŸ·ï¸ Adding metadata columns...\")\n","            bronze_df = source_df \\\n","                .withColumn(\"_load_date\", lit(LOAD_DATE)) \\\n","                .withColumn(\"_load_timestamp\", lit(LOAD_TIMESTAMP)) \\\n","                .withColumn(\"_source_system\", lit(SOURCE_SYSTEM)) \\\n","                .withColumn(\"_source_table\", lit(table_name)) \\\n","                .withColumn(\"_processing_timestamp\", current_timestamp()) \\\n","                .withColumn(\"_record_source\", lit(\"cross_lakehouse_copy\")) \\\n","                .withColumn(\"_load_method\", lit(\"spark_sql_full_extract\")) \\\n","                .withColumn(\"_source_database\", lit(SOURCE_DATABASE)) \\\n","                .withColumn(\"_target_path\", lit(f\"{BRONZE_TARGET_PATH}{table_name}\"))\n","            \n","            # Write to bronze layer as files\n","            table_target_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n","            print(f\"   ğŸ’¾ Writing to Files: {table_target_path}\")\n","            \n","            bronze_df.write \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .parquet(table_target_path)\n","            \n","            print(f\"   âœ… Files saved: {row_count:,} rows\")\n","            \n","            # Also create lakehouse table (data in memory, efficient to do both)\n","            lakehouse_table_name = f\"bronze_{table_name}\"\n","            print(f\"   ğŸ¢ Creating lakehouse table: {lakehouse_table_name}\")\n","            \n","            try:\n","                # Create/replace lakehouse table\n","                bronze_df.write \\\n","                    .mode(\"overwrite\") \\\n","                    .option(\"overwriteSchema\", \"true\") \\\n","                    .saveAsTable(lakehouse_table_name)\n","                \n","                print(f\"   âœ… Table created: {lakehouse_table_name}\")\n","                table_creation_status = \"success\"\n","                \n","            except Exception as table_error:\n","                table_error_msg = str(table_error)[:60]\n","                print(f\"   âš ï¸ Table creation failed: {table_error_msg}...\")\n","                table_creation_status = \"file_only\"\n","            \n","            # Success tracking\n","            total_rows_processed += row_count\n","            results.append({\n","                \"table\": table_name,\n","                \"rows\": row_count,\n","                \"status\": \"success\",\n","                \"target_path\": table_target_path,\n","                \"lakehouse_table\": lakehouse_table_name,\n","                \"table_status\": table_creation_status\n","            })\n","            \n","            print(f\"   ğŸ‰ Successfully processed {row_count:,} rows (Files + Table)\")\n","            \n","        except Exception as e:\n","            error_msg = str(e)[:100]\n","            results.append({\n","                \"table\": table_name,\n","                \"rows\": 0,\n","                \"status\": \"failed\",\n","                \"error\": error_msg\n","            })\n","            print(f\"   âŒ Failed: {error_msg}...\")\n","        \n","        print()\n","    \n","    # Processing summary\n","    successful = [r for r in results if r[\"status\"] == \"success\"]\n","    failed = [r for r in results if r[\"status\"] == \"failed\"]\n","    tables_created = [r for r in successful if r.get(\"table_status\") == \"success\"]\n","    files_only = [r for r in successful if r.get(\"table_status\") == \"file_only\"]\n","    \n","    print(\"ğŸ‰ PROCESSING SUMMARY\")\n","    print(\"=\" * 60)\n","    print(f\"âœ… Successfully processed: {len(successful)} tables\")\n","    print(f\"âŒ Failed processing: {len(failed)} tables\")\n","    print(f\"ğŸ“Š Total rows processed: {total_rows_processed:,}\")\n","    print(f\"ğŸ“ Files created: {len(successful)} (all)\")\n","    print(f\"ğŸ¢ Lakehouse tables created: {len(tables_created)}\")\n","    print(f\"âš ï¸ Files only (table creation failed): {len(files_only)}\")\n","    print(f\"ğŸ“… Processing completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","    \n","    if successful:\n","        print(f\"\\nğŸ“ Bronze layer file structure:\")\n","        print(f\"{BRONZE_TARGET_PATH}\")\n","        for result in successful:\n","            table_marker = \"ğŸ¢\" if result.get(\"table_status\") == \"success\" else \"ğŸ“\"\n","            print(f\"â”œâ”€â”€ {result['table']}/ ({result['rows']:,} rows) {table_marker}\")\n","    \n","    if tables_created:\n","        print(f\"\\nğŸ¢ Lakehouse tables created:\")\n","        for result in tables_created:\n","            print(f\"âœ… {result['lakehouse_table']} ({result['rows']:,} rows)\")\n","    \n","    if files_only:\n","        print(f\"\\nğŸ“ Files only (table creation issues):\")\n","        for result in files_only:\n","            print(f\"âš ï¸ {result['table']} â†’ Files saved, table creation failed\")\n","    \n","    if failed:\n","        print(f\"\\nâš ï¸ Processing failures:\")\n","        for result in failed:\n","            print(f\"âŒ {result['table']}: {result.get('error', 'Unknown error')}\")\n","    \n","    print(f\"\\nğŸ¯ Bronze data ready for downstream processing!\")\n","    print(f\"ğŸ“ Files stored in: {BRONZE_TARGET_PATH}\")\n","    if tables_created:\n","        print(f\"ğŸ¢ Tables available in: {len(tables_created)} lakehouse tables\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:45:37.1531445Z","session_start_time":null,"execution_start_time":"2025-07-18T20:45:37.1546621Z","execution_finish_time":"2025-07-18T20:47:21.1200988Z","parent_msg_id":"9ed5cf3e-efb7-4d9f-b5f7-b5bb1b809b86"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ğŸš€ PROCESSING SALESLT TABLES TO BRONZE\n============================================================\nğŸ“‹ Processing 10 tables\nğŸ“¥ Source: Gaiye_Test_Lakehouse\nğŸ“¤ Target: Files/SalesLT/\nğŸ“… Load date: 2025-07-18\n\n[1/10] Processing Address...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.Address...\n   âœ… Source data loaded: 450 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/Address\n   âœ… Files saved: 450 rows\n   ğŸ¢ Creating lakehouse table: bronze_Address\n   âœ… Table created: bronze_Address\n   ğŸ‰ Successfully processed 450 rows (Files + Table)\n\n[2/10] Processing Customer...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.Customer...\n   âœ… Source data loaded: 847 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/Customer\n   âœ… Files saved: 847 rows\n   ğŸ¢ Creating lakehouse table: bronze_Customer\n   âœ… Table created: bronze_Customer\n   ğŸ‰ Successfully processed 847 rows (Files + Table)\n\n[3/10] Processing CustomerAddress...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.CustomerAddress...\n   âœ… Source data loaded: 417 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/CustomerAddress\n   âœ… Files saved: 417 rows\n   ğŸ¢ Creating lakehouse table: bronze_CustomerAddress\n   âœ… Table created: bronze_CustomerAddress\n   ğŸ‰ Successfully processed 417 rows (Files + Table)\n\n[4/10] Processing Product...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.Product...\n   âœ… Source data loaded: 295 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/Product\n   âœ… Files saved: 295 rows\n   ğŸ¢ Creating lakehouse table: bronze_Product\n   âœ… Table created: bronze_Product\n   ğŸ‰ Successfully processed 295 rows (Files + Table)\n\n[5/10] Processing ProductCategory...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.ProductCategory...\n   âœ… Source data loaded: 41 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/ProductCategory\n   âœ… Files saved: 41 rows\n   ğŸ¢ Creating lakehouse table: bronze_ProductCategory\n   âœ… Table created: bronze_ProductCategory\n   ğŸ‰ Successfully processed 41 rows (Files + Table)\n\n[6/10] Processing ProductDescription...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.ProductDescription...\n   âœ… Source data loaded: 762 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/ProductDescription\n   âœ… Files saved: 762 rows\n   ğŸ¢ Creating lakehouse table: bronze_ProductDescription\n   âœ… Table created: bronze_ProductDescription\n   ğŸ‰ Successfully processed 762 rows (Files + Table)\n\n[7/10] Processing ProductModel...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.ProductModel...\n   âœ… Source data loaded: 128 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/ProductModel\n   âœ… Files saved: 128 rows\n   ğŸ¢ Creating lakehouse table: bronze_ProductModel\n   âœ… Table created: bronze_ProductModel\n   ğŸ‰ Successfully processed 128 rows (Files + Table)\n\n[8/10] Processing ProductModelProductDescription...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.ProductModelProductDescription...\n   âœ… Source data loaded: 762 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/ProductModelProductDescription\n   âœ… Files saved: 762 rows\n   ğŸ¢ Creating lakehouse table: bronze_ProductModelProductDescription\n   âœ… Table created: bronze_ProductModelProductDescription\n   ğŸ‰ Successfully processed 762 rows (Files + Table)\n\n[9/10] Processing SalesOrderDetail...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.SalesOrderDetail...\n   âœ… Source data loaded: 542 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/SalesOrderDetail\n   âœ… Files saved: 542 rows\n   ğŸ¢ Creating lakehouse table: bronze_SalesOrderDetail\n   âœ… Table created: bronze_SalesOrderDetail\n   ğŸ‰ Successfully processed 542 rows (Files + Table)\n\n[10/10] Processing SalesOrderHeader...\n   ğŸ“– Reading from Gaiye_Test_Lakehouse.SalesOrderHeader...\n   âœ… Source data loaded: 32 rows\n   ğŸ·ï¸ Adding metadata columns...\n   ğŸ’¾ Writing to Files: Files/SalesLT/SalesOrderHeader\n   âœ… Files saved: 32 rows\n   ğŸ¢ Creating lakehouse table: bronze_SalesOrderHeader\n   âœ… Table created: bronze_SalesOrderHeader\n   ğŸ‰ Successfully processed 32 rows (Files + Table)\n\nğŸ‰ PROCESSING SUMMARY\n============================================================\nâœ… Successfully processed: 10 tables\nâŒ Failed processing: 0 tables\nğŸ“Š Total rows processed: 4,276\nğŸ“ Files created: 10 (all)\nğŸ¢ Lakehouse tables created: 10\nâš ï¸ Files only (table creation failed): 0\nğŸ“… Processing completed: 2025-07-18 20:47:19\n\nğŸ“ Bronze layer file structure:\nFiles/SalesLT/\nâ”œâ”€â”€ Address/ (450 rows) ğŸ¢\nâ”œâ”€â”€ Customer/ (847 rows) ğŸ¢\nâ”œâ”€â”€ CustomerAddress/ (417 rows) ğŸ¢\nâ”œâ”€â”€ Product/ (295 rows) ğŸ¢\nâ”œâ”€â”€ ProductCategory/ (41 rows) ğŸ¢\nâ”œâ”€â”€ ProductDescription/ (762 rows) ğŸ¢\nâ”œâ”€â”€ ProductModel/ (128 rows) ğŸ¢\nâ”œâ”€â”€ ProductModelProductDescription/ (762 rows) ğŸ¢\nâ”œâ”€â”€ SalesOrderDetail/ (542 rows) ğŸ¢\nâ”œâ”€â”€ SalesOrderHeader/ (32 rows) ğŸ¢\n\nğŸ¢ Lakehouse tables created:\nâœ… bronze_Address (450 rows)\nâœ… bronze_Customer (847 rows)\nâœ… bronze_CustomerAddress (417 rows)\nâœ… bronze_Product (295 rows)\nâœ… bronze_ProductCategory (41 rows)\nâœ… bronze_ProductDescription (762 rows)\nâœ… bronze_ProductModel (128 rows)\nâœ… bronze_ProductModelProductDescription (762 rows)\nâœ… bronze_SalesOrderDetail (542 rows)\nâœ… bronze_SalesOrderHeader (32 rows)\n\nğŸ¯ Bronze data ready for downstream processing!\nğŸ“ Files stored in: Files/SalesLT/\nğŸ¢ Tables available in: 10 lakehouse tables\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ccd7807"},{"cell_type":"markdown","source":["## Step 5: Validate Bronze Layer Data"],"metadata":{},"id":"484b1ba4"},{"cell_type":"code","source":["# Validate bronze layer data quality and completeness\n","print(\"ğŸ” VALIDATING BRONZE LAYER DATA\")\n","print(\"=\" * 60)\n","\n","if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n","    print(\"âŒ No tables to validate. Run previous steps first.\")\n","else:\n","    validation_results = []\n","    total_bronze_rows = 0\n","    \n","    print(f\"ğŸ“‹ Validating {len(TABLES_TO_PROCESS)} bronze files\")\n","    print(f\"ğŸ“ Target location: {BRONZE_TARGET_PATH}\")\n","    print()\n","    \n","    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n","        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Validating {table_name}...\")\n","        \n","        try:\n","            file_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n","            lakehouse_table_name = f\"bronze_{table_name}\"\n","            \n","            # Read bronze data files\n","            bronze_df = spark.read.parquet(file_path)\n","            bronze_count = bronze_df.count()\n","            \n","            # Check if lakehouse table exists and validate\n","            table_count = 0\n","            table_exists = False\n","            try:\n","                table_df = spark.table(lakehouse_table_name)\n","                table_count = table_df.count()\n","                table_exists = True\n","            except Exception:\n","                table_exists = False\n","            \n","            # Check metadata columns\n","            sample_row = bronze_df.select(\n","                \"_load_date\", \n","                \"_source_system\", \n","                \"_source_table\",\n","                \"_load_method\"\n","            ).first()\n","            \n","            # Get column count for schema validation\n","            column_count = len(bronze_df.columns)\n","            metadata_columns = [col for col in bronze_df.columns if col.startswith('_')]\n","            business_columns = [col for col in bronze_df.columns if not col.startswith('_')]\n","            \n","            total_bronze_rows += bronze_count\n","            validation_results.append({\n","                \"table\": table_name,\n","                \"bronze_rows\": bronze_count,\n","                \"table_rows\": table_count,\n","                \"table_exists\": table_exists,\n","                \"total_columns\": column_count,\n","                \"business_columns\": len(business_columns),\n","                \"metadata_columns\": len(metadata_columns),\n","                \"load_date\": sample_row._load_date if sample_row else \"Unknown\",\n","                \"source_system\": sample_row._source_system if sample_row else \"Unknown\",\n","                \"load_method\": sample_row._load_method if sample_row else \"Unknown\",\n","                \"lakehouse_table\": lakehouse_table_name,\n","                \"status\": \"success\"\n","            })\n","            \n","            print(f\"   âœ… Files: {bronze_count:,} rows validated\")\n","            if table_exists:\n","                print(f\"   ğŸ¢ Table: {table_count:,} rows validated\")\n","                row_match = \"âœ…\" if bronze_count == table_count else \"âš ï¸\"\n","                print(f\"   {row_match} Row count match: Files={bronze_count:,}, Table={table_count:,}\")\n","            else:\n","                print(f\"   âš ï¸ Lakehouse table not found: {lakehouse_table_name}\")\n","            print(f\"   ğŸ“Š Columns: {len(business_columns)} business + {len(metadata_columns)} metadata\")\n","            print(f\"   ğŸ“… Load date: {sample_row._load_date if sample_row else 'Unknown'}\")\n","            print(f\"   ğŸ·ï¸ Source: {sample_row._source_system if sample_row else 'Unknown'}\")\n","            \n","        except Exception as e:\n","            error_msg = str(e)[:80]\n","            validation_results.append({\n","                \"table\": table_name,\n","                \"bronze_rows\": 0,\n","                \"status\": \"failed\",\n","                \"error\": error_msg\n","            })\n","            print(f\"   âŒ Validation failed: {error_msg}...\")\n","        \n","        print()\n","    \n","    # Validation summary\n","    successful_validations = [r for r in validation_results if r[\"status\"] == \"success\"]\n","    failed_validations = [r for r in validation_results if r[\"status\"] == \"failed\"]\n","    tables_available = [r for r in successful_validations if r.get(\"table_exists\", False)]\n","    files_only = [r for r in successful_validations if not r.get(\"table_exists\", False)]\n","    \n","    print(\"ğŸ¯ VALIDATION SUMMARY\")\n","    print(\"=\" * 60)\n","    print(f\"âœ… Successfully validated: {len(successful_validations)} files\")\n","    print(f\"âŒ Failed validations: {len(failed_validations)} files\")\n","    print(f\"ğŸ“Š Total bronze rows: {total_bronze_rows:,}\")\n","    print(f\"ğŸ“ Files available: {len(successful_validations)}\")\n","    print(f\"ğŸ¢ Lakehouse tables available: {len(tables_available)}\")\n","    print(f\"âš ï¸ Files only (no table): {len(files_only)}\")\n","    print(f\"ğŸ·ï¸ Metadata enrichment: Load tracking added\")\n","    \n","    if successful_validations:\n","        print(f\"\\nğŸ“‹ Bronze layer inventory:\")\n","        for result in successful_validations:\n","            table_marker = \"ğŸ¢+ğŸ“\" if result.get(\"table_exists\") else \"ğŸ“\"\n","            table_info = f\" | Table: {result.get('table_rows', 0):,}\" if result.get(\"table_exists\") else \"\"\n","            print(f\"  â€¢ {result['table']}: {result['bronze_rows']:,} rows | {result['total_columns']} columns | {result['load_date']} {table_marker}{table_info}\")\n","    \n","    if tables_available:\n","        print(f\"\\nğŸ¢ Available lakehouse tables:\")\n","        for result in tables_available:\n","            print(f\"  âœ… {result['lakehouse_table']}: {result.get('table_rows', 0):,} rows\")\n","    \n","    if files_only:\n","        print(f\"\\nğŸ“ Files only (tables not created):\")\n","        for result in files_only:\n","            print(f\"  âš ï¸ {result['table']}: File available, no lakehouse table\")\n","    \n","    if failed_validations:\n","        print(f\"\\nâš ï¸ Validation failures:\")\n","        for result in failed_validations:\n","            print(f\"  âŒ {result['table']}: {result.get('error', 'Unknown error')}\")\n","    \n","    print(f\"\\nğŸ‰ Bronze layer validation complete!\")\n","    print(f\"ğŸ“ Files location: {BRONZE_TARGET_PATH}\")\n","    if tables_available:\n","        print(f\"ğŸ¢ Tables accessible via: SELECT * FROM bronze_[tablename]\")\n","    print(f\"ğŸš€ Ready for silver layer processing\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"2236780f-4100-4d35-bc75-eae718618527","normalized_state":"finished","queued_time":"2025-07-17T17:47:35.8038802Z","session_start_time":null,"execution_start_time":"2025-07-17T17:47:35.8054881Z","execution_finish_time":"2025-07-17T17:48:03.6253879Z","parent_msg_id":"41ffbeb3-3aa0-4959-8dfe-0b1f81a26f43"},"text/plain":"StatementMeta(, 2236780f-4100-4d35-bc75-eae718618527, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ğŸ” VALIDATING BRONZE LAYER DATA\n============================================================\nğŸ“‹ Validating 10 bronze files\nğŸ“ Target location: Files/SalesLT/\n\n[1/10] Validating Address...\n   âœ… Files: 450 rows validated\n   ğŸ¢ Table: 450 rows validated\n   âœ… Row count match: Files=450, Table=450\n   ğŸ“Š Columns: 9 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[2/10] Validating Customer...\n   âœ… Files: 847 rows validated\n   ğŸ¢ Table: 847 rows validated\n   âœ… Row count match: Files=847, Table=847\n   ğŸ“Š Columns: 15 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[3/10] Validating CustomerAddress...\n   âœ… Files: 417 rows validated\n   ğŸ¢ Table: 417 rows validated\n   âœ… Row count match: Files=417, Table=417\n   ğŸ“Š Columns: 5 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[4/10] Validating Product...\n   âœ… Files: 295 rows validated\n   ğŸ¢ Table: 295 rows validated\n   âœ… Row count match: Files=295, Table=295\n   ğŸ“Š Columns: 17 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[5/10] Validating ProductCategory...\n   âœ… Files: 41 rows validated\n   ğŸ¢ Table: 41 rows validated\n   âœ… Row count match: Files=41, Table=41\n   ğŸ“Š Columns: 5 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[6/10] Validating ProductDescription...\n   âœ… Files: 762 rows validated\n   ğŸ¢ Table: 762 rows validated\n   âœ… Row count match: Files=762, Table=762\n   ğŸ“Š Columns: 4 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[7/10] Validating ProductModel...\n   âœ… Files: 128 rows validated\n   ğŸ¢ Table: 128 rows validated\n   âœ… Row count match: Files=128, Table=128\n   ğŸ“Š Columns: 4 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[8/10] Validating ProductModelProductDescription...\n   âœ… Files: 762 rows validated\n   ğŸ¢ Table: 762 rows validated\n   âœ… Row count match: Files=762, Table=762\n   ğŸ“Š Columns: 5 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[9/10] Validating SalesOrderDetail...\n   âœ… Files: 542 rows validated\n   ğŸ¢ Table: 542 rows validated\n   âœ… Row count match: Files=542, Table=542\n   ğŸ“Š Columns: 8 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\n[10/10] Validating SalesOrderHeader...\n   âœ… Files: 32 rows validated\n   ğŸ¢ Table: 32 rows validated\n   âœ… Row count match: Files=32, Table=32\n   ğŸ“Š Columns: 20 business + 9 metadata\n   ğŸ“… Load date: 2025-07-17\n   ğŸ·ï¸ Source: SalesLT\n\nğŸ¯ VALIDATION SUMMARY\n============================================================\nâœ… Successfully validated: 10 files\nâŒ Failed validations: 0 files\nğŸ“Š Total bronze rows: 4,276\nğŸ“ Files available: 10\nğŸ¢ Lakehouse tables available: 10\nâš ï¸ Files only (no table): 0\nğŸ·ï¸ Metadata enrichment: Load tracking added\n\nğŸ“‹ Bronze layer inventory:\n  â€¢ Address: 450 rows | 18 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 450\n  â€¢ Customer: 847 rows | 24 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 847\n  â€¢ CustomerAddress: 417 rows | 14 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 417\n  â€¢ Product: 295 rows | 26 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 295\n  â€¢ ProductCategory: 41 rows | 14 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 41\n  â€¢ ProductDescription: 762 rows | 13 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 762\n  â€¢ ProductModel: 128 rows | 13 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 128\n  â€¢ ProductModelProductDescription: 762 rows | 14 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 762\n  â€¢ SalesOrderDetail: 542 rows | 17 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 542\n  â€¢ SalesOrderHeader: 32 rows | 29 columns | 2025-07-17 ğŸ¢+ğŸ“ | Table: 32\n\nğŸ¢ Available lakehouse tables:\n  âœ… bronze_Address: 450 rows\n  âœ… bronze_Customer: 847 rows\n  âœ… bronze_CustomerAddress: 417 rows\n  âœ… bronze_Product: 295 rows\n  âœ… bronze_ProductCategory: 41 rows\n  âœ… bronze_ProductDescription: 762 rows\n  âœ… bronze_ProductModel: 128 rows\n  âœ… bronze_ProductModelProductDescription: 762 rows\n  âœ… bronze_SalesOrderDetail: 542 rows\n  âœ… bronze_SalesOrderHeader: 32 rows\n\nğŸ‰ Bronze layer validation complete!\nğŸ“ Files location: Files/SalesLT/\nğŸ¢ Tables accessible via: SELECT * FROM bronze_[tablename]\nğŸš€ Ready for silver layer processing\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28b3a08c"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b160891e-ec7d-471d-961d-4f50cbb006c4"},{"id":"695625b4-4901-48e3-8183-cbaac28086b5"}],"default_lakehouse":"b160891e-ec7d-471d-961d-4f50cbb006c4","default_lakehouse_name":"RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze","default_lakehouse_workspace_id":"88ef0969-45fb-42dd-af36-283224c74eed"}}},"nbformat":4,"nbformat_minor":5}