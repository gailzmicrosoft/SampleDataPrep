{"cells":[{"cell_type":"markdown","source":["# SalesLT to Retail Data Bronze Layer\n","\n","**Objective**: Copy all SalesLT tables to bronze layer in Retail Data lakehouse using simplified Fabric PySpark approach\n","\n","**Prerequisites**:\n","- Microsoft Fabric environment with PySpark runtime\n","- Access to SalesLT tables (via shortcuts or direct tables)\n","- Write permissions to target bronze lakehouse\n","- Source lakehouse shortcut configured in bronze lakehouse\n","\n","**Setup Strategy**:\n","1. **Current Context**: Running in bronze lakehouse (`RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze`)\n","2. **Source Access**: Access SalesLT tables from `Gaiye_Test_Lakehouse` via shortcuts\n","3. **Data Flow**: Shortcut tables → Bronze Files + Tables with metadata enrichment\n","\n","**Expected Tables**: address, customer, customeraddress, product, productcategory, productdescription, productmodel, productmodelproductdescription, salesorderdetail, salesorderheader\n","\n","**Workflow Options**:\n","- **First Run**: Execute all steps (1-5) for initial setup and validation\n","- **Subsequent Runs**: Execute steps 1, 4, 5 only (Setup → Process → Validate) for regular data refreshes\n","- **Quick Refresh**: Steps 2-3 can be skipped once environment is validated and working\n","- **Optimized Refresh**: Execute steps 1, 4 only (Setup → Process) for fastest data updates once pipeline is proven reliable\n"],"metadata":{},"id":"4864afcf"},{"cell_type":"markdown","source":["## Step 1: Environment Setup"],"metadata":{},"id":"4fdef79f"},{"cell_type":"code","source":["# Import required libraries\n","import pandas as pd\n","from datetime import datetime\n","from pyspark.sql.functions import lit, current_timestamp\n","from pyspark.sql.types import StringType\n","\n","# Configuration\n","BRONZE_TARGET_PATH = \"Files/SalesLT/\"\n","SOURCE_SYSTEM = \"SalesLT\"\n","SOURCE_DATABASE = \"Gaiye_Test_Lakehouse\"\n","LOAD_TIMESTAMP = datetime.now().isoformat()\n","LOAD_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n","\n","# Expected SalesLT tables\n","EXPECTED_TABLES = [\n","    'address', 'customer', 'customeraddress', 'product', \n","    'productcategory', 'productdescription', 'productmodel',\n","    'productmodelproductdescription', 'salesorderdetail', 'salesorderheader'\n","]\n","\n","print(\"🚀 SalesLT to Retail Data Bronze Pipeline\")\n","print(\"=\" * 50)\n","print(f\"✅ Libraries imported\")\n","print(f\"📅 Load timestamp: {LOAD_TIMESTAMP}\")\n","print(f\"🎯 Target path: {BRONZE_TARGET_PATH}\")\n","print(f\"📥 Source database: {SOURCE_DATABASE}\")\n","print(f\"📊 Expected tables: {len(EXPECTED_TABLES)}\")\n","print(f\"✅ Microsoft Fabric PySpark environment ready\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:42:43.6604205Z","session_start_time":"2025-07-18T20:42:43.661509Z","execution_start_time":"2025-07-18T20:42:55.4597309Z","execution_finish_time":"2025-07-18T20:43:00.1122798Z","parent_msg_id":"92ae684d-4144-47e3-ad1b-f32c8ce55753"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🚀 SalesLT to Retail Data Bronze Pipeline\n==================================================\n✅ Libraries imported\n📅 Load timestamp: 2025-07-18T20:42:55.981296\n🎯 Target path: Files/SalesLT/\n📥 Source database: Gaiye_Test_Lakehouse\n📊 Expected tables: 10\n✅ Microsoft Fabric PySpark environment ready\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"df0ae949"},{"cell_type":"markdown","source":["## Step 2: Discover Available Tables"],"metadata":{},"id":"600b215c"},{"cell_type":"code","source":["# Discover source tables from Gaiye_Test_Lakehouse\n","print(\"🔍 DISCOVERING SOURCE TABLES\")\n","print(\"=\" * 50)\n","\n","# Check source lakehouse context\n","print(f\"🏠 Current context: Bronze lakehouse (target)\")\n","print(f\"📥 Source database: {SOURCE_DATABASE}\")\n","print(f\"📤 Target path: {BRONZE_TARGET_PATH}\")\n","print(f\"📋 Expected table names: {', '.join(EXPECTED_TABLES)}\")\n","print()\n","\n","try:\n","    # Get tables from source database specifically\n","    print(f\"🔍 Querying tables from {SOURCE_DATABASE}...\")\n","    source_tables_df = spark.sql(f\"SHOW TABLES IN {SOURCE_DATABASE}\").toPandas()\n","    \n","    print(f\"✅ Total tables found in source: {len(source_tables_df)}\")\n","    print(f\"✅ Spark SQL connection confirmed\")\n","    \n","    if len(source_tables_df) > 0:\n","        # Handle flexible column naming\n","        table_column = None\n","        for possible_col in ['tableName', 'table_name', 'name']:\n","            if possible_col in source_tables_df.columns:\n","                table_column = possible_col\n","                break\n","        \n","        if table_column is None:\n","            table_column = source_tables_df.columns[0]\n","            print(f\"🔍 Using column '{table_column}' as table name\")\n","        \n","        print(f\"\\n📋 All available tables in {SOURCE_DATABASE}:\")\n","        for _, row in source_tables_df.iterrows():\n","            table_name = row[table_column]\n","            # Check if this matches our expected tables\n","            is_expected = any(expected.lower() == table_name.lower() for expected in EXPECTED_TABLES)\n","            marker = \"🎯\" if is_expected else \"📋\"\n","            print(f\"   {marker} {table_name}\")\n","    else:\n","        print(f\"\\n⚠️ No tables found in {SOURCE_DATABASE}!\")\n","        print(\"💡 Verify the source lakehouse contains data\")\n","    \n","    # Find matching tables from our expected list\n","    available_tables = []\n","    missing_tables = []\n","    table_mapping = {}  # Map expected names to actual names\n","    \n","    if len(source_tables_df) > 0:\n","        # Get actual table names (case-sensitive)\n","        actual_table_names = source_tables_df[table_column].tolist()\n","        actual_table_names_lower = [name.lower() for name in actual_table_names]\n","        \n","        print(f\"\\n🔍 Matching expected tables with available tables:\")\n","        for expected_table in EXPECTED_TABLES:\n","            # Find case-insensitive match\n","            matching_indices = [i for i, name in enumerate(actual_table_names_lower) \n","                              if name == expected_table.lower()]\n","            \n","            if matching_indices:\n","                # Use the actual table name (with correct case)\n","                actual_table_name = actual_table_names[matching_indices[0]]\n","                available_tables.append(actual_table_name)\n","                table_mapping[expected_table] = actual_table_name\n","                print(f\"   ✅ Found: {expected_table} → {actual_table_name}\")\n","            else:\n","                missing_tables.append(expected_table)\n","                print(f\"   ❌ Missing: {expected_table}\")\n","    \n","    print(f\"\\n📊 DISCOVERY SUMMARY\")\n","    print(f\"✅ Available tables found: {len(available_tables)}\")\n","    print(f\"❌ Missing tables: {len(missing_tables)}\")\n","    \n","    if len(available_tables) == 0:\n","        print(f\"\\n⚠️ No expected tables found in {SOURCE_DATABASE}!\")\n","        print(\"💡 Check that the source lakehouse contains the SalesLT tables\")\n","        print(\"💡 Verify table names match expected format\")\n","        print()\n","        print(\"🔧 TROUBLESHOOTING:\")\n","        print(\"1. Ensure you have a shortcut to the source lakehouse\")\n","        print(\"2. Refresh the lakehouse view in Fabric\")\n","        print(\"3. Check the source lakehouse contains data\")\n","        \n","        # Show what was actually found for debugging\n","        if len(source_tables_df) > 0:\n","            print(f\"\\n🔍 Debug - Available table names in source:\")\n","            for _, row in source_tables_df.iterrows():\n","                print(f\"   📋 '{row[table_column]}'\")\n","    else:\n","        print(f\"\\n🎉 Ready to process {len(available_tables)} tables!\")\n","        \n","        # Store for next steps (use actual table names with correct casing)\n","        TABLES_TO_PROCESS = available_tables\n","        TABLE_MAPPING = table_mapping\n","        print(f\"📝 Tables to process: {', '.join(TABLES_TO_PROCESS)}\")\n","        print(f\"🚀 Source: {SOURCE_DATABASE} → Target: {BRONZE_TARGET_PATH}\")\n","        \n","        if missing_tables:\n","            print(f\"\\n⚠️ Missing tables (will be skipped): {', '.join(missing_tables)}\")\n","        \n","except Exception as e:\n","    print(f\"❌ Failed to discover tables: {str(e)}\")\n","    print()\n","    print(\"🔧 TROUBLESHOOTING:\")\n","    print(f\"1. Ensure {SOURCE_DATABASE} is accessible from this lakehouse\")\n","    print(\"2. Check lakehouse attachment/shortcut configuration\")\n","    print(\"3. Refresh the lakehouse view in Fabric\")\n","    print(\"4. Verify source lakehouse permissions\")\n","    TABLES_TO_PROCESS = []\n","    TABLE_MAPPING = {}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:44:18.6337801Z","session_start_time":null,"execution_start_time":"2025-07-18T20:44:18.6352705Z","execution_finish_time":"2025-07-18T20:44:39.1158145Z","parent_msg_id":"4f1ebba6-7e05-447f-92b0-c2ac2e880efa"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🔍 DISCOVERING SOURCE TABLES\n==================================================\n🏠 Current context: Bronze lakehouse (target)\n📥 Source database: Gaiye_Test_Lakehouse\n📤 Target path: Files/SalesLT/\n📋 Expected table names: address, customer, customeraddress, product, productcategory, productdescription, productmodel, productmodelproductdescription, salesorderdetail, salesorderheader\n\n🔍 Querying tables from Gaiye_Test_Lakehouse...\n✅ Total tables found in source: 10\n✅ Spark SQL connection confirmed\n\n📋 All available tables in Gaiye_Test_Lakehouse:\n   🎯 ProductModelProductDescription\n   🎯 SalesOrderHeader\n   🎯 SalesOrderDetail\n   🎯 Customer\n   🎯 Product\n   🎯 ProductModel\n   🎯 CustomerAddress\n   🎯 Address\n   🎯 ProductDescription\n   🎯 ProductCategory\n\n🔍 Matching expected tables with available tables:\n   ✅ Found: address → Address\n   ✅ Found: customer → Customer\n   ✅ Found: customeraddress → CustomerAddress\n   ✅ Found: product → Product\n   ✅ Found: productcategory → ProductCategory\n   ✅ Found: productdescription → ProductDescription\n   ✅ Found: productmodel → ProductModel\n   ✅ Found: productmodelproductdescription → ProductModelProductDescription\n   ✅ Found: salesorderdetail → SalesOrderDetail\n   ✅ Found: salesorderheader → SalesOrderHeader\n\n📊 DISCOVERY SUMMARY\n✅ Available tables found: 10\n❌ Missing tables: 0\n\n🎉 Ready to process 10 tables!\n📝 Tables to process: Address, Customer, CustomerAddress, Product, ProductCategory, ProductDescription, ProductModel, ProductModelProductDescription, SalesOrderDetail, SalesOrderHeader\n🚀 Source: Gaiye_Test_Lakehouse → Target: Files/SalesLT/\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1aa8ea7c"},{"cell_type":"markdown","source":["## Step 3: Test Bronze Layer Access"],"metadata":{},"id":"9f6a49d7"},{"cell_type":"code","source":["# Test write access to bronze layer\n","print(\"🧪 TESTING BRONZE LAYER ACCESS\")\n","print(\"=\" * 50)\n","\n","test_path = f\"{BRONZE_TARGET_PATH}_test_access\"\n","\n","try:\n","    # Create test data\n","    test_data = [(\"access_test\", LOAD_TIMESTAMP, \"success\")]\n","    test_df = spark.createDataFrame(test_data, [\"test_type\", \"timestamp\", \"status\"])\n","    \n","    # Test write to bronze location\n","    print(f\"📝 Testing write to: {test_path}\")\n","    test_df.write.mode(\"overwrite\").parquet(test_path)\n","    \n","    # Verify read access\n","    verify_df = spark.read.parquet(test_path)\n","    test_count = verify_df.count()\n","    \n","    print(f\"✅ Write access confirmed\")\n","    print(f\"✅ Read access confirmed ({test_count} test records)\")\n","    print(f\"🎯 Target path ready: {BRONZE_TARGET_PATH}\")\n","    \n","    # Display test data to confirm\n","    print(\"\\n📋 Test data sample:\")\n","    verify_df.show()\n","    \n","except Exception as e:\n","    print(f\"❌ Bronze layer access test failed: {str(e)}\")\n","    print(\"💡 Ensure you have write permissions to the current lakehouse\")\n","    print(\"💡 Check Files directory structure and permissions\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:44:51.594009Z","session_start_time":null,"execution_start_time":"2025-07-18T20:44:51.5951742Z","execution_finish_time":"2025-07-18T20:45:22.8479328Z","parent_msg_id":"6beeeaf2-9f19-43fc-abc1-fe332a787e38"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🧪 TESTING BRONZE LAYER ACCESS\n==================================================\n📝 Testing write to: Files/SalesLT/_test_access\n✅ Write access confirmed\n✅ Read access confirmed (1 test records)\n🎯 Target path ready: Files/SalesLT/\n\n📋 Test data sample:\n+-----------+--------------------+-------+\n|  test_type|           timestamp| status|\n+-----------+--------------------+-------+\n|access_test|2025-07-18T20:42:...|success|\n+-----------+--------------------+-------+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ae119fb"},{"cell_type":"markdown","source":["## Step 4: Process SalesLT Tables to Bronze"],"metadata":{},"id":"09179308"},{"cell_type":"code","source":["# Copy SalesLT tables to bronze layer with metadata enrichment\n","print(\"🚀 PROCESSING SALESLT TABLES TO BRONZE\")\n","print(\"=\" * 60)\n","\n","if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n","    print(\"❌ No tables to process. Run previous steps first.\")\n","else:\n","    print(f\"📋 Processing {len(TABLES_TO_PROCESS)} tables\")\n","    print(f\"📥 Source: {SOURCE_DATABASE}\")\n","    print(f\"📤 Target: {BRONZE_TARGET_PATH}\")\n","    print(f\"📅 Load date: {LOAD_DATE}\")\n","    print()\n","    \n","    # Processing results tracking\n","    results = []\n","    total_rows_processed = 0\n","    \n","    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n","        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Processing {table_name}...\")\n","        \n","        try:\n","            # Read source table using qualified name\n","            print(f\"   📖 Reading from {SOURCE_DATABASE}.{table_name}...\")\n","            source_df = spark.sql(f\"SELECT * FROM {SOURCE_DATABASE}.{table_name}\")\n","            row_count = source_df.count()\n","            \n","            print(f\"   ✅ Source data loaded: {row_count:,} rows\")\n","            \n","            # Add bronze layer metadata columns\n","            print(f\"   🏷️ Adding metadata columns...\")\n","            bronze_df = source_df \\\n","                .withColumn(\"_load_date\", lit(LOAD_DATE)) \\\n","                .withColumn(\"_load_timestamp\", lit(LOAD_TIMESTAMP)) \\\n","                .withColumn(\"_source_system\", lit(SOURCE_SYSTEM)) \\\n","                .withColumn(\"_source_table\", lit(table_name)) \\\n","                .withColumn(\"_processing_timestamp\", current_timestamp()) \\\n","                .withColumn(\"_record_source\", lit(\"cross_lakehouse_copy\")) \\\n","                .withColumn(\"_load_method\", lit(\"spark_sql_full_extract\")) \\\n","                .withColumn(\"_source_database\", lit(SOURCE_DATABASE)) \\\n","                .withColumn(\"_target_path\", lit(f\"{BRONZE_TARGET_PATH}{table_name}\"))\n","            \n","            # Write to bronze layer as files\n","            table_target_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n","            print(f\"   💾 Writing to Files: {table_target_path}\")\n","            \n","            bronze_df.write \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteSchema\", \"true\") \\\n","                .parquet(table_target_path)\n","            \n","            print(f\"   ✅ Files saved: {row_count:,} rows\")\n","            \n","            # Also create lakehouse table (data in memory, efficient to do both)\n","            lakehouse_table_name = f\"bronze_{table_name}\"\n","            print(f\"   🏢 Creating lakehouse table: {lakehouse_table_name}\")\n","            \n","            try:\n","                # Create/replace lakehouse table\n","                bronze_df.write \\\n","                    .mode(\"overwrite\") \\\n","                    .option(\"overwriteSchema\", \"true\") \\\n","                    .saveAsTable(lakehouse_table_name)\n","                \n","                print(f\"   ✅ Table created: {lakehouse_table_name}\")\n","                table_creation_status = \"success\"\n","                \n","            except Exception as table_error:\n","                table_error_msg = str(table_error)[:60]\n","                print(f\"   ⚠️ Table creation failed: {table_error_msg}...\")\n","                table_creation_status = \"file_only\"\n","            \n","            # Success tracking\n","            total_rows_processed += row_count\n","            results.append({\n","                \"table\": table_name,\n","                \"rows\": row_count,\n","                \"status\": \"success\",\n","                \"target_path\": table_target_path,\n","                \"lakehouse_table\": lakehouse_table_name,\n","                \"table_status\": table_creation_status\n","            })\n","            \n","            print(f\"   🎉 Successfully processed {row_count:,} rows (Files + Table)\")\n","            \n","        except Exception as e:\n","            error_msg = str(e)[:100]\n","            results.append({\n","                \"table\": table_name,\n","                \"rows\": 0,\n","                \"status\": \"failed\",\n","                \"error\": error_msg\n","            })\n","            print(f\"   ❌ Failed: {error_msg}...\")\n","        \n","        print()\n","    \n","    # Processing summary\n","    successful = [r for r in results if r[\"status\"] == \"success\"]\n","    failed = [r for r in results if r[\"status\"] == \"failed\"]\n","    tables_created = [r for r in successful if r.get(\"table_status\") == \"success\"]\n","    files_only = [r for r in successful if r.get(\"table_status\") == \"file_only\"]\n","    \n","    print(\"🎉 PROCESSING SUMMARY\")\n","    print(\"=\" * 60)\n","    print(f\"✅ Successfully processed: {len(successful)} tables\")\n","    print(f\"❌ Failed processing: {len(failed)} tables\")\n","    print(f\"📊 Total rows processed: {total_rows_processed:,}\")\n","    print(f\"📁 Files created: {len(successful)} (all)\")\n","    print(f\"🏢 Lakehouse tables created: {len(tables_created)}\")\n","    print(f\"⚠️ Files only (table creation failed): {len(files_only)}\")\n","    print(f\"📅 Processing completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","    \n","    if successful:\n","        print(f\"\\n📁 Bronze layer file structure:\")\n","        print(f\"{BRONZE_TARGET_PATH}\")\n","        for result in successful:\n","            table_marker = \"🏢\" if result.get(\"table_status\") == \"success\" else \"📁\"\n","            print(f\"├── {result['table']}/ ({result['rows']:,} rows) {table_marker}\")\n","    \n","    if tables_created:\n","        print(f\"\\n🏢 Lakehouse tables created:\")\n","        for result in tables_created:\n","            print(f\"✅ {result['lakehouse_table']} ({result['rows']:,} rows)\")\n","    \n","    if files_only:\n","        print(f\"\\n📁 Files only (table creation issues):\")\n","        for result in files_only:\n","            print(f\"⚠️ {result['table']} → Files saved, table creation failed\")\n","    \n","    if failed:\n","        print(f\"\\n⚠️ Processing failures:\")\n","        for result in failed:\n","            print(f\"❌ {result['table']}: {result.get('error', 'Unknown error')}\")\n","    \n","    print(f\"\\n🎯 Bronze data ready for downstream processing!\")\n","    print(f\"📁 Files stored in: {BRONZE_TARGET_PATH}\")\n","    if tables_created:\n","        print(f\"🏢 Tables available in: {len(tables_created)} lakehouse tables\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"f89abdc6-ff33-4e90-a79b-cf52c617d910","normalized_state":"finished","queued_time":"2025-07-18T20:45:37.1531445Z","session_start_time":null,"execution_start_time":"2025-07-18T20:45:37.1546621Z","execution_finish_time":"2025-07-18T20:47:21.1200988Z","parent_msg_id":"9ed5cf3e-efb7-4d9f-b5f7-b5bb1b809b86"},"text/plain":"StatementMeta(, f89abdc6-ff33-4e90-a79b-cf52c617d910, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🚀 PROCESSING SALESLT TABLES TO BRONZE\n============================================================\n📋 Processing 10 tables\n📥 Source: Gaiye_Test_Lakehouse\n📤 Target: Files/SalesLT/\n📅 Load date: 2025-07-18\n\n[1/10] Processing Address...\n   📖 Reading from Gaiye_Test_Lakehouse.Address...\n   ✅ Source data loaded: 450 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/Address\n   ✅ Files saved: 450 rows\n   🏢 Creating lakehouse table: bronze_Address\n   ✅ Table created: bronze_Address\n   🎉 Successfully processed 450 rows (Files + Table)\n\n[2/10] Processing Customer...\n   📖 Reading from Gaiye_Test_Lakehouse.Customer...\n   ✅ Source data loaded: 847 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/Customer\n   ✅ Files saved: 847 rows\n   🏢 Creating lakehouse table: bronze_Customer\n   ✅ Table created: bronze_Customer\n   🎉 Successfully processed 847 rows (Files + Table)\n\n[3/10] Processing CustomerAddress...\n   📖 Reading from Gaiye_Test_Lakehouse.CustomerAddress...\n   ✅ Source data loaded: 417 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/CustomerAddress\n   ✅ Files saved: 417 rows\n   🏢 Creating lakehouse table: bronze_CustomerAddress\n   ✅ Table created: bronze_CustomerAddress\n   🎉 Successfully processed 417 rows (Files + Table)\n\n[4/10] Processing Product...\n   📖 Reading from Gaiye_Test_Lakehouse.Product...\n   ✅ Source data loaded: 295 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/Product\n   ✅ Files saved: 295 rows\n   🏢 Creating lakehouse table: bronze_Product\n   ✅ Table created: bronze_Product\n   🎉 Successfully processed 295 rows (Files + Table)\n\n[5/10] Processing ProductCategory...\n   📖 Reading from Gaiye_Test_Lakehouse.ProductCategory...\n   ✅ Source data loaded: 41 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/ProductCategory\n   ✅ Files saved: 41 rows\n   🏢 Creating lakehouse table: bronze_ProductCategory\n   ✅ Table created: bronze_ProductCategory\n   🎉 Successfully processed 41 rows (Files + Table)\n\n[6/10] Processing ProductDescription...\n   📖 Reading from Gaiye_Test_Lakehouse.ProductDescription...\n   ✅ Source data loaded: 762 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/ProductDescription\n   ✅ Files saved: 762 rows\n   🏢 Creating lakehouse table: bronze_ProductDescription\n   ✅ Table created: bronze_ProductDescription\n   🎉 Successfully processed 762 rows (Files + Table)\n\n[7/10] Processing ProductModel...\n   📖 Reading from Gaiye_Test_Lakehouse.ProductModel...\n   ✅ Source data loaded: 128 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/ProductModel\n   ✅ Files saved: 128 rows\n   🏢 Creating lakehouse table: bronze_ProductModel\n   ✅ Table created: bronze_ProductModel\n   🎉 Successfully processed 128 rows (Files + Table)\n\n[8/10] Processing ProductModelProductDescription...\n   📖 Reading from Gaiye_Test_Lakehouse.ProductModelProductDescription...\n   ✅ Source data loaded: 762 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/ProductModelProductDescription\n   ✅ Files saved: 762 rows\n   🏢 Creating lakehouse table: bronze_ProductModelProductDescription\n   ✅ Table created: bronze_ProductModelProductDescription\n   🎉 Successfully processed 762 rows (Files + Table)\n\n[9/10] Processing SalesOrderDetail...\n   📖 Reading from Gaiye_Test_Lakehouse.SalesOrderDetail...\n   ✅ Source data loaded: 542 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/SalesOrderDetail\n   ✅ Files saved: 542 rows\n   🏢 Creating lakehouse table: bronze_SalesOrderDetail\n   ✅ Table created: bronze_SalesOrderDetail\n   🎉 Successfully processed 542 rows (Files + Table)\n\n[10/10] Processing SalesOrderHeader...\n   📖 Reading from Gaiye_Test_Lakehouse.SalesOrderHeader...\n   ✅ Source data loaded: 32 rows\n   🏷️ Adding metadata columns...\n   💾 Writing to Files: Files/SalesLT/SalesOrderHeader\n   ✅ Files saved: 32 rows\n   🏢 Creating lakehouse table: bronze_SalesOrderHeader\n   ✅ Table created: bronze_SalesOrderHeader\n   🎉 Successfully processed 32 rows (Files + Table)\n\n🎉 PROCESSING SUMMARY\n============================================================\n✅ Successfully processed: 10 tables\n❌ Failed processing: 0 tables\n📊 Total rows processed: 4,276\n📁 Files created: 10 (all)\n🏢 Lakehouse tables created: 10\n⚠️ Files only (table creation failed): 0\n📅 Processing completed: 2025-07-18 20:47:19\n\n📁 Bronze layer file structure:\nFiles/SalesLT/\n├── Address/ (450 rows) 🏢\n├── Customer/ (847 rows) 🏢\n├── CustomerAddress/ (417 rows) 🏢\n├── Product/ (295 rows) 🏢\n├── ProductCategory/ (41 rows) 🏢\n├── ProductDescription/ (762 rows) 🏢\n├── ProductModel/ (128 rows) 🏢\n├── ProductModelProductDescription/ (762 rows) 🏢\n├── SalesOrderDetail/ (542 rows) 🏢\n├── SalesOrderHeader/ (32 rows) 🏢\n\n🏢 Lakehouse tables created:\n✅ bronze_Address (450 rows)\n✅ bronze_Customer (847 rows)\n✅ bronze_CustomerAddress (417 rows)\n✅ bronze_Product (295 rows)\n✅ bronze_ProductCategory (41 rows)\n✅ bronze_ProductDescription (762 rows)\n✅ bronze_ProductModel (128 rows)\n✅ bronze_ProductModelProductDescription (762 rows)\n✅ bronze_SalesOrderDetail (542 rows)\n✅ bronze_SalesOrderHeader (32 rows)\n\n🎯 Bronze data ready for downstream processing!\n📁 Files stored in: Files/SalesLT/\n🏢 Tables available in: 10 lakehouse tables\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ccd7807"},{"cell_type":"markdown","source":["## Step 5: Validate Bronze Layer Data"],"metadata":{},"id":"484b1ba4"},{"cell_type":"code","source":["# Validate bronze layer data quality and completeness\n","print(\"🔍 VALIDATING BRONZE LAYER DATA\")\n","print(\"=\" * 60)\n","\n","if 'TABLES_TO_PROCESS' not in locals() or len(TABLES_TO_PROCESS) == 0:\n","    print(\"❌ No tables to validate. Run previous steps first.\")\n","else:\n","    validation_results = []\n","    total_bronze_rows = 0\n","    \n","    print(f\"📋 Validating {len(TABLES_TO_PROCESS)} bronze files\")\n","    print(f\"📍 Target location: {BRONZE_TARGET_PATH}\")\n","    print()\n","    \n","    for i, table_name in enumerate(TABLES_TO_PROCESS, 1):\n","        print(f\"[{i}/{len(TABLES_TO_PROCESS)}] Validating {table_name}...\")\n","        \n","        try:\n","            file_path = f\"{BRONZE_TARGET_PATH}{table_name}\"\n","            lakehouse_table_name = f\"bronze_{table_name}\"\n","            \n","            # Read bronze data files\n","            bronze_df = spark.read.parquet(file_path)\n","            bronze_count = bronze_df.count()\n","            \n","            # Check if lakehouse table exists and validate\n","            table_count = 0\n","            table_exists = False\n","            try:\n","                table_df = spark.table(lakehouse_table_name)\n","                table_count = table_df.count()\n","                table_exists = True\n","            except Exception:\n","                table_exists = False\n","            \n","            # Check metadata columns\n","            sample_row = bronze_df.select(\n","                \"_load_date\", \n","                \"_source_system\", \n","                \"_source_table\",\n","                \"_load_method\"\n","            ).first()\n","            \n","            # Get column count for schema validation\n","            column_count = len(bronze_df.columns)\n","            metadata_columns = [col for col in bronze_df.columns if col.startswith('_')]\n","            business_columns = [col for col in bronze_df.columns if not col.startswith('_')]\n","            \n","            total_bronze_rows += bronze_count\n","            validation_results.append({\n","                \"table\": table_name,\n","                \"bronze_rows\": bronze_count,\n","                \"table_rows\": table_count,\n","                \"table_exists\": table_exists,\n","                \"total_columns\": column_count,\n","                \"business_columns\": len(business_columns),\n","                \"metadata_columns\": len(metadata_columns),\n","                \"load_date\": sample_row._load_date if sample_row else \"Unknown\",\n","                \"source_system\": sample_row._source_system if sample_row else \"Unknown\",\n","                \"load_method\": sample_row._load_method if sample_row else \"Unknown\",\n","                \"lakehouse_table\": lakehouse_table_name,\n","                \"status\": \"success\"\n","            })\n","            \n","            print(f\"   ✅ Files: {bronze_count:,} rows validated\")\n","            if table_exists:\n","                print(f\"   🏢 Table: {table_count:,} rows validated\")\n","                row_match = \"✅\" if bronze_count == table_count else \"⚠️\"\n","                print(f\"   {row_match} Row count match: Files={bronze_count:,}, Table={table_count:,}\")\n","            else:\n","                print(f\"   ⚠️ Lakehouse table not found: {lakehouse_table_name}\")\n","            print(f\"   📊 Columns: {len(business_columns)} business + {len(metadata_columns)} metadata\")\n","            print(f\"   📅 Load date: {sample_row._load_date if sample_row else 'Unknown'}\")\n","            print(f\"   🏷️ Source: {sample_row._source_system if sample_row else 'Unknown'}\")\n","            \n","        except Exception as e:\n","            error_msg = str(e)[:80]\n","            validation_results.append({\n","                \"table\": table_name,\n","                \"bronze_rows\": 0,\n","                \"status\": \"failed\",\n","                \"error\": error_msg\n","            })\n","            print(f\"   ❌ Validation failed: {error_msg}...\")\n","        \n","        print()\n","    \n","    # Validation summary\n","    successful_validations = [r for r in validation_results if r[\"status\"] == \"success\"]\n","    failed_validations = [r for r in validation_results if r[\"status\"] == \"failed\"]\n","    tables_available = [r for r in successful_validations if r.get(\"table_exists\", False)]\n","    files_only = [r for r in successful_validations if not r.get(\"table_exists\", False)]\n","    \n","    print(\"🎯 VALIDATION SUMMARY\")\n","    print(\"=\" * 60)\n","    print(f\"✅ Successfully validated: {len(successful_validations)} files\")\n","    print(f\"❌ Failed validations: {len(failed_validations)} files\")\n","    print(f\"📊 Total bronze rows: {total_bronze_rows:,}\")\n","    print(f\"📁 Files available: {len(successful_validations)}\")\n","    print(f\"🏢 Lakehouse tables available: {len(tables_available)}\")\n","    print(f\"⚠️ Files only (no table): {len(files_only)}\")\n","    print(f\"🏷️ Metadata enrichment: Load tracking added\")\n","    \n","    if successful_validations:\n","        print(f\"\\n📋 Bronze layer inventory:\")\n","        for result in successful_validations:\n","            table_marker = \"🏢+📁\" if result.get(\"table_exists\") else \"📁\"\n","            table_info = f\" | Table: {result.get('table_rows', 0):,}\" if result.get(\"table_exists\") else \"\"\n","            print(f\"  • {result['table']}: {result['bronze_rows']:,} rows | {result['total_columns']} columns | {result['load_date']} {table_marker}{table_info}\")\n","    \n","    if tables_available:\n","        print(f\"\\n🏢 Available lakehouse tables:\")\n","        for result in tables_available:\n","            print(f\"  ✅ {result['lakehouse_table']}: {result.get('table_rows', 0):,} rows\")\n","    \n","    if files_only:\n","        print(f\"\\n📁 Files only (tables not created):\")\n","        for result in files_only:\n","            print(f\"  ⚠️ {result['table']}: File available, no lakehouse table\")\n","    \n","    if failed_validations:\n","        print(f\"\\n⚠️ Validation failures:\")\n","        for result in failed_validations:\n","            print(f\"  ❌ {result['table']}: {result.get('error', 'Unknown error')}\")\n","    \n","    print(f\"\\n🎉 Bronze layer validation complete!\")\n","    print(f\"📁 Files location: {BRONZE_TARGET_PATH}\")\n","    if tables_available:\n","        print(f\"🏢 Tables accessible via: SELECT * FROM bronze_[tablename]\")\n","    print(f\"🚀 Ready for silver layer processing\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"2236780f-4100-4d35-bc75-eae718618527","normalized_state":"finished","queued_time":"2025-07-17T17:47:35.8038802Z","session_start_time":null,"execution_start_time":"2025-07-17T17:47:35.8054881Z","execution_finish_time":"2025-07-17T17:48:03.6253879Z","parent_msg_id":"41ffbeb3-3aa0-4959-8dfe-0b1f81a26f43"},"text/plain":"StatementMeta(, 2236780f-4100-4d35-bc75-eae718618527, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🔍 VALIDATING BRONZE LAYER DATA\n============================================================\n📋 Validating 10 bronze files\n📍 Target location: Files/SalesLT/\n\n[1/10] Validating Address...\n   ✅ Files: 450 rows validated\n   🏢 Table: 450 rows validated\n   ✅ Row count match: Files=450, Table=450\n   📊 Columns: 9 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[2/10] Validating Customer...\n   ✅ Files: 847 rows validated\n   🏢 Table: 847 rows validated\n   ✅ Row count match: Files=847, Table=847\n   📊 Columns: 15 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[3/10] Validating CustomerAddress...\n   ✅ Files: 417 rows validated\n   🏢 Table: 417 rows validated\n   ✅ Row count match: Files=417, Table=417\n   📊 Columns: 5 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[4/10] Validating Product...\n   ✅ Files: 295 rows validated\n   🏢 Table: 295 rows validated\n   ✅ Row count match: Files=295, Table=295\n   📊 Columns: 17 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[5/10] Validating ProductCategory...\n   ✅ Files: 41 rows validated\n   🏢 Table: 41 rows validated\n   ✅ Row count match: Files=41, Table=41\n   📊 Columns: 5 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[6/10] Validating ProductDescription...\n   ✅ Files: 762 rows validated\n   🏢 Table: 762 rows validated\n   ✅ Row count match: Files=762, Table=762\n   📊 Columns: 4 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[7/10] Validating ProductModel...\n   ✅ Files: 128 rows validated\n   🏢 Table: 128 rows validated\n   ✅ Row count match: Files=128, Table=128\n   📊 Columns: 4 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[8/10] Validating ProductModelProductDescription...\n   ✅ Files: 762 rows validated\n   🏢 Table: 762 rows validated\n   ✅ Row count match: Files=762, Table=762\n   📊 Columns: 5 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[9/10] Validating SalesOrderDetail...\n   ✅ Files: 542 rows validated\n   🏢 Table: 542 rows validated\n   ✅ Row count match: Files=542, Table=542\n   📊 Columns: 8 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n[10/10] Validating SalesOrderHeader...\n   ✅ Files: 32 rows validated\n   🏢 Table: 32 rows validated\n   ✅ Row count match: Files=32, Table=32\n   📊 Columns: 20 business + 9 metadata\n   📅 Load date: 2025-07-17\n   🏷️ Source: SalesLT\n\n🎯 VALIDATION SUMMARY\n============================================================\n✅ Successfully validated: 10 files\n❌ Failed validations: 0 files\n📊 Total bronze rows: 4,276\n📁 Files available: 10\n🏢 Lakehouse tables available: 10\n⚠️ Files only (no table): 0\n🏷️ Metadata enrichment: Load tracking added\n\n📋 Bronze layer inventory:\n  • Address: 450 rows | 18 columns | 2025-07-17 🏢+📁 | Table: 450\n  • Customer: 847 rows | 24 columns | 2025-07-17 🏢+📁 | Table: 847\n  • CustomerAddress: 417 rows | 14 columns | 2025-07-17 🏢+📁 | Table: 417\n  • Product: 295 rows | 26 columns | 2025-07-17 🏢+📁 | Table: 295\n  • ProductCategory: 41 rows | 14 columns | 2025-07-17 🏢+📁 | Table: 41\n  • ProductDescription: 762 rows | 13 columns | 2025-07-17 🏢+📁 | Table: 762\n  • ProductModel: 128 rows | 13 columns | 2025-07-17 🏢+📁 | Table: 128\n  • ProductModelProductDescription: 762 rows | 14 columns | 2025-07-17 🏢+📁 | Table: 762\n  • SalesOrderDetail: 542 rows | 17 columns | 2025-07-17 🏢+📁 | Table: 542\n  • SalesOrderHeader: 32 rows | 29 columns | 2025-07-17 🏢+📁 | Table: 32\n\n🏢 Available lakehouse tables:\n  ✅ bronze_Address: 450 rows\n  ✅ bronze_Customer: 847 rows\n  ✅ bronze_CustomerAddress: 417 rows\n  ✅ bronze_Product: 295 rows\n  ✅ bronze_ProductCategory: 41 rows\n  ✅ bronze_ProductDescription: 762 rows\n  ✅ bronze_ProductModel: 128 rows\n  ✅ bronze_ProductModelProductDescription: 762 rows\n  ✅ bronze_SalesOrderDetail: 542 rows\n  ✅ bronze_SalesOrderHeader: 32 rows\n\n🎉 Bronze layer validation complete!\n📁 Files location: Files/SalesLT/\n🏢 Tables accessible via: SELECT * FROM bronze_[tablename]\n🚀 Ready for silver layer processing\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28b3a08c"}],"metadata":{"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b160891e-ec7d-471d-961d-4f50cbb006c4"},{"id":"695625b4-4901-48e3-8183-cbaac28086b5"}],"default_lakehouse":"b160891e-ec7d-471d-961d-4f50cbb006c4","default_lakehouse_name":"RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze","default_lakehouse_workspace_id":"88ef0969-45fb-42dd-af36-283224c74eed"}}},"nbformat":4,"nbformat_minor":5}