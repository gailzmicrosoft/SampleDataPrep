{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85331f0",
   "metadata": {},
   "source": [
    "# Copy SalesLT Data from Shortcuts to Bronze Lakehouse\n",
    "\n",
    "This notebook reads SalesLT tables from shortcuts in `Gaiye_Test_Lakehouse` and copies them to the bronze layer in `RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze`.\n",
    "\n",
    "**Setup Required:**\n",
    "1. **Source**: `Gaiye_Test_Lakehouse` attached as additional lakehouse (contains shortcuts)\n",
    "2. **Target**: `RDS_Fabric_Foundry_workspace_Gaiye_Retail_Solution_Test_IDM_LH_bronze` attached as default lakehouse\n",
    "3. **Operation**: Copy shortcut data â†’ bronze layer Files/bronze/saleslt/\n",
    "\n",
    "**Expected Tables in Source:**\n",
    "- Customer, Product, Address, CustomerAddress\n",
    "- ProductCategory, ProductDescription, ProductModel\n",
    "- ProductModelProductDescription, SalesOrderDetail, SalesOrderHeader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306010ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries and Setup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ğŸ“… Copy process started at: {datetime.now()}\")\n",
    "print(\"ğŸ”„ Source: Gaiye_Test_Lakehouse shortcuts â†’ Target: Bronze lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Environment and Lakehouse Check\n",
    "print(\"ğŸ” ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check Spark session\n",
    "try:\n",
    "    print(f\"âœ… Spark session: {spark.version}\")\n",
    "except:\n",
    "    print(\"âŒ Spark session not available\")\n",
    "    raise Exception(\"Spark session required\")\n",
    "\n",
    "# Check default lakehouse (should be bronze lakehouse)\n",
    "try:\n",
    "    default_tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    print(f\"âœ… Default lakehouse (Bronze): {len(default_tables)} tables visible\")\n",
    "    \n",
    "    # Show sample tables from default lakehouse\n",
    "    if len(default_tables) > 0:\n",
    "        print(\"ğŸ“Š Sample tables in default lakehouse:\")\n",
    "        for table in default_tables['tableName'][:5]:\n",
    "            print(f\"   ğŸ“„ {table}\")\n",
    "        if len(default_tables) > 5:\n",
    "            print(f\"   ... and {len(default_tables)-5} more\")\n",
    "    else:\n",
    "        print(\"ğŸ“Š Default lakehouse is empty (expected for bronze target)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Cannot access default lakehouse: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Connect to Source Lakehouse and Discover SalesLT Tables\n",
    "print(\"ğŸ” CONNECTING TO SOURCE LAKEHOUSE (Gaiye_Test_Lakehouse)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define expected SalesLT table names (from shortcuts)\n",
    "expected_saleslt_tables = {\n",
    "    'address': 'Address Information',\n",
    "    'customer': 'Customer Data', \n",
    "    'customeraddress': 'Customer Data',\n",
    "    'product': 'Product Catalog',\n",
    "    'productcategory': 'Product Catalog',\n",
    "    'productdescription': 'Product Catalog',\n",
    "    'productmodel': 'Product Catalog',\n",
    "    'productmodelproductdescription': 'Product Catalog',\n",
    "    'salesorderdetail': 'Sales Transactions',\n",
    "    'salesorderheader': 'Sales Transactions'\n",
    "}\n",
    "\n",
    "# Try to access source lakehouse tables\n",
    "source_tables_info = []\n",
    "\n",
    "try:\n",
    "    # Method 1: Try to access tables directly (if both lakehouses are attached)\n",
    "    print(\"ğŸ” Method 1: Checking for SalesLT tables in accessible lakehouses...\")\n",
    "    \n",
    "    # Check all available tables across lakehouses\n",
    "    all_tables = spark.sql(\"SHOW TABLES\").toPandas()\n",
    "    \n",
    "    # Look for SalesLT tables\n",
    "    found_source_tables = []\n",
    "    \n",
    "    for _, row in all_tables.iterrows():\n",
    "        table_name = row['tableName']\n",
    "        table_name_lower = table_name.lower()\n",
    "        \n",
    "        # Check if this matches a SalesLT table\n",
    "        if table_name_lower in expected_saleslt_tables:\n",
    "            category = expected_saleslt_tables[table_name_lower]\n",
    "            found_source_tables.append(table_name)\n",
    "            \n",
    "            source_tables_info.append({\n",
    "                'table_name': table_name,\n",
    "                'category': category,\n",
    "                'source_reference': table_name  # Direct table reference\n",
    "            })\n",
    "            \n",
    "            print(f\"   âœ… Found: {table_name} ({category})\")\n",
    "    \n",
    "    if len(source_tables_info) == 0:\n",
    "        print(\"âš ï¸ No SalesLT tables found in directly accessible tables\")\n",
    "        print(\"ğŸ’¡ You may need to:\")\n",
    "        print(\"   1. Attach Gaiye_Test_Lakehouse as additional lakehouse\")\n",
    "        print(\"   2. Or use database.table_name syntax\")\n",
    "        \n",
    "        # Method 2: Try with database prefix\n",
    "        print(\"\\nğŸ” Method 2: Trying with database prefix...\")\n",
    "        \n",
    "        # Check available databases\n",
    "        databases = spark.sql(\"SHOW DATABASES\").toPandas()\n",
    "        print(f\"ğŸ“ Available databases: {databases.iloc[:, 0].tolist()}\")\n",
    "        \n",
    "        # Look for Gaiye-related database\n",
    "        for db_name in databases.iloc[:, 0]:\n",
    "            if 'gaiye' in db_name.lower() or 'test' in db_name.lower():\n",
    "                print(f\"ğŸ” Checking database: {db_name}\")\n",
    "                try:\n",
    "                    db_tables = spark.sql(f\"SHOW TABLES IN {db_name}\").toPandas()\n",
    "                    \n",
    "                    for _, row in db_tables.iterrows():\n",
    "                        table_name = row['tableName']\n",
    "                        table_name_lower = table_name.lower()\n",
    "                        \n",
    "                        if table_name_lower in expected_saleslt_tables:\n",
    "                            category = expected_saleslt_tables[table_name_lower]\n",
    "                            \n",
    "                            source_tables_info.append({\n",
    "                                'table_name': table_name,\n",
    "                                'category': category,\n",
    "                                'source_reference': f\"{db_name}.{table_name}\"\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"   âœ… Found: {db_name}.{table_name} ({category})\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Could not access {db_name}: {str(e)[:50]}...\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DISCOVERY SUMMARY:\")\n",
    "    print(f\"   ğŸ” Total tables found: {len(source_tables_info)}\")\n",
    "    print(f\"   ğŸ“‹ Expected tables: {len(expected_saleslt_tables)}\")\n",
    "    \n",
    "    if len(source_tables_info) == 0:\n",
    "        print(f\"\\nâŒ NO SALESLT TABLES FOUND\")\n",
    "        print(f\"ğŸ”§ TROUBLESHOOTING:\")\n",
    "        print(f\"   1. Ensure Gaiye_Test_Lakehouse is attached as additional lakehouse\")\n",
    "        print(f\"   2. Check that shortcuts are visible in the source lakehouse\")\n",
    "        print(f\"   3. Verify table names match expected format\")\n",
    "        raise Exception(\"No source tables found\")\n",
    "    \n",
    "    elif len(source_tables_info) < len(expected_saleslt_tables):\n",
    "        missing_count = len(expected_saleslt_tables) - len(source_tables_info)\n",
    "        print(f\"\\nâš ï¸ PARTIAL DISCOVERY: {missing_count} tables missing\")\n",
    "        print(f\"   ğŸ’¡ You can proceed with available tables\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nğŸ‰ COMPLETE DISCOVERY: All expected tables found!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Discovery failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Copy Function - Source to Bronze\n",
    "def copy_table_to_bronze(table_info):\n",
    "    \"\"\"Copy a table from source lakehouse to bronze layer\"\"\"\n",
    "    table_name = table_info['table_name']\n",
    "    category = table_info['category']\n",
    "    source_ref = table_info['source_reference']\n",
    "    \n",
    "    print(f\"ğŸ“¦ Copying {table_name} ({category})\")\n",
    "    print(f\"   ğŸ”— Source: {source_ref}\")\n",
    "    \n",
    "    try:\n",
    "        # Read from source table\n",
    "        df = spark.sql(f\"SELECT * FROM {source_ref}\")\n",
    "        \n",
    "        # Get basic info\n",
    "        row_count = df.count()\n",
    "        columns = df.columns\n",
    "        \n",
    "        print(f\"   ğŸ“Š Loaded: {row_count:,} rows, {len(columns)} columns\")\n",
    "        \n",
    "        # Add bronze layer metadata\n",
    "        df_with_metadata = df \\\n",
    "            .withColumn(\"_bronze_load_date\", lit(datetime.now().strftime(\"%Y-%m-%d\"))) \\\n",
    "            .withColumn(\"_bronze_load_timestamp\", lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))) \\\n",
    "            .withColumn(\"_source_system\", lit(\"SalesLT\")) \\\n",
    "            .withColumn(\"_source_table\", lit(table_name)) \\\n",
    "            .withColumn(\"_source_lakehouse\", lit(\"Gaiye_Test_Lakehouse\")) \\\n",
    "            .withColumn(\"_load_method\", lit(\"lakehouse_copy\"))\n",
    "        \n",
    "        # Create bronze path\n",
    "        bronze_path = f\"Files/bronze/saleslt/{table_name.lower()}\"\n",
    "        \n",
    "        print(f\"   ğŸ’¾ Writing to bronze: {bronze_path}\")\n",
    "        \n",
    "        # Write to bronze layer\n",
    "        df_with_metadata.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .parquet(bronze_path)\n",
    "        \n",
    "        print(f\"   âœ… Copy completed: {row_count:,} rows\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"table_name\": table_name,\n",
    "            \"row_count\": row_count,\n",
    "            \"column_count\": len(columns),\n",
    "            \"bronze_path\": bronze_path,\n",
    "            \"category\": category\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"   âŒ Copy failed: {error_msg[:100]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"table_name\": table_name,\n",
    "            \"error\": error_msg\n",
    "        }\n",
    "\n",
    "print(\"âš™ï¸ COPY FUNCTION READY\")\n",
    "print(\"âœ… Source: Gaiye_Test_Lakehouse shortcuts\")\n",
    "print(\"âœ… Target: Bronze lakehouse Files/bronze/saleslt/\")\n",
    "print(\"âœ… Metadata tracking included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Execute Copy Process\n",
    "print(\"ğŸš€ STARTING COPY TO BRONZE LAYER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“… Copy Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ“Š Tables to Copy: {len(source_tables_info)}\")\n",
    "print(f\"ğŸ”— Source: Gaiye_Test_Lakehouse\")\n",
    "print(f\"ğŸ¯ Target: Bronze lakehouse\")\n",
    "print()\n",
    "\n",
    "copy_results = []\n",
    "successful_copies = 0\n",
    "failed_copies = 0\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Copy each table\n",
    "for i, table_info in enumerate(source_tables_info, 1):\n",
    "    print(f\"[{i}/{len(source_tables_info)}] {table_info['table_name']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = copy_table_to_bronze(table_info)\n",
    "    copy_results.append(result)\n",
    "    \n",
    "    if result['success']:\n",
    "        successful_copies += 1\n",
    "    else:\n",
    "        failed_copies += 1\n",
    "    \n",
    "    print()  # Add spacing between tables\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Copy Summary\n",
    "print(\"ğŸ‰ COPY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"â±ï¸ Total Duration: {duration:.1f} seconds\")\n",
    "print(f\"âœ… Successful Copies: {successful_copies}\")\n",
    "print(f\"âŒ Failed Copies: {failed_copies}\")\n",
    "print(f\"ğŸ“Š Total Tables: {len(source_tables_info)}\")\n",
    "\n",
    "if successful_copies > 0:\n",
    "    print(f\"\\nğŸ“ BRONZE LAYER STRUCTURE:\")\n",
    "    print(f\"Files/bronze/saleslt/\")\n",
    "    \n",
    "    # Group by category\n",
    "    from collections import defaultdict\n",
    "    by_category = defaultdict(list)\n",
    "    \n",
    "    for result in copy_results:\n",
    "        if result['success']:\n",
    "            table_info = next((t for t in source_tables_info if t['table_name'] == result['table_name']), None)\n",
    "            if table_info:\n",
    "                by_category[table_info['category']].append(result)\n",
    "    \n",
    "    for category, results in by_category.items():\n",
    "        print(f\"   ğŸ“‚ {category}:\")\n",
    "        for result in results:\n",
    "            print(f\"      ğŸ“„ {result['table_name'].lower()}/ ({result['row_count']:,} rows)\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ NEXT STEPS:\")\n",
    "    print(f\"   1. âœ… Data now available in bronze lakehouse\")\n",
    "    print(f\"   2. ğŸ” Verify data in Files/bronze/saleslt/ folders\")\n",
    "    print(f\"   3. ğŸ“Š Create silver layer transformations\") \n",
    "    print(f\"   4. ğŸ”— Build analytics and reports\")\n",
    "\n",
    "if failed_copies > 0:\n",
    "    print(f\"\\nâš ï¸ COPY FAILURES:\")\n",
    "    for result in copy_results:\n",
    "        if not result['success']:\n",
    "            print(f\"   âŒ {result['table_name']}: {result.get('error', 'Unknown error')[:80]}...\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ“‹ Copy process completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ¯ Bronze data now available in target lakehouse!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63189aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Validation - Check Bronze Layer in Target Lakehouse\n",
    "print(\"ğŸ” BRONZE LAYER VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Check Files directory structure\n",
    "    try:\n",
    "        files_list = dbutils.fs.ls(\"Files/\")\n",
    "        print(f\"ğŸ“ Files directory contains {len(files_list)} items:\")\n",
    "        for file_info in files_list:\n",
    "            print(f\"   ğŸ“‚ {file_info.name}\")\n",
    "        \n",
    "        # Check bronze directory\n",
    "        try:\n",
    "            bronze_list = dbutils.fs.ls(\"Files/bronze/\")\n",
    "            print(f\"\\nğŸ“ Bronze directory contains {len(bronze_list)} items:\")\n",
    "            for file_info in bronze_list:\n",
    "                print(f\"   ğŸ“‚ {file_info.name}\")\n",
    "            \n",
    "            # Check saleslt directory\n",
    "            try:\n",
    "                saleslt_list = dbutils.fs.ls(\"Files/bronze/saleslt/\")\n",
    "                print(f\"\\nğŸ“ SalesLT directory contains {len(saleslt_list)} tables:\")\n",
    "                \n",
    "                total_files = 0\n",
    "                for file_info in saleslt_list:\n",
    "                    if file_info.isDir():\n",
    "                        table_name = file_info.name.rstrip('/')\n",
    "                        try:\n",
    "                            table_files = dbutils.fs.ls(f\"Files/bronze/saleslt/{table_name}/\")\n",
    "                            parquet_files = [f for f in table_files if f.name.endswith('.parquet')]\n",
    "                            total_files += len(parquet_files)\n",
    "                            print(f\"   ğŸ“„ {table_name}/ ({len(parquet_files)} parquet files)\")\n",
    "                        except:\n",
    "                            print(f\"   ğŸ“„ {table_name}/ (could not read contents)\")\n",
    "                \n",
    "                print(f\"\\nğŸ“Š SUMMARY:\")\n",
    "                print(f\"   ğŸ“‚ Table directories: {len(saleslt_list)}\")\n",
    "                print(f\"   ğŸ“„ Total parquet files: {total_files}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Could not access Files/bronze/saleslt/: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Could not access Files/bronze/: {str(e)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not access Files/: {str(e)}\")\n",
    "        print(\"ğŸ’¡ Using alternative method...\")\n",
    "        \n",
    "        # Alternative: Try mssparkutils\n",
    "        try:\n",
    "            from notebookutils import mssparkutils\n",
    "            files_list = mssparkutils.fs.ls(\"Files/bronze/saleslt/\")\n",
    "            print(f\"âœ… Found {len(files_list)} items in bronze/saleslt/:\")\n",
    "            for file_info in files_list:\n",
    "                print(f\"   ğŸ“‚ {file_info.name}\")\n",
    "        except Exception as alt_e:\n",
    "            print(f\"âŒ Alternative method failed: {str(alt_e)}\")\n",
    "    \n",
    "    # Quick data validation\n",
    "    if successful_copies > 0:\n",
    "        print(f\"\\nğŸ” QUICK DATA VALIDATION:\")\n",
    "        \n",
    "        # Try to read a sample table\n",
    "        sample_table = copy_results[0]['table_name'].lower() if copy_results and copy_results[0]['success'] else 'customer'\n",
    "        \n",
    "        try:\n",
    "            sample_df = spark.read.parquet(f\"Files/bronze/saleslt/{sample_table}\")\n",
    "            sample_count = sample_df.count()\n",
    "            sample_columns = sample_df.columns\n",
    "            \n",
    "            print(f\"   âœ… {sample_table}: {sample_count:,} rows, {len(sample_columns)} columns\")\n",
    "            print(f\"   ğŸ“‹ Sample columns: {', '.join(sample_columns[:5])}...\")\n",
    "            \n",
    "            # Show metadata columns\n",
    "            metadata_cols = [col for col in sample_columns if col.startswith('_')]\n",
    "            if metadata_cols:\n",
    "                print(f\"   ğŸ·ï¸ Metadata columns: {', '.join(metadata_cols)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Could not read sample data: {str(e)[:100]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Validation error: {str(e)}\")\n",
    "\n",
    "print(f\"\\nâœ… Validation completed\")\n",
    "print(f\"ğŸ¯ Check your bronze lakehouse Files section for the copied data!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9de008",
   "metadata": {},
   "source": [
    "## Success! ğŸ‰\n",
    "\n",
    "Your SalesLT data has been successfully copied from the shortcuts in `Gaiye_Test_Lakehouse` to the bronze layer in your target lakehouse.\n",
    "\n",
    "### What was accomplished:\n",
    "- âœ… **Source detected**: Found SalesLT tables from Gaiye_Test_Lakehouse shortcuts\n",
    "- âœ… **Data copied**: All tables transferred to bronze layer with metadata tracking\n",
    "- âœ… **Bronze layer created**: Organized structure in `Files/bronze/saleslt/`\n",
    "- âœ… **Metadata added**: Source tracking and load timestamps included\n",
    "\n",
    "### Bronze Layer Structure:\n",
    "```\n",
    "Files/bronze/saleslt/\n",
    "â”œâ”€â”€ address/           # Address information with parquet files\n",
    "â”œâ”€â”€ customer/          # Customer master data\n",
    "â”œâ”€â”€ customeraddress/   # Customer-address relationships  \n",
    "â”œâ”€â”€ product/           # Product catalog\n",
    "â”œâ”€â”€ productcategory/   # Product categories\n",
    "â”œâ”€â”€ productdescription/ # Product descriptions\n",
    "â”œâ”€â”€ productmodel/      # Product models\n",
    "â”œâ”€â”€ productmodelproductdescription/ # Model-description links\n",
    "â”œâ”€â”€ salesorderdetail/  # Order line items\n",
    "â””â”€â”€ salesorderheader/  # Order headers\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "1. **Verify in Lakehouse**: Check Files â†’ bronze â†’ saleslt in your target lakehouse\n",
    "2. **Data Quality Check**: Review row counts and data completeness\n",
    "3. **Create Silver Layer**: Build cleaned and standardized datasets\n",
    "4. **Build Analytics**: Connect to Power BI or create reports\n",
    "\n",
    "### Key Features:\n",
    "- **Metadata Tracking**: Each file includes `_bronze_load_date`, `_source_lakehouse`, etc.\n",
    "- **Overwrite Mode**: Re-running will refresh the data\n",
    "- **Parquet Format**: Optimized for analytics and compression\n",
    "- **Organized Structure**: Each table in its own folder for easy access\n",
    "\n",
    "Your retail data model bronze layer is now ready for the next phase of your data pipeline! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
